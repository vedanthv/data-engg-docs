{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Engineering and Cloud Platform Knowledge Base","text":"<p>This repository is where I keep track of what I\u2019m learning and experimenting with across data engineering, distributed systems, and cloud platforms. It\u2019s part study notes, part code lab, and part reference guide \u2014 essentially a place to capture concepts as I work through them and to revisit later when I need a refresher.</p> <p>I focus on technologies like Apache Spark, Kafka/Redpanda, Databricks, AWS, and Azure, along with supporting tools and practices that are important for building reliable data systems. You\u2019ll find a mix of summaries, deep dives into tricky concepts, performance tuning notes, and small experiments that test how things work in practice.</p> <p>The main goal of this repo is to strengthen my own understanding, but I also hope it can be useful to anyone else navigating similar topics. Think of it as a learning log that balances hands-on exploration with professional best practices - something between a personal notebook and a practical guide.</p>"},{"location":"#data-engineering-knowledge-base","title":"\ud83d\udcd1 Data Engineering Knowledge Base","text":""},{"location":"#azure","title":"Azure","text":"<ol> <li>Azure Integration Databricks</li> <li>Azure Portal Subscriptions Resourcegroups</li> <li>Azure Cli Scenarios</li> <li>Azure Powershell Scenarios</li> <li>Azure Arm Templates</li> <li>Azure Bicep Templates</li> <li>Overview Of Azure Storage</li> <li>Blob Storage Fundamentals</li> <li>Adls Gen2 Overview</li> <li>Azure Rbac Acl</li> <li>Azure Types Of Storage</li> <li>Azure Storage Replication Strategies</li> <li>Soft Delete Pitr Azure Storage</li> <li>Azure Shared Access Signature</li> <li>Azure Lifetime Management Policies</li> <li>Eventgrid Integration Azure</li> <li>Azure Encrpytion Standards</li> <li>Azure+ Private Endpoints</li> <li>Cross Region Replication Azure</li> <li>Azure Storage Rest Api</li> <li>Introduction Azure Data Factory</li> <li>Azure Data Factory Vs Synapse</li> <li>Azure Data Factory Architecture</li> <li>Adf Triggers Intro</li> <li>Adf Parameters</li> <li>Index</li> </ol>"},{"location":"#data-formats","title":"Data-formats","text":"<ol> <li>Data Format Deep Dive Pt1</li> <li>Parquet Format Internals</li> </ol>"},{"location":"#databricks","title":"Databricks","text":"<ol> <li>Azure Databricks Uc Creation</li> <li>Databricks Uc Introduction</li> <li>Databricks Managed External Tables Hive</li> <li>Uc Managed External Tables</li> <li>Uc External Location Storage Credentials</li> <li>Databricks Managed Location Catalog Schema Level</li> <li>Ctas Deep Clone Shallow Clone Databricks</li> <li>Rbac Custom Roles Serviceprincipals</li> <li>Deletion Vectors Delta Lake</li> <li>Liquid Clustering Delta Lake</li> <li>Concurrency Liquid Clustering</li> <li>Copy Into Databricks</li> <li>Autoloader Databricks</li> <li>12.1 Autoloader Databricks Schema Inference</li> <li>Intro Databricks Lakeflow Declarative Pipelines</li> <li>Dlt Batch Vs Streaming Workloads</li> <li>Dlt Data Storage Checkpoints</li> <li>Databricks Secret Scopes</li> <li>Databricks Controlplane Dataplane</li> <li>Databricks Dlt Code Walkthrough</li> <li>Databricks Serverless Compute</li> <li>Databricks Warehouses</li> <li>Databricks Lakehouse Federation</li> <li>Databricks Metrics Views</li> <li>Databricks Streaming Materialized Views Sql</li> <li>Databricks Cli Setup</li> <li>Index</li> </ol>"},{"location":"#docs-deep-dive","title":"Docs-deep-dive","text":""},{"location":"#databricks_1","title":"Databricks","text":"<ol> <li>What Is Lakehouse</li> <li>Lakehouse Vs Delta Lake Vs Warehouse</li> <li>All Delta Things Databricks</li> <li>High Level Architecture</li> <li>Databricks Acid Guarantees</li> <li>Databricks Medallion Architecture</li> <li>Databricks Single Source Of Truth Arch</li> <li>Databricks Scope Of Lakehouse Arch</li> <li>Databricks Architecture Guiding Principles</li> <li>Databricks Objects Catalogs</li> <li>Databricks Objects Volumes Tables</li> <li>Databricks Views</li> <li>Databricks Governed Tags</li> <li>Databricks Connecting To Cloud Object Storage Intro</li> <li>Databricks Managed Storage Location Hierarchy</li> </ol>"},{"location":"#scenarios","title":"Scenarios","text":"<ol> <li>Index</li> </ol>"},{"location":"#adf","title":"Adf","text":"<ol> <li>Architectures</li> </ol>"},{"location":"#databricks_2","title":"Databricks","text":"<ol> <li>Aws Reference Arch Databricks</li> <li>Reference Architectures Pt1</li> <li>Index</li> </ol>"},{"location":"#kafka","title":"Kafka","text":"<ol> <li>Why Closed Segments Files Open</li> <li>How Does Producer Guarantee Exactly Once</li> <li>Does Seq No Remain Same After Producer Goes Down</li> <li>What Happens When Reelection Happens</li> <li>Give Walkthrough Of Leader Epoch Log Truncation</li> <li>Explain Diff Dirtyratio Dirtybackgroundratio</li> <li>Are Kafka Consumers Thread Safe</li> <li>Is Retention Ms Defined Partition Level</li> <li>Difference Btwn Sticky Cooperative Sticky Assignor</li> <li>How Does Kafka Ensure Partial Idempotence</li> <li>Index</li> </ol>"},{"location":"#spark","title":"Spark","text":"<ol> <li>Smj Spill To Disk Q1</li> <li>Smj Spill To Disk Q2</li> <li>Smj Output During Spill Q3</li> <li>Cross Vs Broadcast Join</li> <li>Index</li> </ol>"},{"location":"#spark_1","title":"Spark","text":"<ol> <li>Spark Architecture Yarn</li> <li>Spark Driver Oom</li> <li>Types Of Memory Spark</li> <li>Spark Dynamic Partition Pruning</li> <li>Spark Salting Technique</li> <li>What Is Spark</li> <li>Why Apache Spark</li> <li>Hadoop Vs Spark</li> <li>Spark Ecosystem</li> <li>Spark Ecosystem</li> <li>Spark Architecture</li> <li>Schema In Spark</li> <li>Handling Corrupt Records Spark</li> <li>Spark Transformations Actions</li> <li>Spark Dag Lazy Eval</li> <li>Spark Json Data</li> <li>Spark Sql Engine</li> <li>Spark Rdd</li> <li>Spark Writing Data Disk</li> <li>Spark Partitioning Bucketing</li> <li>Spark Session Vs Context</li> <li>Spark Job Stage Task</li> <li>Spark Transformations</li> <li>Spark Union Vs Unionall</li> <li>Spark Repartition Vs Coalesce</li> <li>Spark Case When</li> <li>Spark Unique Sorted Records</li> <li>Spark Agg Functions</li> <li>Spark Group By</li> <li>Spark Joins Intro</li> <li>Spark Join Strategies</li> <li>Spark Window Functions</li> <li>Spark Memory Management</li> <li>Spark Executor Oom</li> <li>Spark Submit Command</li> <li>Spark Deployment Modes</li> <li>Spark Adaptive Query Execution</li> <li>Spark Dynamic Resource Allocation</li> <li>Spark Dynamic Partition Pruning</li> <li>Index</li> </ol>"},{"location":"#streaming","title":"Streaming","text":"<ol> <li>Index</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<ol> <li>Use Cases Streaming</li> <li>Redpanda Vs Kafka Arch Differences</li> <li>Redpanda Architure In Depth Pt1</li> <li>Index</li> </ol>"},{"location":"#kafka_1","title":"Kafka","text":"<ol> <li>Kafka Kraft Setup</li> <li>Kafka Broker Properties</li> <li>Topic Default Properties</li> <li>Kafka Hardware Considerations</li> <li>Kafka Configuring Clusters Broker Consideration</li> <li>Kafka Broker Os Tuning</li> <li>Kafka Os Tuning Dirty Page Handling</li> <li>Kafka File Descriptors Overcommit Memory</li> <li>Kafka Production Concerns</li> <li>Kafka Message Types</li> <li>Kafka Configuring Producers Pt1</li> <li>Kafka Configuring Producers Pt2</li> <li>Kafka Serializers Avro Pt1</li> <li>Kafka Serializers Avro Pt2</li> <li>Kafka Partitions</li> <li>Kafka Headers</li> <li>Kafka Interceptors</li> <li>Kafka Quotas And Throttling</li> <li>Kafka Consumer Eager And Cooperative Rebalance</li> <li>Kafka Consumer Static Partitioning</li> <li>Kafka Poll Loop</li> <li>Kafka Configuring Consumers Pt1</li> <li>Kafka Configuring Consumers Pt2</li> <li>Kafka Partition Assignment Strategies</li> <li>Kafka Commits Offsets Intro</li> <li>Kafka Types Of Commits</li> <li>Kafka Rebalance Listeners</li> <li>Kafka Consuming Records With Spec Offset</li> <li>Kafka Exiting Consumer Poll Loop</li> <li>Kafka Deserialisers</li> <li>Kafka Standalone Consumers</li> <li>Kafka Internals Zookeeper</li> <li>Kafka Raft Consensus Protocol</li> <li>Kafka Controller Quorum</li> <li>Kafka Replication Concepts</li> <li>Kafka Insync Outofsync Replicas</li> <li>Kafka Request Processing Pt1</li> <li>Kafka Request Processing Pt2 Produce Requests</li> <li>Kafka Fetch Requests Pt1</li> <li>Kafka Fetch Requests Pt2</li> <li>Kafka Physical Storage Introduction</li> <li>Kafka Tiered Storage</li> <li>Kafka Partition Allocation</li> <li>Kafka File Formats Intro</li> <li>Kafka Message Batch Headers</li> <li>Kafka Indexes</li> <li>Kafka Compaction</li> <li>Kafka Tombstoning Records</li> <li>Kafka Reliability Guarantees</li> <li>Kafka Replication Procedures</li> <li>Kafka Broker Config Replication Factor</li> <li>Kafka Broker Configuration Unclean Leader Election</li> <li>Kafka Log Truncation On Out Of Sync Leader</li> <li>Kafka Keeping Replicas In Sync</li> <li>Kafka Using Producers Reliable System Scenarios</li> <li>Index</li> </ol>"},{"location":"azure/","title":"Azure","text":"<p>This is the overview page for Azure.</p>"},{"location":"azure/00-Azure_Integration_Databricks/","title":"Azure Integration Databricks","text":""},{"location":"azure/00-Azure_Integration_Databricks/#azure-integration-with-databricks","title":"Azure Integration with Databricks","text":""},{"location":"azure/00-Azure_Integration_Databricks/#managed-resource-group","title":"Managed Resource Group","text":"<p>We can see:</p> <ul> <li>Identity</li> <li>Managed Storage Account : DBFS is stored here.</li> <li>Access Connector : Used to connect to storage account from databricks.</li> </ul> <p>Blob Containers inside the storage account</p> <p></p>"},{"location":"azure/00-Azure_Integration_Databricks/#creating-compute-in-databricks","title":"Creating Compute in Databricks","text":"<p>When we create compute in databricks we can see VM created. One VM for each driver and worker node</p> <p></p> <p>When compute is terminated the VM is deleted.</p>"},{"location":"azure/00-Azure_Integration_Databricks/#unity-catalog","title":"Unity Catalog","text":"<ul> <li> <p>Metadata is stored in control plane and data is stored in data plane. One region is adviced to have single metastore.</p> </li> <li> <p>Only catalog is securable object, others are non securable.</p> </li> </ul> <p></p> <p>Location in Managed Tables</p> <p></p>"},{"location":"azure/00-Azure_Integration_Databricks/#detailed-steps-to-setup","title":"Detailed Steps to Setup","text":""},{"location":"azure/00-Azure_Integration_Databricks/#1-create-an-azure-databricks-workspace","title":"1. Create an Azure Databricks Workspace","text":"<ul> <li>Go to the Azure Portal.</li> <li>Create a new resource \u2192 search Azure Databricks \u2192 click Create.</li> <li> <p>Fill in:</p> </li> <li> <p>Workspace name</p> </li> <li>Region</li> <li>Pricing tier (Standard, Premium, or Trial)</li> <li>Under Networking, choose whether to deploy into a VNet (VNet Injection) or let Databricks create a managed VNet.</li> <li>Deploy the workspace.</li> </ul>"},{"location":"azure/00-Azure_Integration_Databricks/#2-understand-the-storage-setup","title":"2. Understand the Storage Setup","text":"<p>When you create a Databricks workspace, Azure automatically provisions:</p> <ul> <li> <p>Managed Storage Account:</p> </li> <li> <p>A hidden, Microsoft-managed storage account that Databricks uses for workspace metadata (notebooks, cluster logs, ML models, job configs).</p> </li> <li>You don\u2019t manage or see this storage directly.</li> <li>This is different from your customer-managed storage account, where you store your actual data (e.g., in ADLS Gen2, Blob).</li> </ul> <p>So you\u2019ll normally integrate Databricks with your own Azure Data Lake Storage (ADLS Gen2) for raw and processed data.</p>"},{"location":"azure/00-Azure_Integration_Databricks/#3-grant-access-using-an-access-connector","title":"3. Grant Access Using an Access Connector","text":"<p>To allow Databricks to access your storage securely:</p> <ol> <li> <p>Create a resource called Azure Databricks Access Connector.</p> </li> <li> <p>This acts like a bridge between Databricks and Azure services.</p> </li> <li>It\u2019s assigned a Managed Identity (system-assigned).</li> <li> <p>Assign RBAC roles to this Access Connector on your storage account.    Example:</p> </li> <li> <p><code>Storage Blob Data Contributor</code> \u2192 read/write data in ADLS Gen2.</p> </li> <li><code>Storage Blob Data Reader</code> \u2192 read-only.</li> <li>Go to your Databricks workspace \u2192 Advanced Settings \u2192 Access Connector \u2192 attach the Access Connector you created.</li> </ol>"},{"location":"azure/00-Azure_Integration_Databricks/#4-use-managed-identity-in-databricks","title":"4. Use Managed Identity in Databricks","text":"<ul> <li>The Access Connector\u2019s managed identity is used by Databricks clusters and jobs to authenticate to Azure services without secrets.</li> <li> <p>Benefits:</p> </li> <li> <p>No need to store SAS tokens, keys, or service principal secrets in Databricks.</p> </li> <li>Authentication happens via Azure AD automatically.</li> <li>When you mount ADLS or connect to other services (like Key Vault, Synapse, Event Hub), Databricks uses this managed identity.</li> </ul> <p>Example for mounting ADLS with managed identity:</p> <pre><code>spark.conf.set(\"fs.azure.account.auth.type.&lt;storage_account&gt;.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.&lt;storage_account&gt;.dfs.core.windows.net\",\n               \"org.apache.hadoop.fs.azurebfs.oauth2.ManagedIdentityTokenProvider\")\n\ndf = spark.read.format(\"parquet\").load(\"abfss://container@storage_account.dfs.core.windows.net/data/\")\n</code></pre>"},{"location":"azure/00-Azure_Integration_Databricks/#5-typical-flow-in-production","title":"5. Typical Flow in Production","text":"<ol> <li>Create Databricks workspace \u2192 Managed storage account provisioned automatically.</li> <li>Create Access Connector \u2192 acts as Databricks\u2019 identity in Azure.</li> <li>Assign roles on your ADLS storage account to Access Connector\u2019s identity.</li> <li>Enable Access Connector in Databricks workspace.</li> <li>Access ADLS data from notebooks or jobs using managed identity authentication.</li> </ol>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/","title":"Azure Portal Subscriptions Resourcegroups","text":""},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#azure-portal-subscriptions-and-resource-groups","title":"Azure Portal, Subscriptions and Resource Groups","text":""},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#resource-groups","title":"Resource Groups","text":"<p>In the process of working with Azure, you quickly encounter many different resources connected to it, for example, virtual machines, storage accounts, and databases. When you have such a large number of resources in Azure, it becomes challenging to track the various ones in use or simply keep track of them in general. That is where Resource Group in Azure comes in.</p> <p>A Resource Group in Azure is a method of categorizing or bringing related resources under a similar group in the Azure platform. Similar to how a folder is used to store all the relevant documents required to complete a certain task or work on an application. This allows resources to be easily managed, monitored, and protected due to their central location as well as being easier to group.</p> <p>The resource group can be all the resources for the solution or only discrete resources that you want to create as a group. The resource group in Azure scope is also used throughout the Azure portal to build views that contain cross-resource information. For example:</p> <p>Metrics Blade</p> <p></p> <p>Deployments Blade</p> <p></p> <p>Policy and Compliance Blade</p> <p></p> <p>Diagnostics Blade to send resource group activity to one data storage</p> <p></p>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#advantages","title":"Advantages","text":"<p>Logical Grouping: Resources in the Azure resource group can be related as they are based on a common parameter like requirements for a particular application or service. By arranging resources in such a manner, there is easy organization and management of resources to help achieve set objectives.</p> <p>Deployment Management: You can deploy and manage resources all at once in a resource group in Azure if needed. This is especially helpful when applying large solutions in organizations with numerous dependencies between resources.</p> <p>Access Control: Azure Resource groups enable you to implement Role-Based Access Control that is RBAC. This entails that you get to decide who is allowed to update, or even use, certain resources that are enclosed in a given group.</p> <p>Resource Lifecycle Management: Resource groups in Azure can also be applied to control the resources\u2019 life cycle. All resources within the Azure resource groups can be deployed, updated or deleted in a single attempt as a group.</p> <p>Cost Management: When such resources are disaggregated you can easily manage your costs since they are frequently related. Azure also has features that allow displaying consumption and costs for every created resource group if it is a concern for the user.</p> <p>\ud83d\udd39 Azure Hierarchy: Resources \u2192 Resource Groups \u2192 Subscriptions</p>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#1-azure-resources","title":"1. Azure Resources","text":"<ul> <li>Definition: The actual services you create/use in Azure.</li> <li> <p>Examples:</p> </li> <li> <p>A Storage Account (ADLS Gen2)</p> </li> <li>A SQL Database</li> <li>A Virtual Machine (VM)</li> <li>A Synapse Workspace</li> <li>Key point: These are the building blocks. Everything you deploy in Azure is a resource.</li> </ul>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#2-resource-groups-rg","title":"2. Resource Groups (RG)","text":"<ul> <li>Definition: A logical container that holds related Azure resources.</li> <li> <p>Purpose:</p> </li> <li> <p>Organize resources (by project, department, environment).</p> </li> <li>Apply RBAC (access control) at the group level.</li> <li>Apply tags for cost management.</li> <li>Manage lifecycle (delete the RG \u2192 all resources inside are deleted).</li> <li> <p>Example:</p> </li> <li> <p>RG: <code>RetailAnalytics-Dev-RG</code></p> <ul> <li> <p>Resources inside:</p> </li> <li> <p><code>RetailADLS</code> (Storage Account)</p> </li> <li><code>RetailSQLDB</code> (Azure SQL DB)</li> <li><code>RetailADF</code> (Data Factory)</li> </ul> </li> </ul>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#3-subscriptions","title":"3. Subscriptions","text":"<ul> <li>Definition: The billing boundary in Azure. It defines how you pay and how access is controlled.</li> <li> <p>Purpose:</p> </li> <li> <p>Groups resource groups + resources under one billing account.</p> </li> <li>Has spending limits, quotas, and policies.</li> <li>Tied to an Azure Active Directory tenant.</li> <li> <p>Examples:</p> </li> <li> <p><code>Pay-As-You-Go Subscription</code></p> </li> <li><code>Free Trial Subscription</code></li> <li><code>Enterprise Agreement Subscription</code> (corporate)</li> </ul> <p>\ud83d\udd39 Hierarchy Diagram</p> <pre><code>Subscription (Billing boundary, access policies)\n\u2502\n\u251c\u2500\u2500 Resource Group 1 (Logical container)\n\u2502   \u251c\u2500\u2500 Resource: Azure Data Lake Storage\n\u2502   \u251c\u2500\u2500 Resource: Azure SQL Database\n\u2502   \u2514\u2500\u2500 Resource: Azure Data Factory\n\u2502\n\u2514\u2500\u2500 Resource Group 2\n    \u251c\u2500\u2500 Resource: Event Hub\n    \u251c\u2500\u2500 Resource: Synapse Analytics\n    \u2514\u2500\u2500 Resource: Key Vault\n</code></pre> <p>\ud83d\udd39 Real-Life Analogy</p> <ul> <li>Subscription = A house lease contract (defines who pays the bills, how much you can use).</li> <li>Resource Group = A room in the house (you organize furniture/resources here).</li> <li>Resource = A piece of furniture (bed, desk, chair \u2192 SQL DB, Storage, ADF).</li> </ul> <p>\ud83d\udd39 Interview Cheat Sheet</p> <p>Q1. What is the difference between a Resource and a Resource Group?</p> <ul> <li>A resource is the actual service (e.g., Storage, SQL DB).</li> <li>A resource group is a logical container for related resources.</li> </ul> <p>Q2. Can a resource exist in multiple resource groups?</p> <ul> <li>\u274c No, a resource belongs to only one resource group.</li> <li>But you can move it to another RG (with limitations).</li> </ul> <p>Q3. What is the difference between a Subscription and a Resource Group?</p> <ul> <li>Subscription = billing &amp; access boundary.</li> <li>Resource Group = logical container inside a subscription for resources.</li> </ul> <p>Q4. Can a resource group span multiple subscriptions?</p> <ul> <li>\u274c No, a resource group belongs to exactly one subscription.</li> </ul> <p>Q5. Why do we need multiple subscriptions?</p> <ul> <li>To separate environments (Dev/Test/Prod), billing accounts, or departments.</li> </ul> <p>\u2705 Quick memory hook: Resource \u2192 The \u201cwhat\u201d (service). Resource Group \u2192 The \u201cwhere\u201d (container). Subscription \u2192 The \u201cwho pays\u201d (billing).</p>"},{"location":"azure/01-Azure_Portal_Subscriptions_ResourceGroups/#management-groups","title":"Management Groups","text":"<p>\ud83c\udf10 What are Azure Management Groups?</p> <p>Management Groups in Azure are a way to organize multiple subscriptions into a hierarchy.</p> <p>Think of them as folders in a file system where:</p> <ul> <li>Management Groups = Folders</li> <li>Subscriptions = Files inside those folders</li> <li>Resources = Data inside those files</li> </ul> <p>They allow you to apply governance, policies, and RBAC access across many subscriptions at once.</p> <pre><code>Root Management Group\n \u251c\u2500\u2500 Corp-IT (Management Group)\n \u2502     \u251c\u2500\u2500 Subscription A (Prod)\n \u2502     \u2514\u2500\u2500 Subscription B (Dev)\n \u251c\u2500\u2500 Corp-Finance (Management Group)\n \u2502     \u2514\u2500\u2500 Subscription C (Finance Dept)\n \u2514\u2500\u2500 Corp-Analytics (Management Group)\n       \u2514\u2500\u2500 Subscription D (Data Science)\n</code></pre> <ul> <li>You apply policies (like \"only deploy resources in East US\") at Corp-IT, and it flows down to all subscriptions in that group.</li> <li>RBAC roles assigned at higher-level management groups inherit down.</li> </ul> <p>\u2728 Key Features</p> <ul> <li>Hierarchy up to 6 levels deep (excluding root &amp; subscription).</li> <li>Root Management Group is automatically created for every Azure AD tenant.</li> <li>Policy &amp; RBAC inheritance: Apply once \u2192 affects all child subscriptions/resources.</li> <li>Segregation: You can separate departments, environments (Dev/QA/Prod), or business units.</li> <li>Scalability: Essential for large organizations managing 100s of subscriptions.</li> </ul> <p>\ud83d\udd11 Use Cases</p> <ul> <li>Apply Azure Policies (e.g., only allow specific VM SKUs, enforce tagging).</li> <li>Apply RBAC roles (e.g., Finance team can only access Finance subscriptions).</li> <li>Enforce security baselines across multiple subscriptions.</li> <li>Manage costs by grouping subscriptions by business unit.</li> </ul>"},{"location":"azure/02-Azure_CLI_Scenarios/","title":"Azure Cli Scenarios","text":""},{"location":"azure/02-Azure_CLI_Scenarios/#azure-cli-scenarios","title":"Azure CLI Scenarios","text":""},{"location":"azure/02-Azure_CLI_Scenarios/#exercise-1-setup-resource-groups","title":"\ud83d\udd39 Exercise 1: Setup &amp; Resource Groups","text":"<p>Login to Azure CLI</p> <pre><code>az login\n</code></pre> <p>Set your active subscription</p> <pre><code>az account list --output table\naz account set --subscription \"&lt;your-subscription-id&gt;\"\n</code></pre> <p>Create a resource group</p> <pre><code>az group create --name MyRG --location eastus\n</code></pre> <p>Verify it</p> <pre><code>az group list --output table\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#exercise-2-storage-account-blob","title":"\ud83d\udd39 Exercise 2: Storage Account + Blob","text":"<p>Create a storage account <pre><code>az storage account create \\\n  --name mystorage123xyz --resource-group MyRG \\\n  --location eastus --sku Standard_LRS --kind StorageV2\n</code></pre></p> <p>Create a container</p> <pre><code>az storage container create \\\n  --account-name mystorage123xyz --name rawdata\n</code></pre> <p>Upload a CSV file</p> <pre><code>echo \"id,name,age\n1,Alice,30\n2,Bob,25\n3,Charlie,28\" &gt; sample.csv\n</code></pre> <pre><code>az storage blob upload --account-name storageaccountfromclivb \\\n--container-name clidatacontainer --name sample.csv --file ./sample.csv\n</code></pre> <p>List blobs</p> <pre><code>az storage blob list \\\n  --account-name storageaccountfromclivb --container-name clidatacontainer \\\n  --output table\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#exercise-3-sql-database","title":"\ud83d\udd39 Exercise 3: SQL Database","text":"<p>Create a SQL server</p> <pre><code>az sql server create \\\n  --name my-sql-server123 --resource-group MyRG \\\n  --location eastus --admin-user myadmin --admin-password MyP@ssw0rd!\n</code></pre> <p>Create a database</p> <pre><code>az sql db create \\\n  --resource-group MyRG --server my-sql-server123 \\\n  --name mydb --service-objective S0\n</code></pre> <p>Show details</p> <pre><code>az sql db show --resource-group MyRG --server my-sql-server123 --name mydb\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#exercise-4-virtual-machine","title":"\ud83d\udd39 Exercise 4: Virtual Machine","text":"<p>Create a Linux VM</p> <pre><code>az vm create \\\n  --resource-group MyRG --name MyVM \\\n  --image UbuntuLTS --admin-username azureuser --generate-ssh-keys\n</code></pre> <p>Check running VMs</p> <pre><code>az vm list --output table\n</code></pre> <p>Stop the VM</p> <pre><code>az vm stop --resource-group MyRG --name MyVM\n</code></pre> <p>Start it again</p> <pre><code>az vm start --resource-group MyRG --name MyVM\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#exercise-5-build-a-mini-data-lake-pipeline","title":"\ud83d\udd39 Exercise 5: Build a Mini Data Lake Pipeline","text":"<p>Goal: Create a Data Lake storage, upload raw data, then use queries to check it.</p> <ol> <li>Create a storage account with hierarchical namespace (Data Lake Gen2):</li> </ol> <pre><code>az storage account create \\\n  --name mydatalake123 --resource-group MyRG \\\n  --location eastus --sku Standard_LRS --kind StorageV2 \\\n  --hierarchical-namespace true\n</code></pre> <p>Create a container for raw data:</p> <pre><code>az storage container create \\\n  --account-name mydatalake123 --name raw\n</code></pre> <p>Upload multiple files (CSV, JSON):</p> <pre><code>for file in ./data/*; do\n  az storage blob upload \\\n    --account-name mydatalake123 \\\n    --container-name raw \\\n    --name $(basename $file) \\\n    --file $file\ndone\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#basename-file","title":"<code>basename $file</code>","text":"<ul> <li><code>basename</code> is a Unix command.</li> <li>It strips the directory path from a file and returns only the filename.</li> <li>Example:</li> </ul> <pre><code>file=\"/home/user/docs/report.pdf\"\nbasename $file\n</code></pre> <p>Output:</p> <pre><code>report.pdf\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#-name-basename-file","title":"<code>--name $(basename $file)</code>","text":"<ul> <li>The <code>$(...)</code> syntax means command substitution: run the command inside and replace it with its output.</li> <li>So <code>$(basename $file)</code> gets replaced by the filename (without path).</li> <li>If <code>file=\"/home/user/docs/report.pdf\"</code>, then:</li> </ul> <pre><code>--name $(basename $file)\n</code></pre> <p>becomes:</p> <pre><code>--name report.pdf\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#why-its-used","title":"Why it\u2019s used","text":"<p>Often, scripts loop through files in directories. Instead of passing full paths, they just want the filename for naming a resource, argument, or output.</p> <p>Example in Azure CLI:</p> <pre><code>for file in /path/to/files/*; do\n  az storage blob upload \\\n    --account-name mystorage \\\n    --container-name mycontainer \\\n    --file $file \\\n    --name $(basename $file)\ndone\n</code></pre> <p>Here:</p> <ul> <li><code>--file $file</code> \u2192 full path to the file.</li> <li><code>--name $(basename $file)</code> \u2192 just the filename in the blob storage.</li> </ul> <p>Verify using query:</p> <pre><code>az storage blob list \\\n  --account-name mydatalake123 --container-name raw \\\n  --query \"[].{Name:name, Size:properties.contentLength}\" --output table\n</code></pre> <p>That syntax is from the Azure CLI, and specifically it uses JMESPath (a JSON query language) to filter and reshape JSON output from Azure commands.</p> <p>Let\u2019s break it down:</p>"},{"location":"azure/02-Azure_CLI_Scenarios/#the-context","title":"The context","text":"<p>Most <code>az</code> CLI commands return JSON objects. For example, listing blobs:</p> <pre><code>az storage blob list \\\n  --account-name mystorage \\\n  --container-name mycontainer \\\n  --output json\n</code></pre> <p>Might return something like:</p> <pre><code>[\n  {\n    \"name\": \"file1.txt\",\n    \"properties\": {\n      \"contentLength\": 1234,\n      \"contentType\": \"text/plain\"\n    }\n  },\n  {\n    \"name\": \"file2.csv\",\n    \"properties\": {\n      \"contentLength\": 5678,\n      \"contentType\": \"text/csv\"\n    }\n  }\n]\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#the-query-explained","title":"The query explained","text":"<p><code>--query \"[].{Name:name, Size:properties.contentLength}\"</code></p> <ol> <li> <p><code>[]</code></p> </li> <li> <p>Means \"go through each item in the JSON array\".</p> </li> <li> <p><code>{Name:name, Size:properties.contentLength}</code></p> </li> <li> <p>Reshapes each object to keep only:</p> <ul> <li><code>Name</code> \u2192 mapped from the field <code>name</code>.</li> <li><code>Size</code> \u2192 mapped from nested field <code>properties.contentLength</code>.</li> </ul> </li> </ol> <p>So the output would look like:</p> <pre><code>[\n  {\n    \"Name\": \"file1.txt\",\n    \"Size\": 1234\n  },\n  {\n    \"Name\": \"file2.csv\",\n    \"Size\": 5678\n  }\n]\n</code></pre>"},{"location":"azure/02-Azure_CLI_Scenarios/#why-its-useful","title":"Why it\u2019s useful","text":"<ul> <li>Avoids huge JSON output.</li> <li>Lets you extract just the fields you care about.</li> <li>Works with <code>--output table</code> for nice tabular summaries:</li> </ul> <pre><code>az storage blob list \\\n  --account-name mystorage \\\n  --container-name mycontainer \\\n  --query \"[].{Name:name, Size:properties.contentLength}\" \\\n  --output table\n</code></pre> <p>Gives:</p> <pre><code>Name       Size\n---------  -----\nfile1.txt  1234\nfile2.csv  5678\n</code></pre>"},{"location":"azure/03-Azure_Powershell_Scenarios/","title":"Azure Powershell Scenarios","text":""},{"location":"azure/03-Azure_Powershell_Scenarios/#azure-powershell-basics","title":"\u26a1 Azure PowerShell Basics","text":""},{"location":"azure/03-Azure_Powershell_Scenarios/#1-install-setup","title":"1. Install &amp; Setup","text":"<pre><code># Install the Az module (latest version)\nInstall-Module -Name Az -Scope CurrentUser -Repository PSGallery -Force\n\n# Import the module\nImport-Module Az\n\n# Login\nConnect-AzAccount\n\n# Check current subscription\nGet-AzContext\n\n# Switch subscription\nSet-AzContext -Subscription \"SUBSCRIPTION-ID\"\n</code></pre>"},{"location":"azure/03-Azure_Powershell_Scenarios/#scenario-1-create-resource-groups-for-data-projects","title":"\ud83d\udd39 Scenario 1: Create Resource Groups for Data Projects","text":"<pre><code># Create resource groups for different environments\nNew-AzResourceGroup -Name DataRG-Dev -Location eastus\nNew-AzResourceGroup -Name DataRG-Test -Location eastus\nNew-AzResourceGroup -Name DataRG-Prod -Location eastus\n</code></pre>"},{"location":"azure/03-Azure_Powershell_Scenarios/#scenario-2-deploy-a-data-lake-storage-account","title":"\ud83d\udd39 Scenario 2: Deploy a Data Lake Storage Account","text":"<pre><code># Create a Data Lake Gen2 Storage Account\nNew-AzStorageAccount `\n  -ResourceGroupName DataRG-Dev `\n  -Name datalakeps123 `\n  -Location eastus `\n  -SkuName Standard_LRS `\n  -Kind StorageV2 `\n  -EnableHierarchicalNamespace $true\n</code></pre>"},{"location":"azure/03-Azure_Powershell_Scenarios/#scenario-3-upload-raw-data-to-data-lake","title":"\ud83d\udd39 Scenario 3: Upload Raw Data to Data Lake","text":"<pre><code># Connect to storage account\n$ctx = New-AzStorageContext -StorageAccountName datalakeps123 -UseConnectedAccount\n\n# Create container\nNew-AzStorageContainer -Name raw -Context $ctx\n\n# Upload file\nSet-AzStorageBlobContent -File \"./sales.csv\" -Container raw -Blob \"sales.csv\" -Context $ctx\n</code></pre>"},{"location":"azure/03-Azure_Powershell_Scenarios/#scenario-5-secure-secrets-in-key-vault","title":"\ud83d\udd39 Scenario 5: Secure Secrets in Key Vault","text":"<pre><code># Create Key Vault\nNew-AzKeyVault -Name MyVault123 -ResourceGroupName DataRG-Dev -Location eastus\n\n# Store SQL password\n$secret = ConvertTo-SecureString \"StrongP@ssword123!\" -AsPlainText -Force\nSet-AzKeyVaultSecret -VaultName MyVault123 -Name SqlPassword -SecretValue $secret\n</code></pre>"},{"location":"azure/05-Azure_ARM_Templates/","title":"Azure Arm Templates","text":"<p>\ud83d\udccc What are ARM Templates?</p> <p>ARM (Azure Resource Manager) templates are JSON files that define the infrastructure and configuration you want to deploy in Azure.</p> <p>Think of them as Infrastructure as Code (IaC) for Azure. Instead of manually creating resources in the portal or using CLI/PowerShell, you write a template once and deploy it repeatedly.</p> <p>\ud83d\udd39 Key Points</p> <p>Format: JSON file (.json)</p> <ul> <li> <p>Declarative: You describe what you want (VM, Storage, SQL, etc.), Azure figures out how to create it.</p> </li> <li> <p>Idempotent: You can deploy the same template multiple times, Azure won\u2019t create duplicates \u2013 it ensures the desired state.</p> </li> <li> <p>Repeatable: Use the same template for dev, test, prod environments.</p> </li> <li> <p>Automation: Works well with CI/CD (e.g., GitHub Actions, Azure DevOps).</p> </li> </ul> <p>Custom Template to Create Storage Account</p> <pre><code>@description('Unique Name for Storage Account')\nparam storageAccountName string\n\n@description('Azure Region for Deployment')\nparam location string = 'eastus2'\n\n@allowed([\n  'Standard_LRS'\n  'Standard_GRS'\n  'Standard_ZRS'\n  'Premium_LRS'\n])\n@description('Performance and replication type.')\nparam skuName string = 'Standard_LRS'\n\n// Resource: Storage Account\nresource storageAccount 'Microsoft.Storage/storageAccounts@2025-06-01' = {\n  name: storageAccountName\n  location: location\n  sku: {\n    name: skuName\n  }\n  kind: 'StorageV2'\n  properties: {\n    accessTier: 'Hot'\n    isHnsEnabled: true  // Hierarchical namespace for ADLS Gen2\n    networkRuleSet: {\n      defaultAction: 'Deny'   // Block public access\n      bypass: [\n        'AzureServices'\n      ]\n    }\n  }\n}\n</code></pre> <p></p> <p></p>"},{"location":"azure/05-Azure_ARM_Templates/#deploying-using-az","title":"Deploying using az","text":"<pre><code>az deployment group create \\\n  --resource-group MyResourceGroup \\\n  --template-file storage-gen2.json \\\n  --parameters storageAccountName=mystorageacctgen2\n</code></pre> <p>This is an Azure Resource Manager (ARM) template. It defines how to deploy a Storage Account in Azure. Let me walk you through every section in depth:</p> <p>Okay, let me explain your fixed Bicep template in a simple way, step by step:</p>"},{"location":"azure/05-Azure_ARM_Templates/#1-parameters","title":"1. Parameters","text":"<p>Parameters let you pass values when you deploy, so the template is reusable.</p> <ul> <li>storageAccountName \u2192 The unique name of the storage account.</li> <li>location \u2192 The Azure region (default is <code>eastus2</code>).</li> <li> <p>skuName \u2192 The storage account\u2019s performance/replication option. Allowed values are:</p> </li> <li> <p><code>Standard_LRS</code> \u2192 locally redundant</p> </li> <li><code>Standard_GRS</code> \u2192 geo-redundant</li> <li><code>Standard_ZRS</code> \u2192 zone redundant</li> <li><code>Premium_LRS</code> \u2192 premium performance</li> </ul>"},{"location":"azure/05-Azure_ARM_Templates/#2-resource-definition","title":"2. Resource Definition","text":"<p>This creates the storage account itself.</p> <pre><code>resource storageAccount 'Microsoft.Storage/storageAccounts@2022-01-01' = {\n</code></pre> <ul> <li>Tells Azure: \u201cI want to create a storage account using API version 2022-01-01.\u201d</li> </ul>"},{"location":"azure/05-Azure_ARM_Templates/#3-storage-account-settings","title":"3. Storage Account Settings","text":"<p>Inside the resource block, you configure:</p> <ul> <li>name \u2192 uses the <code>storageAccountName</code> parameter.</li> <li>location \u2192 uses the <code>location</code> parameter.</li> <li>sku \u2192 set to whatever you passed in <code>skuName</code>.</li> <li>kind: 'StorageV2' \u2192 makes it a modern storage account that supports all services.</li> <li> <p>properties:</p> </li> <li> <p><code>accessTier: 'Hot'</code> \u2192 assumes data is accessed frequently.</p> </li> <li><code>isHnsEnabled: true</code> \u2192 turns on Hierarchical Namespace, required for Data Lake Gen2.</li> <li> <p><code>networkRuleSet</code>:</p> <ul> <li><code>defaultAction: 'Deny'</code> \u2192 blocks public access by default.</li> <li><code>bypass: ['AzureServices']</code> \u2192 allows trusted Azure services (like Databricks, Synapse) to still connect.</li> </ul> </li> </ul>"},{"location":"azure/05-Azure_ARM_Templates/#4-what-happens-when-you-deploy","title":"4. What happens when you deploy","text":"<ul> <li>Azure will create a new StorageV2 account.</li> <li>It will use the name, location, and SKU you provided (or the defaults).</li> <li>The account will support Data Lake Gen2.</li> <li>Public access is blocked, but Azure services can still use it.</li> </ul>"},{"location":"azure/05-Azure_ARM_Templates/#in-one-line","title":"In one line:","text":"<p>This Bicep template deploys a secure StorageV2 account with ADLS Gen2 enabled, parameterized for name, location, and redundancy type, and blocks public access except for trusted Azure services.</p>"},{"location":"azure/06-Azure_Bicep_Templates/","title":"Azure Bicep Templates","text":""},{"location":"azure/06-Azure_Bicep_Templates/#bicep-templates-in-azure","title":"Bicep Templates in Azure","text":"<p>Bicep is the modern, cleaner alternative to ARM JSON templates. It\u2019s declarative, easier to read, and natively compiles to ARM templates.</p> <pre><code>// Parameters\nparam storageAccountName string {\n  metadata: {\n    description: 'Unique name for the storage account.'\n  }\n}\n\nparam location string = 'eastus' {\n  metadata: {\n    description: 'Azure region for deployment.'\n  }\n}\n\nparam skuName string = 'Standard_LRS' {\n  allowed: [\n    'Standard_LRS'\n    'Standard_GRS'\n    'Standard_ZRS'\n    'Premium_LRS'\n  ]\n  metadata: {\n    description: 'Performance and replication type.'\n  }\n}\n\n// Resource: Storage Account\nresource storageAccount 'Microsoft.Storage/storageAccounts@2022-09-01' = {\n  name: storageAccountName\n  location: location\n  sku: {\n    name: skuName\n  }\n  kind: 'StorageV2'\n  properties: {\n    accessTier: 'Hot'\n    isHnsEnabled: true  // Hierarchical namespace for ADLS Gen2\n    networkAcls: {\n      defaultAction: 'Deny'   // Block public access\n      bypass: 'AzureServices'\n    }\n  }\n}\n</code></pre>"},{"location":"azure/06-Azure_Bicep_Templates/#deployment","title":"Deployment","text":"<p><pre><code># Create resource group (if needed)\naz group create --name MyResourceGroup --location eastus\n\n# Deploy Bicep file\naz deployment group create \\\n  --resource-group MyResourceGroup \\\n  --template-file storage.bicep \\\n  --parameters storageAccountName=mystorageacctbicep\n</code></pre> // Output: Storage Account Resource ID output storageAccountId string = storageAccount.id</p>"},{"location":"azure/07-Overview_Of_Azure_Storage/","title":"Overview Of Azure Storage","text":""},{"location":"azure/07-Overview_Of_Azure_Storage/#overview-of-azure-storage","title":"Overview of Azure Storage","text":""},{"location":"azure/07-Overview_Of_Azure_Storage/#what-is-azure-storage","title":"\ud83d\udd39 What is Azure Storage?","text":"<p>Azure Storage is Microsoft\u2019s cloud-based storage platform, providing highly available, durable, and secure storage for blobs, files, queues, and tables.</p> <p>Think of it like a giant hard drive in the cloud, but with specialized \u201cdrawers\u201d for different types of data.</p>"},{"location":"azure/07-Overview_Of_Azure_Storage/#core-types-of-azure-storage","title":"\ud83d\udd39 Core Types of Azure Storage","text":"<ol> <li> <p>Blob Storage (Data Lake Gen2)</p> </li> <li> <p>Stores unstructured data: text, images, video, JSON, Parquet, CSV.</p> </li> <li>Supports HDFS-like hierarchical namespace (HNS) when enabled \u2192 required for Data Lake Gen2.</li> <li> <p>Used for:</p> <ul> <li>Data lakes (ETL, big data, analytics).</li> <li>Storing files for machine learning.</li> <li>Backups, archives.</li> <li>Example: <code>abfss://container@account.dfs.core.windows.net/</code></li> </ul> </li> </ol> <ol> <li> <p>File Storage (Azure Files)</p> </li> <li> <p>Fully managed file shares accessible via SMB/NFS.</p> </li> <li>Used when apps expect a network file share.</li> <li> <p>Example use cases:</p> <ul> <li>Lift-and-shift legacy apps that require shared drives.</li> <li>Store config files for apps running in Azure VMs or Kubernetes.</li> </ul> </li> </ol> <ol> <li> <p>Queue Storage</p> </li> <li> <p>Stores messages that applications can send and receive asynchronously.</p> </li> <li>Each message up to 64 KB.</li> <li>Used for decoupling applications (producer/consumer).</li> <li>Example: A web app puts a message in a queue, and a background worker picks it up for processing.</li> </ol> <ol> <li> <p>Table Storage (or Cosmos DB Table API)</p> </li> <li> <p>NoSQL key-value store.</p> </li> <li>Stores structured, non-relational data.</li> <li>Example: IoT telemetry, user profiles, metadata.</li> </ol>"},{"location":"azure/07-Overview_Of_Azure_Storage/#storage-account-types","title":"\ud83d\udd39 Storage Account Types","text":"<p>When you create a Storage Account in Azure, you\u2019re creating a top-level container for these services.</p> <p>Types:</p> <ol> <li>General-purpose v2 (GPv2) \u2192 recommended, supports blobs, files, queues, tables, Data Lake Gen2.</li> <li>Blob storage account \u2192 specialized for blobs.</li> <li>Premium storage \u2192 optimized for low-latency scenarios (SSD-backed).</li> </ol>"},{"location":"azure/07-Overview_Of_Azure_Storage/#key-features","title":"\ud83d\udd39 Key Features","text":"<ul> <li> <p>Redundancy (Replication):</p> </li> <li> <p>LRS (Locally redundant).</p> </li> <li>ZRS (Zone redundant).</li> <li>GRS (Geo-redundant).</li> <li>RA-GRS (Geo + Read access).</li> <li> <p>Security:</p> </li> <li> <p>RBAC + IAM.</p> </li> <li>SAS tokens (time-limited access).</li> <li>Integration with Azure AD (Managed Identities).</li> <li> <p>Lifecycle Management:</p> </li> <li> <p>Move data between hot \u2192 cool \u2192 archive tiers automatically.</p> </li> <li> <p>Encryption:</p> </li> <li> <p>All data encrypted at rest (Microsoft-managed or customer-managed keys).</p> </li> </ul>"},{"location":"azure/07-Overview_Of_Azure_Storage/#example-data-engineer-workflow","title":"\ud83d\udd39 Example: Data Engineer Workflow","text":"<p>Let\u2019s say you\u2019re building a pipeline with Databricks + Delta Lake:</p> <ol> <li>Create a Storage Account with HNS enabled (for ADLS Gen2).</li> <li>Create a container (<code>bronze</code>, <code>silver</code>, <code>gold</code>).</li> <li>Upload raw CSVs to <code>bronze</code>.</li> <li>Mount storage in Databricks (via Managed Identity + UC external location).</li> <li>Write cleaned Delta tables to <code>silver</code>.</li> <li>Curated aggregates go to <code>gold</code>.</li> </ol> <p>\ud83d\udc49 This setup is the lakehouse pattern.</p>"},{"location":"azure/07-Overview_Of_Azure_Storage/#quick-analogy","title":"\ud83d\udd39 Quick Analogy","text":"<ul> <li>Storage Account = the house.</li> <li>Container (Blob) = big room in the house.</li> <li>Folder = cabinet inside the room.</li> <li>File (Blob object) = actual book/document.</li> <li>RBAC/SAS = the keys to the house/room.</li> </ul>"},{"location":"azure/08-Blob_Storage_Fundamentals/","title":"\ud83d\udd39 What is Blob Storage?","text":"<p>Blob = Binary Large Object \u2192 any file (text, image, video, parquet, JSON, etc.) Azure Blob Storage is Microsoft\u2019s object storage solution for unstructured data.</p> <p>It\u2019s cheap, scalable, durable \u2192 you can store petabytes of data and pay only for what you use.</p>"},{"location":"azure/08-Blob_Storage_Fundamentals/#types-of-blobs","title":"\ud83d\udd39 Types of Blobs","text":"<p>Azure Blob storage supports 3 types:</p> <ol> <li> <p>Block Blob (most common)</p> </li> <li> <p>Optimized for streaming and storing files.</p> </li> <li>Stores data as blocks \u2192 you can upload in chunks.</li> <li> <p>Used for: documents, CSV, Parquet, images, logs.</p> </li> <li> <p>Append Blob</p> </li> <li> <p>Optimized for append operations.</p> </li> <li> <p>Great for logs \u2192 you can only add to the end, not modify existing content.</p> </li> <li> <p>Page Blob</p> </li> <li> <p>Optimized for random read/write.</p> </li> <li>Used for VM disks (VHD files).</li> </ol> <p>\ud83d\udc49 For Data Engineering / Delta Lake \u2192 you\u2019ll almost always use Block Blobs.</p>"},{"location":"azure/08-Blob_Storage_Fundamentals/#storage-account-containers","title":"\ud83d\udd39 Storage Account + Containers","text":"<ul> <li>A Storage Account = the root of your blob storage.</li> <li>Inside it, you create containers \u2192 logical groups of blobs.</li> <li>Inside containers, you can have folders (if Hierarchical Namespace is enabled = ADLS Gen2).</li> </ul> <p>Example path:</p> <pre><code>abfss://bronze@mydatalake.dfs.core.windows.net/2025/08/data.csv\n</code></pre> <p>Breakdown:</p> <ul> <li><code>abfss</code> \u2192 protocol for ADLS Gen2 secure access.</li> <li><code>bronze</code> \u2192 container.</li> <li><code>mydatalake</code> \u2192 storage account.</li> <li><code>dfs.core.windows.net</code> \u2192 ADLS Gen2 endpoint.</li> <li><code>2025/08/data.csv</code> \u2192 folder path + file.</li> </ul>"},{"location":"azure/08-Blob_Storage_Fundamentals/#access-tiers-cost-optimization","title":"\ud83d\udd39 Access Tiers (Cost Optimization)","text":"<p>Blob storage offers 3 main tiers:</p> <ol> <li>Hot \u2013 frequently accessed, higher cost per GB, lower access cost.</li> <li>Cool \u2013 infrequently accessed, cheaper storage, higher access charges.</li> <li>Archive \u2013 very cheap storage, but must be \u201crehydrated\u201d before use (hours).</li> </ol> <p>Example:</p> <ul> <li>Store last 30 days of logs in Hot.</li> <li>Move logs &gt; 30 days old to Cool.</li> <li>Move logs &gt; 1 year old to Archive.</li> </ul>"},{"location":"azure/08-Blob_Storage_Fundamentals/#security-access","title":"\ud83d\udd39 Security &amp; Access","text":"<ol> <li> <p>Authentication options:</p> </li> <li> <p>Azure AD (recommended) \u2192 RBAC roles, Managed Identity.</p> </li> <li>Shared Key (account key) \u2192 full access, risky.</li> <li> <p>SAS Tokens \u2192 temporary, limited access (e.g., read-only link valid for 1 hour).</p> </li> <li> <p>Authorization:</p> </li> <li> <p>RBAC roles:</p> <ul> <li><code>Storage Blob Data Reader</code> \u2192 read only.</li> <li><code>Storage Blob Data Contributor</code> \u2192 read/write.</li> <li><code>Storage Blob Data Owner</code> \u2192 full control.</li> </ul> </li> <li> <p>Networking:</p> </li> <li> <p>Private endpoints (VNet integration).</p> </li> <li>Firewalls + IP restrictions.</li> </ol>"},{"location":"azure/08-Blob_Storage_Fundamentals/#features-for-data-engineering","title":"\ud83d\udd39 Features for Data Engineering","text":"<ul> <li> <p>Hierarchical Namespace (HNS) \u2192 required for Data Lake Gen2.</p> </li> <li> <p>Allows directories + POSIX-like permissions.</p> </li> <li>Needed for Delta Lake + Databricks UC.</li> <li>Soft delete / versioning \u2192 recover accidentally deleted blobs.</li> <li>Lifecycle rules \u2192 auto-move data across tiers.</li> <li>Event Grid integration \u2192 trigger pipelines when new data arrives.</li> <li>Immutable blobs (WORM) \u2192 compliance, can\u2019t be modified/deleted.</li> </ul>"},{"location":"azure/08-Blob_Storage_Fundamentals/#example-scenario-etl-pipeline-with-blob-storage","title":"\ud83d\udd39 Example Scenario (ETL Pipeline with Blob Storage)","text":"<ol> <li>Raw CSV files land in <code>bronze</code> container.</li> <li>Azure Function + Event Grid detects new files.</li> <li>Data Factory (ADF) or Databricks picks up files \u2192 transforms \u2192 saves as Delta in <code>silver</code>.</li> <li>Aggregated tables saved in <code>gold</code>.</li> <li>Access controlled via Unity Catalog external location with Managed Identity.</li> </ol>"},{"location":"azure/08-Blob_Storage_Fundamentals/#quick-analogy","title":"\ud83d\udd39 Quick Analogy","text":"<ul> <li>Block Blob = Lego blocks (you can build files in chunks).</li> <li>Append Blob = notebook (you can only keep adding pages).</li> <li>Page Blob = hard disk (you can jump to any page and edit).</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/","title":"Azure Data Lake Gen2 Overview","text":""},{"location":"azure/09-ADLS_Gen2_Overview/#what-is-adls-gen2","title":"\ud83d\udd39 What is ADLS Gen2?","text":"<ul> <li>Azure Data Lake Storage Gen2 is Microsoft\u2019s enterprise-grade, big data storage service built on top of Azure Blob Storage.</li> <li>It combines the scalability, durability, and low cost of Blob Storage with a hierarchical namespace (folders &amp; files like a traditional file system).</li> <li>It\u2019s designed for analytics and big data workloads (Spark, Databricks, Synapse, HDInsight, etc.), while still being general-purpose storage.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#key-features","title":"\ud83d\udd39 Key Features","text":"<ol> <li> <p>Hierarchical Namespace (HNS)</p> </li> <li> <p>Unlike flat Blob Storage, ADLS Gen2 organizes data into directories and subdirectories.</p> </li> <li>Enables atomic file operations like rename and move at the directory/file level.</li> <li> <p>Reduces cost and complexity of working with files in analytics.</p> </li> <li> <p>Unified Storage</p> </li> <li> <p>Built on Blob Storage \u2192 same account, same data redundancy options, same durability.</p> </li> <li> <p>No need to maintain separate \u201cdata lake\u201d and \u201cblob\u201d accounts.</p> </li> <li> <p>Optimized for Big Data Analytics</p> </li> <li> <p>Works natively with Apache Hadoop (HDFS) APIs.</p> </li> <li> <p>Seamless integration with Azure Databricks, Synapse Analytics, HDInsight, Azure Data Factory.</p> </li> <li> <p>Security</p> </li> <li> <p>Supports Azure RBAC (Role-Based Access Control) and POSIX-like ACLs (Access Control Lists).</p> </li> <li>Fine-grained permissions down to folder/file level.</li> <li> <p>Integrated with Azure Active Directory (AAD) for authentication.</p> </li> <li> <p>Cost-Effective</p> </li> <li> <p>Pay-as-you-go pricing (like Blob).</p> </li> <li>Storage tiers (Hot, Cool, Archive) available.</li> <li> <p>Hierarchical namespace reduces overhead for analytics jobs (cheaper file operations).</p> </li> <li> <p>Scalability &amp; Performance</p> </li> <li> <p>Handles petabytes to exabytes of data.</p> </li> <li>Optimized throughput for parallel analytics jobs.</li> <li>Works with serverless and distributed compute engines.</li> </ol>"},{"location":"azure/09-ADLS_Gen2_Overview/#use-cases","title":"\ud83d\udd39 Use Cases","text":"<ul> <li>Data Lakes: Centralized storage for structured + semi-structured + unstructured data.</li> <li>Analytics: Source for Spark, Synapse, Databricks, HDInsight.</li> <li>Machine Learning: Storing training datasets and ML feature stores.</li> <li>ETL Pipelines: Staging raw \u2192 curated \u2192 consumable zones.</li> <li>Archival Storage: Retain large volumes of log/event data at low cost.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#adls-gen2-vs-blob-storage","title":"\ud83d\udd39 ADLS Gen2 vs Blob Storage","text":"Feature Blob Storage ADLS Gen2 Namespace Flat Hierarchical File operations Expensive (copy + delete) Atomic (rename/move) Security Azure RBAC only RBAC + POSIX ACLs Analytics integration Limited Optimized for big data APIs Blob REST APIs Blob APIs + HDFS-compatible APIs"},{"location":"azure/09-ADLS_Gen2_Overview/#architecture-in-a-data-lake","title":"\ud83d\udd39 Architecture in a Data Lake","text":"<p>A typical ADLS Gen2 data lake is organized into layers:</p> <ul> <li>Raw Zone \u2192 direct dump from source systems.</li> <li>Staging/Curated Zone \u2192 cleaned, transformed datasets.</li> <li>Presentation Zone \u2192 business-ready, aggregated data.</li> </ul> <p>Great question \ud83d\udc4d The Hierarchical Namespace (HNS) is actually the defining feature of ADLS Gen2, so let\u2019s go deeper.</p>"},{"location":"azure/09-ADLS_Gen2_Overview/#what-is-a-hierarchical-namespace","title":"\ud83d\udd39 What is a Hierarchical Namespace?","text":"<ul> <li> <p>Normally, Blob Storage is a flat namespace:</p> </li> <li> <p>Every object (blob) lives in a single flat container.</p> </li> <li>The \u201cfolders\u201d you see in the Azure portal are just virtual prefixes in blob names (<code>sales/2025/january/data.csv</code> is just a string, not a real folder).</li> <li> <p>Operations like rename or move are simulated (copy + delete), which is slow and costly.</p> </li> <li> <p>In ADLS Gen2, the Hierarchical Namespace (HNS) adds:</p> </li> <li> <p>True directories and subdirectories (like an actual file system).</p> </li> <li>Objects are tracked as files within directories, not just as strings.</li> <li>File system operations (rename, move, delete directory, list directory) become atomic and efficient.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#why-hierarchical-namespace-matters","title":"\ud83d\udd39 Why Hierarchical Namespace Matters","text":""},{"location":"azure/09-ADLS_Gen2_Overview/#1-efficient-file-operations","title":"1. Efficient File Operations","text":"<ul> <li>Rename/Move: In Blob storage \u2192 requires copy + delete (slow, doubles cost).   In ADLS Gen2 \u2192 instant metadata update (atomic, cheap).</li> <li>Delete Directory: In Blob storage \u2192 must delete each file one by one.   In ADLS Gen2 \u2192 single operation at directory level.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#2-security-access-control","title":"2. Security &amp; Access Control","text":"<ul> <li> <p>Supports POSIX-like ACLs (Access Control Lists) at folder/file level.   Example:</p> </li> <li> <p><code>/raw/sales</code> \u2192 only raw-data team has read/write.</p> </li> <li><code>/curated/finance</code> \u2192 finance team has read-only.</li> <li>Much finer granularity than just account/container level RBAC.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#3-performance-for-analytics","title":"3. Performance for Analytics","text":"<ul> <li>Hadoop/Spark jobs expect a hierarchical filesystem (HDFS).</li> <li>With HNS, ADLS Gen2 behaves like HDFS \u2192 making Spark/Synapse/Databricks integration seamless.</li> <li>Listing, partition pruning, directory scans are faster.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#4-atomic-consistency","title":"4. Atomic Consistency","text":"<ul> <li>Guarantees atomic directory and file operations.</li> <li>Example: If you rename a folder of 1M files \u2192 operation is atomic at the namespace level, no risk of half-renamed state.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#technical-details-of-hns","title":"\ud83d\udd39 Technical Details of HNS","text":"<ul> <li>Enabled at account creation \u2192 You must check \u201cHierarchical namespace\u201d when creating a Storage Account for ADLS Gen2. (Cannot be disabled later.)</li> <li> <p>Once enabled:</p> </li> <li> <p>Storage Account = Root</p> </li> <li>Containers = File systems</li> <li>Directories = Actual folders</li> <li>Files = Data objects</li> </ul> <p>Path example (with HNS):</p> <pre><code>abfss://datalake@storageaccount.dfs.core.windows.net/raw/2025/transactions/file1.parquet\n</code></pre> <p>Here:</p> <ul> <li><code>storageaccount</code> \u2192 ADLS Gen2 account</li> <li><code>datalake</code> \u2192 File system (container)</li> <li><code>raw/2025/transactions</code> \u2192 Real directories</li> <li><code>file1.parquet</code> \u2192 File object</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#analogy","title":"\ud83d\udd39 Analogy","text":"<p>Think of:</p> <ul> <li>Blob Storage (Flat) \u2192 A big box of papers where you prefix filenames with labels (<code>sales_2025_jan_data.csv</code>).</li> <li>ADLS Gen2 (HNS) \u2192 A real filing cabinet with folders, subfolders, and files inside.</li> </ul>"},{"location":"azure/09-ADLS_Gen2_Overview/#benefits-summary","title":"\ud83d\udd39 Benefits Summary","text":"<ul> <li>\u2705 Faster rename/move/delete (atomic ops)</li> <li>\u2705 Lower cost for file management</li> <li>\u2705 Fine-grained ACL-based security</li> <li>\u2705 Seamless HDFS compatibility (Spark, Hadoop)</li> <li>\u2705 Cleaner data lake organization (raw \u2192 curated \u2192 presentation)</li> </ul>"},{"location":"azure/10-Azure_RBAC_ACL/","title":"RBAC (Role Based Access Control) vs ACL (Access Control List)","text":""},{"location":"azure/10-Azure_RBAC_ACL/#rbac-role-based-access-control","title":"\ud83d\udd39 RBAC (Role-Based Access Control)","text":"<ul> <li>Scope: At the Azure Resource level (Subscription \u2192 Resource Group \u2192 Storage Account \u2192 Container/File System).</li> <li>Purpose: Controls management and broad access to resources.</li> <li>Assigned via: Azure Active Directory (Azure AD).</li> <li> <p>Examples of RBAC roles:</p> </li> <li> <p>Storage Blob Data Reader \u2192 can read blobs/files.</p> </li> <li>Storage Blob Data Contributor \u2192 can read/write/delete.</li> <li>Storage Blob Data Owner \u2192 full control.</li> </ul> <p>\u2705 Strengths:</p> <ul> <li>Centralized (assign once at container level, applies to all).</li> <li>Great for coarse-grained permissions.</li> <li>Easy to manage across thousands of users.</li> </ul> <p>\u274c Limitations:</p> <ul> <li>Not file/folder level \u2192 If you grant access to a file system, users see everything inside.</li> <li>Cannot express \u201cUser A can only read <code>/raw/sales/2025</code> but not <code>/raw/hr</code>.\u201d</li> </ul>"},{"location":"azure/10-Azure_RBAC_ACL/#acl-access-control-lists","title":"\ud83d\udd39 ACL (Access Control Lists)","text":"<ul> <li>Scope: At the data level (directory and file).</li> <li>Purpose: Provides fine-grained, POSIX-like permissions within the hierarchical namespace.</li> <li>Assigned via: Set on directories/files using ADLS Gen2 APIs, CLI, or Databricks/Spark.</li> <li> <p>ACLs have three types:</p> </li> <li> <p>Read (r) \u2013 view file contents, list directory.</p> </li> <li>Write (w) \u2013 modify contents.</li> <li>Execute (x) \u2013 traverse directory / access child objects.</li> </ul> <p>\u2705 Strengths:</p> <ul> <li>Very granular (control at file/folder level).</li> <li>Mimics traditional file systems (POSIX model).</li> <li>Perfect for multi-team data lakes where each team should only see their zone.</li> </ul> <p>\u274c Limitations:</p> <ul> <li>Can get complex to manage if you have thousands of folders.</li> <li>Inheritance isn\u2019t automatic unless you set default ACLs.</li> </ul>"},{"location":"azure/10-Azure_RBAC_ACL/#how-rbac-and-acl-work-together-in-adls-gen2","title":"\ud83d\udd39 How RBAC and ACL Work Together in ADLS Gen2","text":"<p>\ud83d\udc49 Think of it as two layers of security:</p> <ol> <li>RBAC decides: Can this user access this storage account / file system at all?</li> <li>ACLs decide: Within that file system, what directories and files can they actually read/write?</li> </ol> <p>\ud83d\udd11 Rule: RBAC grants the door key, ACLs decide which rooms inside you can enter.</p>"},{"location":"azure/10-Azure_RBAC_ACL/#example","title":"\ud83d\udd39 Example","text":""},{"location":"azure/10-Azure_RBAC_ACL/#scenario","title":"Scenario:","text":"<ul> <li>Storage account: <code>datalakeprod</code></li> <li>File system: <code>finance</code></li> <li>Directory: <code>/finance/reports/2025/</code></li> </ul>"},{"location":"azure/10-Azure_RBAC_ACL/#user-alice-finance-analyst","title":"User: Alice (Finance Analyst)","text":"<ol> <li>RBAC: Assign Storage Blob Data Reader at the <code>finance</code> file system level \u2192 Alice can access the file system.</li> <li> <p>ACL:</p> </li> <li> <p><code>/finance/reports/2025/</code> \u2192 grant Alice read + execute</p> </li> <li><code>/finance/raw/</code> \u2192 deny access</li> </ol> <p>\ud83d\udc49 Result: Alice can see and read reports from 2025, but she cannot even list or open files in the <code>raw</code> folder.</p>"},{"location":"azure/10-Azure_RBAC_ACL/#summary","title":"\ud83d\udd39 Summary","text":"Feature RBAC ACL Scope Azure resource level File system (directory/file) Granularity Broad Fine-grained Assigned via Azure AD POSIX-like model Use case \u201cWho can access this storage account or container?\u201d \u201cWithin the container, what files/folders can they access?\u201d Best for Coarse access control Detailed data lake permissions <p>\ud83d\udc49 In short:</p> <ul> <li>RBAC = Door access to the building.</li> <li>ACL = Which rooms and drawers inside you can open.</li> </ul>"},{"location":"azure/10-Azure_RBAC_ACL/#practical-how-to-setup-rbac-and-acls","title":"Practical : How to setup RBAC and ACLs?","text":"<pre><code>az role assignment create \\\n  --assignee &lt;userObjectIdOrEmail&gt; \\\n  --role \"Storage Blob Data Reader\" \\\n  --scope /subscriptions/&lt;subId&gt;/resourceGroups/&lt;rgName&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storageAccountName&gt;\n</code></pre> <pre><code>az storage fs access set \\\n  --account-name &lt;storageAccountName&gt; \\\n  --file-system &lt;containerName&gt; \\\n  --path &lt;folderName&gt; \\\n  --acl \"user:&lt;userObjectId&gt;:r-x\"\n</code></pre>"},{"location":"azure/11-Azure_Types_Of_Storage/","title":"Azure Types Of Storage","text":""},{"location":"azure/11-Azure_Types_Of_Storage/#types-of-storage-in-azure","title":"Types of Storage in Azure","text":""},{"location":"azure/11-Azure_Types_Of_Storage/#1-azure-file-storage","title":"1. Azure File Storage","text":"<p>Definition: Azure File Storage provides fully managed file shares in the cloud, accessible via SMB (Server Message Block) or NFS protocols. It\u2019s like having a network drive in the cloud.</p> <p>Key Features:</p> Feature Details Access Protocol SMB 3.0 (for Windows/Linux), NFS 4.1 (premium tier). Structure Folders and files (like a traditional file system). Persistence Fully persistent, replicated. Mounting Can mount directly to VMs, Azure Kubernetes, or local machines. Typical Use Cases Lift-and-shift apps, shared storage for VMs, legacy apps needing a file system interface. <p>Example Use Case:</p> <ul> <li>Shared logs folder for multiple Azure VMs.</li> <li>Migrating on-premises applications that rely on file shares.</li> </ul>"},{"location":"azure/11-Azure_Types_Of_Storage/#2-azure-queue-storage","title":"2. Azure Queue Storage","text":"<p>Definition: Azure Queue Storage provides a messaging queue for asynchronous communication between components of an application. Each message is stored for up to 7 days by default.</p> <p>Key Features:</p> Feature Details Structure A queue is a collection of messages. Each message is up to 64 KB. Access REST API or Azure SDKs. Persistence Messages are durably stored until dequeued or expired. Processing Pattern FIFO-ish (first-in, first-out), but not guaranteed. Typical Use Cases Decoupling application components, task scheduling, background jobs, buffering workloads. <p>Example Use Case:</p> <ul> <li>A web app pushes tasks into a queue; a worker VM dequeues and processes them asynchronously.</li> <li>Event-driven data pipelines.</li> </ul>"},{"location":"azure/11-Azure_Types_Of_Storage/#3-azure-table-storage","title":"3. Azure Table Storage","text":"<p>Definition: Azure Table Storage is a NoSQL key-value store for structured, semi-structured data. It\u2019s highly scalable, low-latency, and schema-less.</p> <p>Key Features:</p> Feature Details Structure Tables \u2192 PartitionKey + RowKey + properties (columns). Schema Schema-less; each row can have different properties. Access REST API, SDKs, or OData. Querying Efficient on PartitionKey + RowKey; limited secondary indexing. Typical Use Cases Storing metadata, logs, IoT telemetry, lightweight structured datasets. <p>Example Use Case:</p> <ul> <li>Storing IoT device readings (temperature, humidity) with timestamp.</li> <li>Metadata store for blobs or files.</li> <li>User session data in web applications.</li> </ul>"},{"location":"azure/11-Azure_Types_Of_Storage/#4-quick-comparison","title":"4. Quick Comparison","text":"Feature File Storage Queue Storage Table Storage Type File system Messaging queue NoSQL key-value store Access SMB/NFS REST API / SDK REST API / SDK Data Structure Files &amp; directories Messages Rows (PartitionKey + RowKey) Use Case Shared storage, lift-and-shift apps Decoupled messaging, async tasks Structured/semi-structured data, logs, metadata Persistence Durable Durable until read/expire Durable <p>\u2705 Summary / Guidance:</p> <ul> <li>File Storage \u2192 Use when you need a traditional file system in the cloud.</li> <li>Queue Storage \u2192 Use for decoupling components, async processing, or buffering workloads.</li> <li>Table Storage \u2192 Use for scalable NoSQL storage with structured/semi-structured data.</li> </ul>"},{"location":"azure/11-Azure_Types_Of_Storage/#1-what-is-immutable-blob-storage","title":"1. What is Immutable Blob Storage?","text":"<p>Immutable Blob Storage is a type of Azure Storage account or configuration that prevents modification or deletion of blobs for a specified retention period.</p> <ul> <li>Once a blob is written, it cannot be changed or deleted until the retention period expires.</li> <li>Useful for compliance, regulatory, and audit requirements, e.g., finance, healthcare, or legal data.</li> </ul>"},{"location":"azure/11-Azure_Types_Of_Storage/#2-key-concepts","title":"2. Key Concepts","text":"Concept Description Immutability Policy Rules that define how long a blob is protected (e.g., 30 days, 365 days). Legal Hold Option to indefinitely prevent deletion until explicitly removed. Retention Period Duration (in days) for which blobs cannot be modified or deleted. Write Once, Read Many (WORM) The blob can be read many times, but written only once."},{"location":"azure/11-Azure_Types_Of_Storage/#3-types-of-immutable-policies","title":"3. Types of Immutable Policies","text":"<ol> <li> <p>Time-based retention</p> </li> <li> <p>Blobs cannot be modified or deleted for a fixed period.</p> </li> <li> <p>Example: 90-day retention for financial transactions.</p> </li> <li> <p>Legal hold</p> </li> <li> <p>Prevents modification/deletion indefinitely until legal hold is cleared.</p> </li> <li>Often used in audits or legal investigations.</li> </ol>"},{"location":"azure/11-Azure_Types_Of_Storage/#4-how-it-works-in-azure-blob-storage","title":"4. How it Works in Azure Blob Storage","text":"<ul> <li>Create a container in a storage account that supports immutability (must be a general-purpose v2 storage account).</li> <li>Enable immutable storage on the container.</li> <li> <p>Apply a policy:</p> </li> <li> <p>Time-based retention \u2192 specify days.</p> </li> <li>Legal hold \u2192 optionally apply.</li> <li> <p>Once a blob is uploaded:</p> </li> <li> <p>It cannot be deleted or overwritten until the retention period expires.</p> </li> <li>Reads are allowed.</li> </ul> <p>Important: Only newly uploaded blobs are protected. Existing blobs can be migrated into immutable containers if needed.</p>"},{"location":"azure/11-Azure_Types_Of_Storage/#5-use-cases","title":"5. Use Cases","text":"Scenario Why Immutable Storage? Financial transaction logs Regulatory compliance (SOX, SEC, FINRA) Healthcare records HIPAA compliance Legal or audit archives Prevent tampering or accidental deletion Backup data Ensure backups are safe from ransomware"},{"location":"azure/11-Azure_Types_Of_Storage/#6-creating-immutable-blob-storage-azure-portal","title":"6. Creating Immutable Blob Storage (Azure Portal)","text":"<ol> <li>Go to Storage Account \u2192 Containers \u2192 + Container.</li> <li>Set Access level (private / blob).</li> <li> <p>After creating the container:</p> </li> <li> <p>Go to Container \u2192 Immutable blob storage \u2192 Policies.</p> </li> <li>Add Time-based retention (e.g., 365 days).</li> <li>Optionally, add a Legal hold.</li> <li>Upload blobs \u2192 they are now write-once, read-many (WORM).</li> </ol>"},{"location":"azure/11-Azure_Types_Of_Storage/#7-creating-via-azure-cli","title":"7. Creating via Azure CLI","text":"<pre><code># Create a container\naz storage container create \\\n    --name immutable-container \\\n    --account-name mystorageacct\n\n# Set time-based immutability policy (e.g., 90 days)\naz storage container immutability-policy create \\\n    --account-name mystorageacct \\\n    --container-name immutable-container \\\n    --period 90 \\\n    --allow-protected-append-writes true\n</code></pre> <p><code>--allow-protected-append-writes</code> allows append operations (for logs) without breaking immutability.</p> <p>\u2705 Summary</p> <ul> <li>Immutable Blob Storage ensures write-once, read-many (WORM) behavior.</li> <li>Supports time-based retention or legal hold.</li> <li>Protects critical data from accidental or malicious deletion.</li> <li>Common in compliance-heavy industries.</li> </ul>"},{"location":"azure/12-Azure_Storage_Replication_Strategies/","title":"Azure Storage Replication Strategies","text":""},{"location":"azure/12-Azure_Storage_Replication_Strategies/#replication-strategies-in-azure-storage","title":"Replication Strategies in Azure Storage","text":"<p>Here\u2019s a detailed overview of replication strategies in Azure Storage, why they matter, and when to use each:</p>"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#1-why-replication-matters","title":"1. Why Replication Matters","text":"<p>Azure Storage replication ensures high availability, durability, and disaster recovery by keeping multiple copies of your data.</p> <ul> <li>Guarantees 99.999999999% (11 9s) durability for objects.</li> <li>Protects against hardware failures, datacenter outages, and regional disasters.</li> </ul>"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#2-azure-storage-replication-options","title":"2. Azure Storage Replication Options","text":"<p>Azure provides four main replication strategies for Blob, File, Queue, and Table Storage:</p> Strategy Acronym Description Pros Cons Use Cases Locally Redundant Storage LRS Keeps 3 copies of data within a single datacenter. Low cost, low latency Data lost if entire datacenter fails Non-critical apps, dev/test, temporary data Zone-Redundant Storage ZRS Keeps 3 copies across availability zones in the same region. High availability, survives zone failures Slightly higher cost Production workloads needing SLA uptime Geo-Redundant Storage GRS Keeps 6 copies: 3 in primary region, 3 in secondary region. Protects against regional disasters Higher latency for secondary region; read access not automatic Disaster recovery, backup data Read-Access Geo-Redundant Storage RA-GRS Same as GRS but allows read access from the secondary region. DR-ready, read scalability Higher cost, eventual consistency Global read-heavy apps, disaster recovery"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#3-how-replication-works","title":"3. How Replication Works","text":"<ol> <li> <p>LRS:</p> </li> <li> <p>All copies in the same datacenter.</p> </li> <li> <p>Protects against hardware failure, but not datacenter outage.</p> </li> <li> <p>ZRS:</p> </li> <li> <p>Copies are in different Availability Zones in the same region.</p> </li> <li> <p>Protects against zone failure (power/network outage in one zone).</p> </li> <li> <p>GRS / RA-GRS:</p> </li> <li> <p>Data is asynchronously replicated to a secondary region hundreds of miles away.</p> </li> <li>RA-GRS allows reads from the secondary region, GRS does not.</li> <li>There is a small replication lag (\\~15 minutes).</li> </ol>"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#4-choosing-a-strategy","title":"4. Choosing a Strategy","text":"Requirement Recommended Low cost, dev/test LRS High availability in region ZRS Disaster recovery across regions GRS DR + read scalability RA-GRS"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#5-setting-replication-in-azure","title":"5. Setting Replication in Azure","text":"<p>Azure Portal:</p> <ol> <li>Go to Storage Account \u2192 Settings \u2192 Configuration \u2192 Replication.</li> <li>Choose LRS, ZRS, GRS, or RA-GRS.</li> <li>Click Save.</li> </ol> <p>Azure CLI Example:</p> <pre><code>az storage account create \\\n  --name mystorageacct \\\n  --resource-group myResourceGroup \\\n  --location eastus \\\n  --sku Standard_LRS \\\n  --kind StorageV2\n</code></pre> <p>To change replication after creation:</p> <pre><code>az storage account update \\\n  --name mystorageacct \\\n  --resource-group myResourceGroup \\\n  --sku Standard_GRS\n</code></pre>"},{"location":"azure/12-Azure_Storage_Replication_Strategies/#6-notes-best-practices","title":"6. Notes / Best Practices","text":"<ul> <li>Use ZRS for production workloads within a region for high availability.</li> <li>Use GRS or RA-GRS for critical workloads needing regional disaster recovery.</li> <li>Cost increases with more durable replication options (ZRS &lt; GRS &lt; RA-GRS).</li> <li>RA-GRS allows read access from the secondary region without failover.</li> </ul> <p>\u2705 Summary</p> <p>Replication strategies in Azure allow you to balance cost, availability, and disaster recovery requirements:</p> <ul> <li>LRS \u2192 Cheap, protects against hardware failure.</li> <li>ZRS \u2192 Protects against zone failure.</li> <li>GRS \u2192 Protects against regional disaster, read/write in primary.</li> <li>RA-GRS \u2192 Same as GRS + read access from secondary.</li> </ul>"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/","title":"Soft Delete Pitr Azure Storage","text":""},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#soft-deletes-and-point-in-time-restore","title":"Soft Deletes and Point in Time Restore","text":""},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#1-soft-delete","title":"\ud83d\udd39 1. Soft Delete","text":"<p>Soft delete = \"safety net\" for accidental deletion. When enabled, deleted data isn\u2019t immediately removed - instead, it\u2019s kept for a retention period so you can restore it.</p>"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#where-it-applies","title":"Where it applies","text":"<ul> <li> <p>Azure Blob Storage / Data Lake Storage Gen2</p> </li> <li> <p>When you delete a blob or snapshot, it goes into a soft-delete state.</p> </li> <li>Retention period: 1-365 days (configurable).</li> <li> <p>You can list and restore these blobs from the portal, PowerShell, or CLI.</p> </li> <li> <p>Azure Files</p> </li> <li> <p>Protects deleted file shares.</p> </li> <li> <p>Azure SQL Database / Managed Instance</p> </li> <li> <p>Soft delete applies to backups. Deleted database backups are retained for 7 days by default.</p> </li> </ul> <p>\ud83d\udccc Example (Blob Storage):</p> <ul> <li>Delete a blob at <code>container1/data.csv</code>.</li> <li>It\u2019s recoverable for (say) 30 days.</li> <li>After 30 days, it\u2019s permanently purged.</li> </ul>"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#2-point-in-time-restore-pitr","title":"\ud83d\udd39 2. Point-in-Time Restore (PITR)","text":"<p>PITR = restore database to a specific time within a retention period. It uses continuous transaction log backups + full/differential backups.</p>"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#where-it-applies_1","title":"Where it applies","text":"<ul> <li> <p>Azure SQL Database</p> </li> <li> <p>Default retention: 7\u201335 days (depending on service tier).</p> </li> <li>You can restore to any second within that window.</li> <li> <p>PITR creates a new database (it doesn\u2019t overwrite the original).</p> </li> <li> <p>Cosmos DB</p> </li> <li> <p>Continuous backup with PITR up to 30 days.</p> </li> <li> <p>Azure Blob Storage</p> </li> <li> <p>Versioning + change feed + soft delete together simulate PITR at object level.</p> </li> </ul> <p>\ud83d\udccc Example (SQL Database):</p> <ul> <li>Retention set to 14 days.</li> <li>A DROP TABLE happened at <code>2025-08-25 10:00:00</code>.</li> <li>You can restore the database to <code>2025-08-25 09:59:59</code> and recover data.</li> </ul>"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#difference","title":"\ud83d\udd11 Difference","text":"Feature Purpose Retention Soft Delete Recover deleted data (blob, file share, backups). 1\u2013365 days (configurable) PITR Restore entire DB/container to any time in past. SQL: 7\u201335 days (default)"},{"location":"azure/13-Soft_Delete_PITR_Azure_Storage/#when-to-use","title":"\u2705 When to Use","text":"<ul> <li>Soft delete \u2192 accidental object deletion (blob/file/share/backup).</li> <li>PITR \u2192 logical corruption, dropped table, wrong update query, ransomware attack.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/","title":"Azure Shared Access Signature","text":""},{"location":"azure/14-Azure_Shared_Access_Signature/#shared-access-signatures-in-azure","title":"Shared Access Signatures in Azure","text":""},{"location":"azure/14-Azure_Shared_Access_Signature/#what-is-a-sas","title":"\ud83d\udd39 What is a SAS?","text":"<p>A Shared Access Signature (SAS) is like a temporary key with limited permissions that you can give to someone (or an app) so they can access your Azure Storage resources without sharing your account keys.</p> <ul> <li>Your storage account keys = the master key to the whole house.</li> <li>A SAS token = a guest pass to just one room, for a limited time.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#types-of-sas","title":"\ud83d\udd39 Types of SAS","text":"<ol> <li> <p>Account SAS</p> </li> <li> <p>Grants access to services at the account level.</p> </li> <li>Can apply to: Blob, File, Queue, Table.</li> <li> <p>Example: \u201cGive read/write to all blob containers for 2 hours.\u201d</p> </li> <li> <p>Service SAS</p> </li> <li> <p>Grants access to a specific resource (like one blob, or one file share).</p> </li> <li> <p>Example: \u201cUser can download just this one blob until midnight.\u201d</p> </li> <li> <p>User Delegation SAS</p> </li> <li> <p>Created using Azure AD credentials instead of account key.</p> </li> <li>More secure because you avoid using storage account keys.</li> <li>Example: \u201cAn app logged in with Azure AD gets a SAS for one blob.\u201d</li> </ol>"},{"location":"azure/14-Azure_Shared_Access_Signature/#what-can-you-control-with-a-sas","title":"\ud83d\udd39 What can you control with a SAS?","text":"<ul> <li> <p>Permissions</p> </li> <li> <p>Read (<code>r</code>), Write (<code>w</code>), Delete (<code>d</code>), List (<code>l</code>), Add, Create, Update.</p> </li> <li> <p>Start time and expiry time</p> </li> <li> <p>Define when the SAS is valid.</p> </li> <li> <p>Resource scope</p> </li> <li> <p>Container, Blob, File, Queue, Table.</p> </li> <li> <p>IP address restrictions</p> </li> <li> <p>Limit access to certain IP ranges.</p> </li> <li> <p>Protocol restrictions</p> </li> <li> <p>Allow HTTPS only (recommended).</p> </li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#sas-structure","title":"\ud83d\udd39 SAS Structure","text":"<p>A SAS is basically a token string you append to a resource URL. Example:</p> <pre><code>https://mystorageaccount.blob.core.windows.net/mycontainer/myfile.txt?sv=2023-11-14&amp;ss=b&amp;srt=o&amp;sp=r&amp;se=2025-09-05T10:00Z&amp;st=2025-09-05T08:00Z&amp;spr=https&amp;sig=abcd1234\n</code></pre> <p>Parts:</p> <ul> <li><code>sv</code> \u2192 Storage version.</li> <li><code>ss</code> \u2192 Services.</li> <li><code>srt</code> \u2192 Resource types.</li> <li><code>sp</code> \u2192 Permissions.</li> <li><code>st</code> / <code>se</code> \u2192 Start/Expiry time.</li> <li><code>sig</code> \u2192 The cryptographic signature.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#how-to-create-sas","title":"\ud83d\udd39 How to create SAS","text":""},{"location":"azure/14-Azure_Shared_Access_Signature/#option-1-azure-portal","title":"Option 1: Azure Portal","text":"<ul> <li>Go to your Storage Account \u2192 Shared access signature \u2192 Choose permissions, expiry, allowed IP \u2192 Generate SAS token.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#option-2-azure-storage-explorer","title":"Option 2: Azure Storage Explorer","text":"<ul> <li>Right-click container/blob \u2192 Get Shared Access Signature.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#option-3-azure-cli","title":"Option 3: Azure CLI","text":"<pre><code>az storage blob generate-sas \\\n  --account-name mystorageaccount \\\n  --container-name mycontainer \\\n  --name myfile.txt \\\n  --permissions r \\\n  --expiry 2025-09-05T10:00Z \\\n  --https-only \\\n  --output tsv\n</code></pre>"},{"location":"azure/14-Azure_Shared_Access_Signature/#option-4-sdk-python-example","title":"Option 4: SDK (Python example)","text":"<pre><code>from azure.storage.blob import generate_blob_sas, BlobSasPermissions\nfrom datetime import datetime, timedelta\n\nsas_token = generate_blob_sas(\n    account_name=\"mystorageaccount\",\n    container_name=\"mycontainer\",\n    blob_name=\"myfile.txt\",\n    account_key=\"your_storage_key\",\n    permission=BlobSasPermissions(read=True),\n    expiry=datetime.utcnow() + timedelta(hours=1)\n)\n\nprint(\"SAS Token:\", sas_token)\n</code></pre>"},{"location":"azure/14-Azure_Shared_Access_Signature/#best-practices","title":"\ud83d\udd39 Best Practices","text":"<ul> <li>Use User Delegation SAS with Azure AD when possible.</li> <li>Keep expiry times short (principle of least privilege).</li> <li>Use stored access policies if you need to revoke SAS without waiting for expiry.</li> <li>Restrict to HTTPS and specific IP ranges.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#shared-access-signatures-sas","title":"\ud83d\udd39 Shared Access Signatures (SAS)","text":"<ul> <li>What it is: A signed token you append to a storage resource URL (e.g., blob, file, queue).</li> <li> <p>How it works:</p> </li> <li> <p>Signed with the storage account key (or via Azure AD for user delegation SAS).</p> </li> <li>Grants specific permissions (R/W/D/L) for a time window.</li> <li>Passed around as part of a URL.</li> </ul> <p>\ud83d\udc49 Think: \u201cHere\u2019s a temporary guest pass to this file/container.\u201d</p> <p>Pros:</p> <ul> <li>Very flexible (can be scoped down to one blob, for 5 minutes).</li> <li>Easy to share (just a URL).</li> <li>No need for caller to authenticate with Azure AD.</li> </ul> <p>Cons:</p> <ul> <li>Hard to revoke (unless you use stored access policies).</li> <li>If leaked, anyone with the token can use it until it expires.</li> <li>Still relies on account keys (for Service SAS / Account SAS).</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#managed-identities-mi","title":"\ud83d\udd39 Managed Identities (MI)","text":"<ul> <li>What it is: An Azure AD identity automatically managed by Azure for your resource (VM, Function, App Service, Databricks, etc.).</li> <li> <p>How it works:</p> </li> <li> <p>Resource (e.g., Databricks cluster) has a system-assigned or user-assigned identity.</p> </li> <li>Identity is trusted by Azure AD.</li> <li>When the resource needs to access storage, it requests a token from Azure AD using its MI.</li> <li>Access is controlled via Azure RBAC (e.g., \u201cBlob Data Reader\u201d).</li> </ul> <p>\ud83d\udc49 Think: \u201cThe building security system recognizes you because you\u2019re wearing your office badge.\u201d</p> <p>Pros:</p> <ul> <li>No secrets, no SAS tokens, no keys to manage.</li> <li>Access controlled centrally by Azure RBAC.</li> <li>Tokens are short-lived and auto-rotated.</li> <li>More secure for long-running apps (no risk of token leaks in code).</li> </ul> <p>Cons:</p> <ul> <li>Less flexible for fine-grained sharing (you can\u2019t say \u201cgive access to just this one blob for 15 minutes\u201d).</li> <li>Requires the caller to run inside Azure (VM, Function, App Service, Databricks, etc.).</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#when-to-use-what","title":"\ud83d\udd39 When to use what","text":"Scenario Use SAS Use Managed Identity Share a file with an external partner \u2705 Yes \u274c No Grant temporary access to a single blob/container \u2705 Yes \u274c No Long-running Azure app accessing storage \u274c No \u2705 Yes Secure, keyless access with automatic token rotation \u274c No \u2705 Yes Fine-grained, time-limited access without RBAC changes \u2705 Yes \u274c No <p>\u2705 Summary:</p> <ul> <li>SAS = best for short-term, fine-grained, external sharing (like \u201cdownload this blob until tonight\u201d).</li> <li>Managed Identity = best for apps running in Azure that need ongoing secure access without key management.</li> </ul> <p>Great question \u2014 in Azure Storage, there are three types of Shared Access Signatures (SAS) you can use to delegate access. They differ in how they\u2019re issued, what keys/identities they rely on, and their typical use cases. Let\u2019s break them down clearly:</p>"},{"location":"azure/14-Azure_Shared_Access_Signature/#1-account-sas","title":"1. Account SAS","text":"<ul> <li>What it is: Grants access to resources in one entire storage account (across multiple services like Blob, File, Queue, Table).</li> <li> <p>How it\u2019s created:</p> </li> <li> <p>Signed with the storage account key.</p> </li> <li>Generated by someone who has the account key (usually admins or automation).</li> <li> <p>Scope:</p> </li> <li> <p>Can allow access to any service in the account.</p> </li> <li>Example: grant access to all blobs + queues in that storage account.</li> <li> <p>Use case:</p> </li> <li> <p>When you want broad access across multiple services in one storage account (e.g., backup apps).</p> </li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#2-service-sas","title":"2. Service SAS","text":"<ul> <li>What it is: Grants access to one specific service (Blob, File, Queue, or Table) in the storage account.</li> <li> <p>How it\u2019s created:</p> </li> <li> <p>Also signed with the storage account key.</p> </li> <li> <p>Scope:</p> </li> <li> <p>Limited to one service and possibly a narrower scope (e.g., one container or even one blob).</p> </li> <li> <p>Use case:</p> </li> <li> <p>When you want fine-grained control over access.</p> </li> <li>Example: give a client app permission to upload to a single blob container, but nothing else.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#3-user-delegation-sas","title":"3. User Delegation SAS","text":"<ul> <li>What it is: Grants access to Blob storage only, but signed with an Azure AD identity (user, group, service principal, or managed identity).</li> <li> <p>How it\u2019s created:</p> </li> <li> <p>You first obtain a User Delegation Key by authenticating with Azure AD.</p> </li> <li>Then you use that key to create the SAS.</li> <li> <p>Scope:</p> </li> <li> <p>Blob containers or blobs.</p> </li> <li>Access is tied to Azure AD role assignments + RBAC.</li> <li> <p>Use case:</p> </li> <li> <p>When you don\u2019t want to use account keys (better security, least privilege).</p> </li> <li>Example: a web app authenticated with Azure AD issues SAS tokens for users to securely upload files to Blob storage.</li> </ul>"},{"location":"azure/14-Azure_Shared_Access_Signature/#key-differences","title":"\ud83d\udd11 Key Differences","text":"Feature Account SAS Service SAS User Delegation SAS Signed with Storage account key Storage account key Azure AD (via user delegation key) Scope All services in account Single service (Blob/File/Queue/Table) Blob storage only Security model Key-based Key-based Identity-based (Azure AD + RBAC) Granularity Broad More fine-grained Fine-grained + tied to identity Best use case Admin apps needing wide access Client/service access to specific resources Apps/users where security and least privilege are critical"},{"location":"azure/14-Azure_Shared_Access_Signature/#example-scenarios","title":"\ud83d\udcdd Example Scenarios","text":"<ul> <li>Account SAS:   A backup tool needs access to all blobs and queues in a storage account.</li> <li>Service SAS:   A partner app should only write into one blob container but not see others.</li> <li>User Delegation SAS:   A web portal where users log in with Azure AD and get temporary SAS tokens to upload their files \u2014 no account keys exposed.</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/","title":"Azure Lifetime Management Policies","text":""},{"location":"azure/15-Azure_Lifetime_Management_Policies/#what-is-a-lifecycle-management-policy","title":"\ud83d\udd39 What is a Lifecycle Management Policy?","text":"<p>A lifecycle management policy in Azure Storage is a set of rules that automatically move or delete blob data based on conditions you define (like age, last access, or storage tier).</p> <p>\ud83d\udc49 Think of it like a cleaning robot for your storage:</p> <ul> <li>Move old files to cheaper storage (Cool / Archive).</li> <li>Delete files after X days.</li> <li>Keep only recently accessed data hot.</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#why-use-it","title":"\ud83d\udd39 Why use it?","text":"<ul> <li>Save money \ud83d\udcb0 by moving rarely used data to Cool or Archive tiers.</li> <li>Automatically clean up expired or obsolete data.</li> <li>Enforce compliance (e.g., delete logs after 365 days).</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#what-can-a-policy-do","title":"\ud83d\udd39 What can a policy do?","text":"<p>You define rules with filters and actions.</p>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#filters-what-data-is-affected","title":"Filters (what data is affected)","text":"<ul> <li>Blob type: block blob, append blob.</li> <li>Container or blob prefix: apply to a specific container or folder-like path.</li> <li>Blob index tags: apply only to blobs matching certain key-value tags.</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#conditions-when-to-act","title":"Conditions (when to act)","text":"<ul> <li><code>daysSinceModificationGreaterThan</code> \u2192 based on last modified date.</li> <li><code>daysAfterLastAccessTimeGreaterThan</code> \u2192 based on last access date (requires last access tracking).</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#actions-what-to-do","title":"Actions (what to do)","text":"<ul> <li>Move to a different tier: Hot \u2192 Cool \u2192 Archive.</li> <li>Delete the blob.</li> <li>Delete blob snapshots or versions.</li> </ul>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#example-scenarios","title":"\ud83d\udd39 Example Scenarios","text":"<ol> <li> <p>Archive old data</p> </li> <li> <p>Move blobs older than 90 days to Archive.</p> </li> <li> <p>Delete stale logs</p> </li> <li> <p>Delete blobs older than 365 days in <code>logs/</code> container.</p> </li> <li> <p>Tier by access</p> </li> <li> <p>If not accessed for 30 days \u2192 move to Cool.</p> </li> <li>If not accessed for 180 days \u2192 move to Archive.</li> </ol>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#example-policy-json","title":"\ud83d\udd39 Example Policy (JSON)","text":"<pre><code>{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"archiveOldLogs\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"filters\": {\n          \"blobTypes\": [\"blockBlob\"],\n          \"prefixMatch\": [\"logs/\"]\n        },\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysSinceModificationGreaterThan\": 30\n            },\n            \"tierToArchive\": {\n              \"daysSinceModificationGreaterThan\": 90\n            },\n            \"delete\": {\n              \"daysSinceModificationGreaterThan\": 365\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#how-to-configure","title":"\ud83d\udd39 How to configure","text":"<ol> <li> <p>Azure Portal</p> </li> <li> <p>Storage Account \u2192 Data Management \u2192 Lifecycle Management \u2192 Add rule.</p> </li> <li> <p>Azure CLI</p> </li> </ol> <pre><code>az storage account management-policy create \\\n  --account-name mystorageacct \\\n  --resource-group myRG \\\n  --policy @policy.json\n</code></pre> <ol> <li> <p>ARM Template / Terraform</p> </li> <li> <p>Infrastructure-as-code way to apply lifecycle policies.</p> </li> </ol>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#best-practices","title":"\ud83d\udd39 Best Practices","text":"<ul> <li>Use prefixes (like <code>logs/</code>, <code>archive/</code>) to separate hot vs cold data.</li> <li>Turn on last access tracking if you want rules based on read activity (but note it adds metadata overhead).</li> <li>Test rules with simulation (in Portal) before applying at scale.</li> <li>Use different tiers (Hot, Cool, Archive) strategically for cost optimization.</li> </ul> <p>\u2705 Summary: Lifecycle management policies = automatic rules that move or delete blobs based on age or last access, helping with cost savings and compliance.</p>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#immutable-blob-storage-worm-vs-lifecycle-managmeent-policies","title":"Immutable Blob Storage (WORM) vs Lifecycle Managmeent Policies","text":""},{"location":"azure/15-Azure_Lifetime_Management_Policies/#1-lifecycle-management-policies","title":"\ud83d\udd39 1. Lifecycle Management Policies","text":"<ul> <li>Goal: Cost optimization + cleanup.</li> <li> <p>What it does:</p> </li> <li> <p>Moves blobs between tiers (Hot \u2192 Cool \u2192 Archive).</p> </li> <li>Deletes blobs after X days or if unused.</li> <li>Control: You define JSON rules with conditions like last modified or last accessed time.</li> <li>Flexibility: You can change or remove policies anytime.</li> <li> <p>Use case:</p> </li> <li> <p>Logs older than 30 days \u2192 Cool tier.</p> </li> <li>Logs older than 365 days \u2192 Delete.</li> </ul> <p>\ud83d\udc49 Think: \u201cMove old clothes to the attic, throw them away after a year.\u201d</p>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#2-immutable-blob-storage-worm-write-once-read-many","title":"\ud83d\udd39 2. Immutable Blob Storage (WORM = Write Once, Read Many)","text":"<ul> <li>Goal: Compliance + data protection.</li> <li> <p>What it does:</p> </li> <li> <p>Locks blobs for a retention period (days to years).</p> </li> <li>Prevents deletion or overwrite (even by admins).</li> <li> <p>Control:</p> </li> <li> <p>Time-based retention \u2192 e.g., \u201cKeep for 7 years.\u201d</p> </li> <li>Legal hold \u2192 indefinite retention until manually cleared.</li> <li>Flexibility: Once a retention policy is locked, it cannot be shortened (only extended).</li> <li> <p>Use case:</p> </li> <li> <p>Financial records retention for 7 years.</p> </li> <li>Healthcare data that cannot be altered.</li> </ul> <p>\ud83d\udc49 Think: \u201cPut important documents in a sealed safe. You can read them, but not shred them until the timer expires.\u201d</p>"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#key-differences","title":"\ud83d\udd39 Key Differences","text":"Feature Lifecycle Management Immutable Storage Purpose Cost savings, cleanup Compliance, data protection Action Move, delete, tier data Prevent delete/overwrite Control JSON policy (flexible, editable) Retention lock (WORM) Who can override Admins can always change policies Nobody (not even account owner) until retention expires Use case Log cleanup, archive old data Legal/financial records, regulatory compliance Risk if misused Could delete important data Could lock data forever, increasing cost"},{"location":"azure/15-Azure_Lifetime_Management_Policies/#how-they-work-together","title":"\ud83d\udd39 How they work together","text":"<ul> <li>You cannot apply lifecycle deletion to blobs under immutable retention (deletion will fail).</li> <li>But you can tier immutable blobs (e.g., keep in Archive tier for cost savings).</li> </ul> <p>\u2705 Summary</p> <ul> <li>Lifecycle Policies = cost management tool.</li> <li>Immutable Storage = compliance + legal protection tool.</li> <li>Both are about data aging, but Lifecycle = flexible cleanup, Immutable = strict lock-down.</li> </ul>"},{"location":"azure/16-EventGrid_Integration_Azure/","title":"\ud83e\uddea Demo: Event Grid + Blob Storage + Azure Function","text":""},{"location":"azure/16-EventGrid_Integration_Azure/#1-create-a-storage-account","title":"1\ufe0f\u20e3 Create a Storage Account","text":"<pre><code>az group create --name demo-rg --location eastus\n\naz storage account create \\\n  --name demoeventgridstore \\\n  --resource-group demo-rg \\\n  --location eastus \\\n  --sku Standard_LRS \\\n  --kind StorageV2\n</code></pre> <p>Create a container:</p> <pre><code>az storage container create \\\n  --name demo-container \\\n  --account-name demoeventgridstore\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#2-create-a-function-app-event-handler","title":"2\ufe0f\u20e3 Create a Function App (event handler)","text":"<p>Install extension if not installed:</p> <pre><code>az extension add --name functionapp\n</code></pre> <p>Create resources:</p> <pre><code>az storage account create \\\n  --name demofunctionstore \\\n  --resource-group demo-rg \\\n  --location eastus \\\n  --sku Standard_LRS\n\naz functionapp create \\\n  --resource-group demo-rg \\\n  --consumption-plan-location eastus \\\n  --runtime python \\\n  --functions-version 4 \\\n  --name demo-eventgrid-func \\\n  --storage-account demofunctionstore\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#3-create-an-event-grid-subscription","title":"3\ufe0f\u20e3 Create an Event Grid Subscription","text":"<p>Hook Blob Storage events to the Function:</p> <pre><code>az eventgrid event-subscription create \\\n  --name demo-subscription \\\n  --source-resource-id $(az storage account show \\\n    --name demoeventgridstore \\\n    --resource-group demo-rg \\\n    --query id -o tsv) \\\n  --endpoint-type azurefunction \\\n  --endpoint $(az functionapp show \\\n    --name demo-eventgrid-func \\\n    --resource-group demo-rg \\\n    --query id -o tsv)\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#4-add-function-code","title":"4\ufe0f\u20e3 Add Function Code","text":"<p>Inside your Function App (can be done in VS Code or portal):</p> <p><code>__init__.py</code></p> <pre><code>import logging\nimport json\n\nimport azure.functions as func\n\ndef main(event: func.EventGridEvent):\n    logging.info('Event received: %s', event.get_json())\n    result = json.dumps({\n        'id': event.id,\n        'data': event.get_json(),\n        'topic': event.topic,\n        'subject': event.subject,\n        'event_type': event.event_type\n    })\n    logging.info('Processed Event: %s', result)\n</code></pre> <p>This will log the event payload whenever triggered.</p>"},{"location":"azure/16-EventGrid_Integration_Azure/#5-trigger-the-event","title":"5\ufe0f\u20e3 Trigger the Event","text":"<p>Upload a test file:</p> <pre><code>az storage blob upload \\\n  --account-name demoeventgridstore \\\n  --container-name demo-container \\\n  --name hello.txt \\\n  --file hello.txt \\\n  --auth-mode login\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#6-verify","title":"6\ufe0f\u20e3 Verify","text":"<p>Check logs of the Function App:</p> <pre><code>az functionapp log tail --name demo-eventgrid-func --resource-group demo-rg\n</code></pre> <p>You should see Event Grid delivering the event with metadata like:</p> <pre><code>{\n  \"id\": \"abcd-1234\",\n  \"data\": {\n    \"api\": \"PutBlob\",\n    \"clientRequestId\": \"...\",\n    \"requestId\": \"...\",\n    \"contentType\": \"text/plain\",\n    \"blobType\": \"BlockBlob\",\n    \"url\": \"https://demoeventgridstore.blob.core.windows.net/demo-container/hello.txt\"\n  },\n  \"topic\": \"/subscriptions/.../resourceGroups/demo-rg/providers/Microsoft.Storage/storageAccounts/demoeventgridstore\",\n  \"subject\": \"/blobServices/default/containers/demo-container/blobs/hello.txt\",\n  \"event_type\": \"Microsoft.Storage.BlobCreated\"\n}\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#why-do-we-need-an-event-grid-subscription","title":"\ud83d\udd11 Why do we need an Event Grid subscription?","text":"<ul> <li> <p>Event Grid itself is just an event router.</p> </li> <li> <p>It listens to event sources (like Blob Storage, IoT Hub, custom topics).</p> </li> <li> <p>But it doesn\u2019t know where to send events unless you explicitly tell it.</p> </li> <li> <p>An Event Grid subscription is the \u201crouting rule\u201d:</p> </li> <li> <p>Defines which events you care about (filters by event type, subject, prefix/suffix).</p> </li> <li>Defines where to send them (endpoint like Function, Logic App, Event Hub, Webhook).</li> </ul> <p>Without a subscription, the events are generated but simply dropped \u2014 nothing consumes them.</p>"},{"location":"azure/16-EventGrid_Integration_Azure/#example-blob-storage-function","title":"\ud83d\udccc Example (Blob Storage \u2192 Function)","text":"<ol> <li>Blob Storage generates an event: \u201cBlobCreated\u201d.</li> <li>Event Grid sees it but needs a subscription.</li> <li> <p>The Event Grid subscription says:</p> </li> <li> <p>Source = <code>demoeventgridstore</code> (Blob Storage).</p> </li> <li>Event Type = <code>Microsoft.Storage.BlobCreated</code>.</li> <li>Target = Function <code>demo-eventgrid-func</code>.</li> <li>Now, when a blob is uploaded \u2192 Event Grid matches subscription \u2192 delivers event to Function.</li> </ol>"},{"location":"azure/16-EventGrid_Integration_Azure/#analogy","title":"\ud83e\udde0 Analogy","text":"<p>Think of it like YouTube:</p> <ul> <li>Blob Storage (publisher) = YouTube channel.</li> <li>Event Grid (event router) = YouTube platform.</li> <li>Event Grid Subscription = you clicking \u201cSubscribe + Notify\u201d to a channel.</li> <li>Function/Logic App = your phone getting the notification.</li> </ul> <p>If you don\u2019t subscribe, the channel is still publishing videos (events), but you\u2019ll never see them.</p> <p>\u2705 So, you need to create a subscription every time you want to connect an event source to an event handler.</p>"},{"location":"azure/16-EventGrid_Integration_Azure/#example","title":"Example","text":""},{"location":"azure/16-EventGrid_Integration_Azure/#architecture-overview","title":"\ud83d\udd39 Architecture Overview","text":"<ol> <li>Blob Storage \u2192 file lands (raw data).</li> <li>Event Grid (system topic) \u2192 automatically emits <code>BlobCreated</code> event.</li> <li>Event Subscription \u2192 routes event to an Azure Function.</li> <li>Azure Function \u2192 parses event payload (which blob was uploaded) and calls Databricks Jobs API.</li> <li>Databricks Job \u2192 runs notebook/ETL to process the file.</li> </ol>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-1-create-storage-account","title":"\ud83d\udd39 Step 1. Create Storage Account","text":"<p>This is the event source.</p> <pre><code>az storage account create \\\n  --name mydatalake123 \\\n  --resource-group myResourceGroup \\\n  --location eastus \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --hierarchical-namespace true\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-2-create-an-event-handler-azure-function","title":"\ud83d\udd39 Step 2. Create an Event Handler (Azure Function)","text":"<p>This Function will receive BlobCreated events and trigger Databricks.</p> <pre><code>az functionapp create \\\n  --resource-group myResourceGroup \\\n  --consumption-plan-location eastus \\\n  --runtime python \\\n  --functions-version 4 \\\n  --name databrickstriggerfunc \\\n  --storage-account mydatalake123\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-3-create-event-subscription","title":"\ud83d\udd39 Step 3. Create Event Subscription","text":"<p>Connect Blob Storage \u2192 Event Grid \u2192 Function.</p> <pre><code>az eventgrid event-subscription create \\\n  --name blobCreatedToDatabricks \\\n  --source-resource-id /subscriptions/&lt;subId&gt;/resourceGroups/myResourceGroup/providers/Microsoft.Storage/storageAccounts/mydatalake123 \\\n  --endpoint-type azurefunction \\\n  --endpoint /subscriptions/&lt;subId&gt;/resourceGroups/myResourceGroup/providers/Microsoft.Web/sites/databrickstriggerfunc/functions/&lt;functionName&gt;\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-4-function-code-python","title":"\ud83d\udd39 Step 4. Function Code (Python)","text":"<p>This Function will:</p> <ol> <li>Receive BlobCreated event.</li> <li>Extract blob URL.</li> <li>Call Databricks Jobs API (authenticated with Personal Access Token or Managed Identity).</li> </ol> <pre><code>import logging\nimport os\nimport requests\nimport azure.functions as func\n\n# Databricks config\nDATABRICKS_INSTANCE = os.environ[\"DATABRICKS_INSTANCE\"]   # e.g. https://adb-123456789012.12.azuredatabricks.net\nDATABRICKS_TOKEN = os.environ[\"DATABRICKS_TOKEN\"]         # Store securely in Key Vault\nDATABRICKS_JOB_ID = os.environ[\"DATABRICKS_JOB_ID\"]       # Job you want to trigger\n\ndef main(event: func.EventGridEvent):\n    result = event.get_json()\n    logging.info(f\"Received event: {result}\")\n\n    # Check for blob created event\n    if event.event_type == \"Microsoft.Storage.BlobCreated\":\n        blob_url = result.get(\"url\")\n        logging.info(f\"New blob detected: {blob_url}\")\n\n        # Trigger Databricks job via REST API\n        url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now\"\n        headers = {\"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\"}\n        payload = {\n            \"job_id\": DATABRICKS_JOB_ID,\n            \"notebook_params\": {\n                \"input_blob\": blob_url\n            }\n        }\n\n        response = requests.post(url, headers=headers, json=payload)\n        if response.status_code == 200:\n            logging.info(\"Databricks job triggered successfully\")\n        else:\n            logging.error(f\"Failed to trigger job: {response.text}\")\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-5-databricks-job","title":"\ud83d\udd39 Step 5. Databricks Job","text":"<ol> <li>Create a Job in Databricks (pointing to a Notebook/Delta Live Table).</li> <li>Add a parameter <code>input_blob</code> so the notebook knows which file to process.</li> </ol> <p>Example Notebook:</p> <pre><code>dbutils.widgets.text(\"input_blob\", \"\")\nblob_url = dbutils.widgets.get(\"input_blob\")\n\nprint(f\"Processing file: {blob_url}\")\n\n# Example: Read from Blob/ADLS into Spark\ndf = spark.read.text(blob_url)\n# Do ETL...\n</code></pre>"},{"location":"azure/16-EventGrid_Integration_Azure/#step-6-test-it","title":"\ud83d\udd39 Step 6. Test It","text":"<p>Upload a file to Blob Storage:</p> <pre><code>az storage blob upload \\\n  --account-name mydatalake123 \\\n  --container-name raw \\\n  --name testdata.csv \\\n  --file ./testdata.csv\n</code></pre> <p>Event Grid \u2192 Function \u2192 Databricks job \u2192 Notebook runs with the blob path.</p>"},{"location":"azure/16-EventGrid_Integration_Azure/#extras-production-ready","title":"\ud83d\udd39 Extras (Production Ready)","text":"<ul> <li>Secure secrets \u2192 Store <code>DATABRICKS_TOKEN</code> in Azure Key Vault and integrate with Function.</li> <li>Retries \u2192 Event Grid automatically retries delivery (with exponential backoff).</li> <li>Dead-letter destination \u2192 configure a Blob container to store undelivered events.</li> <li>Monitoring \u2192 Use Application Insights on the Function + Event Grid metrics.</li> </ul> <p>\u2705 With this setup, every time a new file lands in storage, your Databricks pipeline kicks in automatically \u2014 no polling needed, fully event-driven.</p>"},{"location":"azure/17-Azure_Encrpytion_Standards/","title":"\ud83d\udd12 1. Types of Encryption in Azure Storage","text":""},{"location":"azure/17-Azure_Encrpytion_Standards/#a-encryption-at-rest-server-side-encryption-sse","title":"a. Encryption at Rest (Server-Side Encryption, SSE)","text":"<ul> <li>All data written to Azure Storage (Blob, File, Queue, Table, Disk) is automatically encrypted.</li> <li>Uses 256-bit AES encryption (FIPS 140-2 compliant).</li> <li>Happens before persisting data to disk and is transparent to you.</li> <li>No extra cost.</li> </ul> <p>You can choose key management options:</p> <ol> <li>Microsoft-managed keys (default) \u2192 Azure manages keys automatically.</li> <li> <p>Customer-managed keys (CMK) \u2192 You provide keys in Azure Key Vault or Managed HSM.</p> </li> <li> <p>Useful for compliance and rotation policies.</p> </li> </ol>"},{"location":"azure/17-Azure_Encrpytion_Standards/#b-encryption-in-transit","title":"b. Encryption in Transit","text":"<ul> <li>All communications to Azure Storage use HTTPS/TLS.</li> <li>You can enforce HTTPS-only traffic by disabling HTTP at the storage account level.</li> <li>SMB 3.0 encryption is used for Azure Files.</li> </ul>"},{"location":"azure/17-Azure_Encrpytion_Standards/#c-client-side-encryption","title":"c. Client-Side Encryption","text":"<ul> <li>Optional, you encrypt data before uploading to Azure.</li> <li>You manage keys and encryption.</li> <li>Useful for very sensitive scenarios where you want full control.</li> </ul>"},{"location":"azure/17-Azure_Encrpytion_Standards/#2-how-to-enable-configure","title":"\ud83d\udee0 2. How to Enable / Configure","text":""},{"location":"azure/17-Azure_Encrpytion_Standards/#by-default","title":"By default:","text":"<ul> <li>Encryption at rest (SSE with Microsoft-managed keys) is always on, you don\u2019t have to do anything.</li> </ul>"},{"location":"azure/17-Azure_Encrpytion_Standards/#to-use-customer-managed-keys-cmk","title":"To use Customer-Managed Keys (CMK):","text":"<ol> <li>Create or use an Azure Key Vault.</li> <li>Generate or import your encryption key.</li> <li>Grant the Storage Account access permissions to the key.</li> <li>Configure the Storage Account to use that key for encryption.</li> </ol>"},{"location":"azure/17-Azure_Encrpytion_Standards/#3-example-azure-cli","title":"\ud83d\udcc2 3. Example \u2013 Azure CLI","text":"<p>Enable CMK with a Key Vault key:</p> <pre><code>az storage account update \\\n  --name mystorageaccount \\\n  --resource-group myResourceGroup \\\n  --encryption-key-source Microsoft.Keyvault \\\n  --encryption-key-vault https://mykeyvault.vault.azure.net/ \\\n  --encryption-key-name myKey\n</code></pre>"},{"location":"azure/17-Azure_Encrpytion_Standards/#4-quick-summary","title":"\u2705 4. Quick Summary","text":"<ul> <li>At Rest \u2192 Always encrypted with AES-256.</li> <li>In Transit \u2192 Encrypted with TLS (HTTPS/SMB).</li> <li>Keys \u2192 Microsoft-managed by default, or CMK via Key Vault/HSM.</li> <li>Extra \u2192 You can do client-side encryption for max control.</li> </ul> <p>\ud83d\udc49 Do you want me to also show you how Spark / Databricks integrates with Azure Storage encryption (e.g., when reading/writing to ADLS Gen2)?</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/","title":"Azure Private Endpoints","text":""},{"location":"azure/18-Azure%2B_Private_Endpoints/#what-is-an-azure-private-endpoint","title":"\ud83d\udd12 What is an Azure Private Endpoint?","text":"<ul> <li>A private endpoint is a network interface in your Virtual Network (VNet) that connects you privately and securely to an Azure service.</li> <li>Instead of accessing the service (e.g., Storage, SQL Database, Cosmos DB, Key Vault) via a public IP, traffic flows through a private IP inside your VNet.</li> <li>Uses Azure Private Link technology.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#how-it-works","title":"\ud83d\udee0 How It Works","text":"<ol> <li>You create a Private Endpoint for a resource (like a Storage Account).</li> <li>Azure assigns a private IP from your VNet to this endpoint.</li> <li>Your VNet traffic \u2192 goes through this private IP \u2192 securely reaches the Azure service \u2192 without leaving Microsoft\u2019s backbone network.</li> <li>The service\u2019s public endpoint is still there, but you can restrict/block it.</li> </ol>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#example-azure-storage-with-private-endpoint","title":"\ud83d\udcc2 Example: Azure Storage with Private Endpoint","text":"<ul> <li>You have a Storage Account <code>mystorage.blob.core.windows.net</code>.</li> <li>Normally, you\u2019d connect via the public internet using that FQDN.</li> <li>With a Private Endpoint, Azure will map:</li> </ul> <pre><code>mystorage.privatelink.blob.core.windows.net \u2192 10.1.0.5  (private IP inside your VNet)\n</code></pre> <ul> <li>So apps in your VNet access Storage via private IP.</li> <li>You can then disable all public access to the Storage Account for max security.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#benefits","title":"\u2705 Benefits","text":"<ul> <li>Security: No public internet exposure.</li> <li>Compliance: Meets strict data residency/security requirements.</li> <li>Integration: Works with Azure PaaS (Storage, SQL, Cosmos DB, Key Vault, etc.) and your own services behind Azure Standard Load Balancer.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#configuration-steps-high-level","title":"\u2699\ufe0f Configuration Steps (High-Level)","text":"<ol> <li>Create a VNet &amp; Subnet.</li> <li> <p>Create a Private Endpoint:</p> </li> <li> <p>Choose target service (e.g., Storage Account \u2192 Blob).</p> </li> <li>Pick the VNet + subnet.</li> <li>A NIC with private IP gets created.</li> <li> <p>Update DNS:</p> </li> <li> <p>Ensure the service FQDN resolves to the private IP (via Azure Private DNS Zone).</p> </li> <li> <p>Restrict Public Access:</p> </li> <li> <p>Disable public network access on the resource.</p> </li> </ol>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#azure-cli-example","title":"\ud83d\ude80 Azure CLI Example","text":"<pre><code># Create Private Endpoint for Storage Account\naz network private-endpoint create \\\n  --name mystorage-pe \\\n  --resource-group myResourceGroup \\\n  --vnet-name myVNet \\\n  --subnet mySubnet \\\n  --private-connection-resource-id $(az storage account show -n mystorage -g myResourceGroup --query id -o tsv) \\\n  --group-id blob \\\n  --connection-name mystorage-connection\n\n# Link Private DNS Zone\naz network private-dns zone create -g myResourceGroup -n \"privatelink.blob.core.windows.net\"\naz network private-dns link vnet create -g myResourceGroup -n \"link-myvnet\" -z \"privatelink.blob.core.windows.net\" -v myVNet -e true\n</code></pre>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#architecture","title":"Architecture","text":""},{"location":"azure/18-Azure%2B_Private_Endpoints/#real-world-use-cases","title":"\ud83c\udf0d Real-World Use Cases","text":"<ul> <li>Connect Azure SQL Database from on-prem \u2192 via ExpressRoute/VPN + private endpoint (no internet).</li> <li>Secure Azure Storage for Databricks / Synapse pipelines.</li> <li>Access Key Vault privately from inside a VNet.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#easier-explanation","title":"Easier Explanation","text":"<p>In Azure, a Private Endpoint is a network interface that connects you privately and securely to a service powered by Azure Private Link.</p> <p>Instead of accessing services over the public internet, a private endpoint lets you access them through your virtual network (VNet) using private IP addresses.</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#key-points","title":"\ud83d\udd11 Key Points","text":"<ol> <li>Private IP \u2013 The service (e.g., Azure Storage, SQL Database, Key Vault, etc.) gets a private IP inside your VNet.</li> <li>No Public Exposure \u2013 Traffic stays within the Microsoft backbone network instead of going over the internet.</li> <li>DNS Integration \u2013 You use private DNS zones so that service names (e.g., <code>mystorageaccount.blob.core.windows.net</code>) resolve to the private IP.</li> <li>Secure Access \u2013 Only resources in your VNet (or peered VNets) can connect to the service.</li> <li>Isolation \u2013 You can disable all public access to the service and allow only private endpoint traffic.</li> </ol>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#example-scenarios","title":"\u2705 Example Scenarios","text":"<ul> <li>Azure Storage Account: Instead of accessing a blob container over the internet, a private endpoint gives your VM a private IP connection.</li> <li>Azure SQL Database: Applications inside your VNet can connect privately without opening public firewall rules.</li> <li>Azure Key Vault: Secrets can be retrieved over a private IP, protecting against data exfiltration.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#how-it-works_1","title":"\ud83d\udcca How it works","text":"<ol> <li>You create a Private Endpoint in your VNet.</li> <li>Azure assigns a private IP from your VNet to that endpoint.</li> <li>When your application resolves the service\u2019s FQDN, it gets the private IP (via DNS configuration).</li> <li>The traffic routes securely through Azure Private Link (Microsoft backbone).</li> </ol> <p>\ud83d\udc49 Think of it like this: Instead of going out to the internet and back into Azure services, Private Endpoints bring the service into your VNet using a private IP.</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#private-endpoints-vs-serivce-endpoints","title":"Private Endpoints vs Serivce Endpoints","text":"<p>Perfect question \ud83d\udc4d \u2014 this comes up a lot in interviews. Let\u2019s break it down:</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#private-endpoint-vs-service-endpoint","title":"\ud83d\udd39 Private Endpoint vs. Service Endpoint","text":"Feature Private Endpoint Service Endpoint Connectivity Connects to the service using a private IP inside your VNet. Extends your VNet identity to the service\u2019s public IP. Traffic still flows to the service\u2019s public endpoint. Security Service can be locked down to only allow private endpoints \u2192 No public exposure. Service is still reachable on the public internet, but limited to specific VNet/subnet(s). Traffic Path Goes through Azure backbone via Private Link \u2192 never leaves the Microsoft network. Still reaches the service\u2019s public endpoint, but Azure ensures it stays on the backbone (doesn\u2019t traverse the open internet). DNS Requirement Needs DNS zone integration so service FQDN resolves to the private IP. No DNS changes needed (uses public IP). Granularity Works at the instance level (e.g., a specific storage account, SQL DB, Key Vault). Works at the service level (all storage accounts in a region, etc.). Access Control You can disable public access completely and force all traffic through private endpoint. Public access is still available unless explicitly restricted. Cost Additional cost for Private Link/Private Endpoint. No extra cost (free). Use Cases High-security workloads, regulatory compliance, zero-trust architectures. Simpler setup when you just want secure connectivity without exposing entire internet."},{"location":"azure/18-Azure%2B_Private_Endpoints/#example","title":"\ud83d\udccc Example","text":"<ul> <li> <p>If you have an Azure SQL Database:</p> </li> <li> <p>Private Endpoint \u2192 Your app in VNet connects to SQL over a private IP. You can block all public access.</p> </li> <li>Service Endpoint \u2192 Your app connects over SQL\u2019s public IP, but Azure recognizes it\u2019s coming from your VNet and allows it.</li> </ul> <p>\ud83d\udc49 In short:</p> <ul> <li>Private Endpoint = Private IP, highest security, service instance\u2013level.</li> <li>Service Endpoint = Public IP, simpler, service-level restriction.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#analogy","title":"Analogy","text":"<p>Great \u2014 let\u2019s make this real-world and easy to remember \ud83d\udc47</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#analogy-private-endpoint-vs-service-endpoint","title":"\ud83c\udfe0 Analogy: Private Endpoint vs Service Endpoint","text":""},{"location":"azure/18-Azure%2B_Private_Endpoints/#private-endpoint-vip-door-inside-your-house","title":"Private Endpoint (VIP Door Inside Your House)","text":"<ul> <li>Imagine you hire a bank (Azure service) to keep your valuables.</li> <li>Instead of visiting their public branch office (internet), the bank builds a special private door inside your house that directly connects to your locker.</li> <li>Only you (inside your house / VNet) can use it.</li> <li>Outsiders can\u2019t even see the locker\u2019s public branch anymore (because you can disable public access).</li> <li>More secure, but costs extra (you\u2019re paying for that VIP private door).</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#service-endpoint-fast-lane-to-the-bank-branch","title":"Service Endpoint (Fast Lane to the Bank Branch)","text":"<ul> <li>You still go to the public branch office (public IP of service).</li> <li>But the bank recognizes you as a VIP customer from your gated community (VNet/subnet).</li> <li>They let you skip the long queue and give you a secure corridor directly into the branch.</li> <li>Others can still access the branch (public access remains).</li> <li>Free and simpler, but less private than the \u201cdoor inside your house.\u201d</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#quick-mnemonic","title":"\ud83c\udfaf Quick Mnemonic","text":"<ul> <li>Private Endpoint = Private Door (service inside your VNet)</li> <li>Service Endpoint = VIP Lane (still public, just secured to your VNet)</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#example_1","title":"Example","text":"<p>Great question \u2014 let\u2019s go through an Azure Private Link example step by step.</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#what-is-azure-private-link","title":"\ud83d\udd39 What is Azure Private Link?","text":"<p>Azure Private Link lets you access Azure services over a private IP address inside your Virtual Network (VNet).</p> <ul> <li>Without Private Link \u2192 your app connects to a public endpoint (internet-exposed).</li> <li>With Private Link \u2192 your app connects to a private endpoint (private IP in your VNet), but traffic still reaches the Azure service securely over Microsoft\u2019s backbone network.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#example-scenario","title":"\ud83d\udd39 Example Scenario","text":"<p>You have:</p> <ul> <li>An App VM in a Virtual Network.</li> <li>An Azure Storage Account.</li> </ul> <p>\ud83d\udc49 You want the VM to connect to the storage account securely without going over the public internet.</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#step-by-step-setup","title":"\ud83d\udd39 Step-by-Step Setup","text":""},{"location":"azure/18-Azure%2B_Private_Endpoints/#1-create-a-vnet-vm","title":"1. Create a VNet + VM","text":"<pre><code># Create resource group\naz group create -n myRG -l eastus\n\n# Create VNet + subnet\naz network vnet create \\\n  --name myVNet \\\n  --resource-group myRG \\\n  --address-prefix 10.0.0.0/16 \\\n  --subnet-name mySubnet \\\n  --subnet-prefix 10.0.1.0/24\n\n# Create VM in VNet (Linux example)\naz vm create \\\n  --resource-group myRG \\\n  --name myVM \\\n  --image UbuntuLTS \\\n  --admin-username azureuser \\\n  --generate-ssh-keys \\\n  --vnet-name myVNet \\\n  --subnet mySubnet\n</code></pre>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#2-create-a-storage-account","title":"2. Create a Storage Account","text":"<pre><code>az storage account create \\\n  --name mystoragepldemo \\\n  --resource-group myRG \\\n  --location eastus \\\n  --sku Standard_LRS \\\n  --kind StorageV2\n</code></pre>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#3-create-a-private-endpoint","title":"3. Create a Private Endpoint","text":"<p>This links the storage account to your VNet with a private IP.</p> <pre><code>az network private-endpoint create \\\n  --resource-group myRG \\\n  --name myPrivateEndpoint \\\n  --vnet-name myVNet \\\n  --subnet mySubnet \\\n  --private-connection-resource-id $(az storage account show \\\n        --name mystoragepldemo \\\n        --resource-group myRG \\\n        --query \"id\" -o tsv) \\\n  --group-id blob \\\n  --connection-name myConnection\n</code></pre> <ul> <li><code>--group-id blob</code> \u2192 connects specifically to Blob service.</li> <li>A private IP (like <code>10.0.1.4</code>) is assigned inside <code>mySubnet</code>.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#4-configure-private-dns","title":"4. Configure Private DNS","text":"<p>Private endpoints require DNS to resolve the storage account name to the private IP.</p> <pre><code>az network private-dns zone create \\\n  --resource-group myRG \\\n  --name \"privatelink.blob.core.windows.net\"\n\naz network private-dns link vnet create \\\n  --resource-group myRG \\\n  --zone-name \"privatelink.blob.core.windows.net\" \\\n  --name MyDNSLink \\\n  --virtual-network myVNet \\\n  --registration-enabled false\n\naz network private-endpoint dns-zone-group create \\\n  --resource-group myRG \\\n  --endpoint-name myPrivateEndpoint \\\n  --name MyZoneGroup \\\n  --private-dns-zone \"privatelink.blob.core.windows.net\" \\\n  --zone-name \"privatelink.blob.core.windows.net\"\n</code></pre> <p>Now, <code>mystoragepldemo.blob.core.windows.net</code> resolves to the private IP (10.0.x.x) inside your VNet.</p> <p>A Record <code>mystorageacct.blob.core.windows.net \u2192 10.0.1.4</code> created in the DNS</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#5-test-from-vm","title":"5. Test from VM","text":"<p>SSH into the VM:</p> <pre><code>ssh azureuser@&lt;public-ip-of-vm&gt;\n</code></pre> <p>Test DNS resolution:</p> <pre><code>nslookup mystoragepldemo.blob.core.windows.net\n</code></pre> <p>\u2705 Should resolve to <code>10.0.1.x</code> (private IP).</p> <p>Test connectivity:</p> <pre><code>curl https://mystoragepldemo.blob.core.windows.net/\n</code></pre> <p>Traffic goes through the private endpoint, not the public internet.</p>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#real-world-uses","title":"\ud83d\udd39 Real-World Uses","text":"<ul> <li>Databricks accessing ADLS Gen2 over private link.</li> <li>SQL Database private endpoint to keep DB off the public internet.</li> <li>Key Vault private endpoint so secrets are only accessible in-VNet.</li> <li>App Service \u2192 Storage Account private integration.</li> </ul>"},{"location":"azure/18-Azure%2B_Private_Endpoints/#key-benefits","title":"\ud83d\udd39 Key Benefits","text":"<ul> <li>Removes exposure to public internet.</li> <li>Simplifies network security (no IP whitelisting).</li> <li>Uses Microsoft\u2019s backbone network for traffic.</li> <li>Works with Azure Monitor logs to track connections.</li> </ul>"},{"location":"azure/19-Cross_Region_Replication_Azure/","title":"Cross Region Replication","text":""},{"location":"azure/19-Cross_Region_Replication_Azure/#1-built-in-redundancy-options-storage-account-level","title":"\ud83d\udd11 1. Built-in Redundancy Options (Storage Account Level)","text":"<p>When you create an ADLS Gen2 account, you choose a redundancy type. Azure handles replication under the hood.</p>"},{"location":"azure/19-Cross_Region_Replication_Azure/#options","title":"\u2705 Options:","text":"<ol> <li> <p>LRS (Locally Redundant Storage)</p> </li> <li> <p>3 copies within a single datacenter in one region.</p> </li> <li> <p>Cheapest but no cross-region replication.</p> </li> <li> <p>ZRS (Zone-Redundant Storage)</p> </li> <li> <p>3 copies across 3 availability zones in one region.</p> </li> <li> <p>Provides resilience against zone failures, but not region-wide outages.</p> </li> <li> <p>GRS (Geo-Redundant Storage)</p> </li> <li> <p>6 copies: 3 in primary region + 3 in a paired secondary region.</p> </li> <li> <p>Replication is asynchronous.</p> </li> <li> <p>RA-GRS (Read-Access Geo-Redundant Storage)</p> </li> <li> <p>Same as GRS, but read access to the secondary region is available.</p> </li> <li> <p>Useful for disaster recovery and read-heavy workloads.</p> </li> <li> <p>GZRS (Geo-Zone Redundant Storage)</p> </li> <li> <p>Combines ZRS (in primary region) + asynchronous replication to a paired secondary region.</p> </li> <li> <p>High durability + regional disaster recovery.</p> </li> <li> <p>RA-GZRS (Read-Access GZRS)</p> </li> <li> <p>Same as GZRS but with read access to secondary region.</p> </li> <li>Best option for mission-critical cross-region replication.</li> </ol>"},{"location":"azure/19-Cross_Region_Replication_Azure/#2-asynchronous-data-movement-custom-replication","title":"\ud83d\udd11 2. Asynchronous Data Movement (Custom Replication)","text":"<p>Sometimes built-in GRS/RA-GRS may not meet your latency, cost, or compliance requirements. In that case, you build custom cross-region replication:</p>"},{"location":"azure/19-Cross_Region_Replication_Azure/#approaches","title":"Approaches:","text":"<ul> <li> <p>Azure Data Factory (ADF) Copy Activity</p> </li> <li> <p>Schedule pipelines to copy data from ADLS in Region A \u2192 ADLS in Region B.</p> </li> <li>Flexible (supports filtering, transformations, scheduling).</li> <li> <p>Good for batch replication.</p> </li> <li> <p>Azure Data Share</p> </li> <li> <p>For sharing snapshots of datasets across regions.</p> </li> <li> <p>Suited for read-only scenarios.</p> </li> <li> <p>AzCopy / Azure Storage Sync Service</p> </li> <li> <p>AzCopy CLI can sync containers across regions.</p> </li> <li> <p>Best for ad-hoc or bulk replication.</p> </li> <li> <p>Event-Driven Replication with Event Grid + Functions</p> </li> <li> <p>Trigger on BlobCreated/Updated events \u2192 replicate object to another region.</p> </li> <li> <p>Provides near real-time cross-region replication.</p> </li> <li> <p>Third-party tools (Databricks, Apache NiFi, Informatica, etc.)</p> </li> <li> <p>For more complex pipelines involving CDC (Change Data Capture) or multi-cloud replication.</p> </li> </ul>"},{"location":"azure/19-Cross_Region_Replication_Azure/#3-strategy-selection-when-to-use-what","title":"\ud83d\udd11 3. Strategy Selection (When to Use What)","text":"<ul> <li>Mission-critical, zero-downtime needs \u2192 Use RA-GZRS for built-in cross-region replication with read access.</li> <li>Compliance requirements (regulatory control over region) \u2192 Use custom ADF/Event Grid pipelines to replicate only selected data.</li> <li>Disaster recovery only (cold standby) \u2192 Use GRS/RA-GRS with failover enabled.</li> <li>Performance optimization (local reads in multiple geographies) \u2192 Use RA-GRS or RA-GZRS so consumers in another geography can read from the secondary.</li> </ul>"},{"location":"azure/19-Cross_Region_Replication_Azure/#4-failover-considerations","title":"\ud83d\udd11 4. Failover Considerations","text":"<ul> <li>For GRS/RA-GRS/GZRS/RA-GZRS, failover to secondary is manual.</li> <li>When you trigger failover, the secondary becomes primary.</li> <li>Important: failover breaks replication (you must reconfigure after).</li> </ul> <p>\u2705 Summary:</p> <ul> <li>Use RA-GZRS if you want the best balance of availability + performance with cross-region reads.</li> <li>Use ADF/Event Grid/AzCopy if you need fine-grained control over what, when, and how data is replicated across regions.</li> <li>Always align your choice with RPO (Recovery Point Objective), RTO (Recovery Time Objective), and compliance needs.</li> </ul>"},{"location":"azure/20-Azure_Storage_Rest_API/","title":"Azure Storage Rest APIs","text":""},{"location":"azure/20-Azure_Storage_Rest_API/#1-what-it-is","title":"\ud83d\udd11 1. What It Is","text":"<p>The Azure Storage REST API allows you to programmatically perform operations on:</p> <ul> <li>Blob storage (block, append, page blobs \u2192 includes ADLS Gen2)</li> <li>Queue storage</li> <li>Table storage (legacy, use Cosmos DB Table API now)</li> <li>File shares (Azure Files)</li> </ul> <p>Instead of SDKs (Python, .NET, Java, etc.), you can call storage endpoints directly via HTTPS.</p>"},{"location":"azure/20-Azure_Storage_Rest_API/#2-rest-api-endpoint-pattern","title":"\ud83d\udd11 2. REST API Endpoint Pattern","text":"<p>Every request goes to:</p> <pre><code>https://&lt;account_name&gt;.blob.core.windows.net/&lt;container&gt;/&lt;blob&gt;?&lt;optional_parameters&gt;\n</code></pre> <p>Example (reading a file in ADLS Gen2):</p> <pre><code>GET https://mystorageaccount.blob.core.windows.net/mycontainer/myfolder/myfile.csv HTTP/1.1\nx-ms-date: Mon, 15 Sep 2025 18:00:00 GMT\nx-ms-version: 2023-11-03\nAuthorization: SharedKey mystorageaccount:&lt;signature&gt;\n</code></pre>"},{"location":"azure/20-Azure_Storage_Rest_API/#3-authentication-methods","title":"\ud83d\udd11 3. Authentication Methods","text":"<p>You must authenticate every REST API request. Options:</p> <ol> <li> <p>Shared Key (HMAC)</p> </li> <li> <p>Uses your storage account name + access key.</p> </li> <li>You compute an HMAC signature for each request (<code>Authorization: SharedKey &lt;account&gt;:&lt;signature&gt;</code>).</li> <li> <p>Very low-level but powerful.</p> </li> <li> <p>SAS (Shared Access Signature)</p> </li> <li> <p>Pre-signed URL with limited permissions (read/write/list/delete).</p> </li> <li> <p>Example:</p> <p><pre><code>https://mystorageaccount.blob.core.windows.net/mycontainer/myfile.csv?sv=2023-11-03&amp;ss=b&amp;srt=o&amp;sp=rw&amp;se=2025-09-16T18:00Z&amp;sig=&lt;signature&gt;\n</code></pre>    * Great for temporary access (e.g., users, apps, third parties).</p> </li> <li> <p>OAuth 2.0 / Azure AD (Recommended)</p> </li> <li> <p>Use Azure AD service principal or managed identity.</p> </li> <li> <p>Add bearer token in header:</p> <pre><code>Authorization: Bearer &lt;access_token&gt;\n</code></pre> </li> </ol>"},{"location":"azure/20-Azure_Storage_Rest_API/#4-common-rest-operations-blobs-adls-gen2","title":"\ud83d\udd11 4. Common REST Operations (Blobs / ADLS Gen2)","text":""},{"location":"azure/20-Azure_Storage_Rest_API/#container-operations","title":"Container Operations","text":"<ul> <li>Create Container</li> </ul> <p><pre><code>PUT https://&lt;account&gt;.blob.core.windows.net/&lt;container&gt;?restype=container\n</code></pre> * List Containers</p> <pre><code>GET https://&lt;account&gt;.blob.core.windows.net/?comp=list\n</code></pre>"},{"location":"azure/20-Azure_Storage_Rest_API/#blobfile-operations","title":"Blob/File Operations","text":"<ul> <li>Upload Blob</li> </ul> <pre><code>PUT https://&lt;account&gt;.blob.core.windows.net/&lt;container&gt;/&lt;blob&gt;\nx-ms-blob-type: BlockBlob\n</code></pre> <p>(Body contains file data)</p> <ul> <li>Download Blob</li> </ul> <pre><code>GET https://&lt;account&gt;.blob.core.windows.net/&lt;container&gt;/&lt;blob&gt;\n</code></pre> <ul> <li>Delete Blob</li> </ul> <pre><code>DELETE https://&lt;account&gt;.blob.core.windows.net/&lt;container&gt;/&lt;blob&gt;\n</code></pre>"},{"location":"azure/20-Azure_Storage_Rest_API/#adls-gen2-specific-hierarchical-namespace-enabled","title":"ADLS Gen2-Specific (Hierarchical Namespace enabled)","text":"<ul> <li>Create Directory</li> </ul> <pre><code>PUT https://&lt;account&gt;.dfs.core.windows.net/&lt;filesystem&gt;/&lt;directory&gt;?resource=directory\n</code></pre> <ul> <li>Create File</li> </ul> <pre><code>PUT https://&lt;account&gt;.dfs.core.windows.net/&lt;filesystem&gt;/&lt;directory&gt;/&lt;file&gt;?resource=file\n</code></pre> <ul> <li>Append Data</li> </ul> <pre><code>PATCH https://&lt;account&gt;.dfs.core.windows.net/&lt;filesystem&gt;/&lt;directory&gt;/&lt;file&gt;?action=append&amp;position=0\n</code></pre> <ul> <li>Flush Data (commit)</li> </ul> <pre><code>PATCH https://&lt;account&gt;.dfs.core.windows.net/&lt;filesystem&gt;/&lt;directory&gt;/&lt;file&gt;?action=flush&amp;position=&lt;length&gt;\n</code></pre>"},{"location":"azure/20-Azure_Storage_Rest_API/#5-versioning","title":"\ud83d\udd11 5. Versioning","text":"<ul> <li>Each request must specify an API version in the <code>x-ms-version</code> header.</li> <li>Example: <code>x-ms-version: 2023-11-03</code></li> <li>This ensures consistent behavior as Azure evolves.</li> </ul>"},{"location":"azure/20-Azure_Storage_Rest_API/#6-tools-for-testing","title":"\ud83d\udd11 6. Tools for Testing","text":"<ul> <li>Postman / Insomnia \u2192 manually call REST APIs.</li> <li>cURL for CLI-based requests.</li> <li>AzCopy (built on REST API).</li> <li>Azure Storage Explorer (GUI built on REST API).</li> </ul> <p>\u2705 Summary: The Azure Storage REST API is the backbone of ADLS &amp; Blob operations. You can:</p> <ul> <li>Authenticate with Shared Key, SAS, or Azure AD.</li> <li>Use Blob endpoints for standard blob storage.</li> <li>Use DFS endpoints (<code>.dfs.core.windows.net</code>) for ADLS Gen2 hierarchical namespace features.</li> <li>Issue standard HTTP verbs (<code>GET</code>, <code>PUT</code>, <code>PATCH</code>, <code>DELETE</code>) with required headers.</li> </ul>"},{"location":"azure/20-Azure_Storage_Rest_API/#python-example","title":"Python Example","text":"<pre><code>import requests\nfrom azure.identity import ClientSecretCredential\n\n# Azure AD app registration\ntenant_id = \"&lt;tenant-id&gt;\"\nclient_id = \"&lt;client-id&gt;\"\nclient_secret = \"&lt;client-secret&gt;\"\n\n# Authenticate\ncred = ClientSecretCredential(tenant_id, client_id, client_secret)\ntoken = cred.get_token(\"https://storage.azure.com/.default\").token\n\n# Storage details\naccount_name = \"mystorageaccount\"\nfilesystem = \"mycontainer\"\nfile_path = \"demo_folder/test2.txt\"\nbase_url = f\"https://{account_name}.dfs.core.windows.net/{filesystem}/{file_path}\"\n\n# Create file\nheaders = {\"Authorization\": f\"Bearer {token}\", \"x-ms-version\": \"2023-11-03\"}\nresp = requests.put(f\"{base_url}?resource=file\", headers=headers)\nprint(\"Create:\", resp.status_code, resp.text)\n\n# Append\ndata = b\"Secure upload via Azure AD!\"\nresp = requests.patch(f\"{base_url}?action=append&amp;position=0\",\n                      headers={**headers, \"Content-Length\": str(len(data))},\n                      data=data)\nprint(\"Append:\", resp.status_code, resp.text)\n\n# Flush\nresp = requests.patch(f\"{base_url}?action=flush&amp;position={len(data)}\",\n                      headers={**headers, \"Content-Length\": \"0\"})\nprint(\"Flush:\", resp.status_code, resp.text)\n</code></pre>"},{"location":"azure/21-Introduction_Azure_Data_Factory/","title":"Introduction Azure Data Factory","text":""},{"location":"azure/21-Introduction_Azure_Data_Factory/#introduction-to-azure-data-factory-adf","title":"\ud83c\udf10 Introduction to Azure Data Factory (ADF)","text":""},{"location":"azure/21-Introduction_Azure_Data_Factory/#what-is-adf","title":"\ud83d\udd39 What is ADF?","text":"<p>Azure Data Factory is Microsoft\u2019s cloud-based ETL &amp; data integration service. Think of it as a factory for moving and transforming data across different systems, both on-premises and in the cloud.</p> <p>It\u2019s a serverless service (you don\u2019t manage servers), and it allows you to build data pipelines that automate data movement, ingestion, and transformation.</p>"},{"location":"azure/21-Introduction_Azure_Data_Factory/#why-adf","title":"\ud83d\udd39 Why ADF?","text":"<ul> <li> <p>Companies often have data scattered across:</p> </li> <li> <p>Databases (SQL, Oracle, PostgreSQL, MongoDB, etc.)</p> </li> <li>Files (CSV, JSON, Parquet in blob storage, data lake, S3, etc.)</li> <li>SaaS apps (Salesforce, SAP, Dynamics, etc.)</li> <li>ADF connects these sources, moves data, and transforms it into a structured form for reporting, analytics, or AI/ML.</li> </ul>"},{"location":"azure/21-Introduction_Azure_Data_Factory/#core-concepts","title":"\ud83d\udd39 Core Concepts","text":"<ol> <li> <p>Pipelines</p> </li> <li> <p>A pipeline = workflow that defines a series of activities (like copying, transforming, loading).</p> </li> <li> <p>Example: Extract data from SQL \u2192 Transform in Databricks \u2192 Load into Synapse.</p> </li> <li> <p>Activities</p> </li> <li> <p>Steps inside a pipeline.</p> </li> <li> <p>Types:</p> <ul> <li>Data movement: Copy data from source to sink.</li> <li>Data transformation: Run Databricks notebooks, Spark jobs, SQL scripts.</li> <li>Control: Loops, conditions, wait, execute another pipeline.</li> </ul> </li> <li> <p>Datasets</p> </li> <li> <p>Represent the data structure (like a table, a file path, or a folder).</p> </li> <li> <p>Example: A dataset could point to a CSV file in Azure Blob Storage.</p> </li> <li> <p>Linked Services</p> </li> <li> <p>Connection information (credentials, endpoints).</p> </li> <li> <p>Example: Linked service for Azure SQL DB, one for Data Lake.</p> </li> <li> <p>Integration Runtime (IR)</p> </li> <li> <p>The compute infrastructure ADF uses to move/transform data.</p> </li> <li> <p>Types:</p> <ul> <li>Azure IR: Fully managed in the cloud (default).</li> <li>Self-hosted IR: For connecting on-prem systems.</li> <li>SSIS IR: For running SSIS packages.</li> </ul> </li> </ol>"},{"location":"azure/21-Introduction_Azure_Data_Factory/#common-use-cases","title":"\ud83d\udd39 Common Use Cases","text":"<ul> <li>ETL / ELT pipelines   Ingest raw data \u2192 transform into clean data \u2192 load into data warehouse (like Synapse or Snowflake).</li> <li>Data Lake Ingestion   Collect logs/files into Azure Data Lake Gen2.</li> <li>Hybrid Data Movement   Move data from on-prem SQL Server to Azure Synapse.</li> <li>Big Data Integration   Orchestrate Databricks notebooks, Spark, or HDInsight.</li> <li>Scheduling &amp; Monitoring   Automate jobs, monitor them with logs and alerts.</li> </ul>"},{"location":"azure/21-Introduction_Azure_Data_Factory/#example-workflow","title":"\ud83d\udd39 Example Workflow","text":"<ol> <li>Copy sales data from on-prem SQL Server into Azure Data Lake daily.</li> <li>Trigger a Databricks notebook to clean and enrich the data.</li> <li>Load processed data into Azure Synapse Analytics.</li> <li>Business analysts connect Power BI \u2192 create dashboards.</li> </ol>"},{"location":"azure/21-Introduction_Azure_Data_Factory/#benefits","title":"\ud83d\udd39 Benefits","text":"<ul> <li>Serverless \u2192 no infra to manage.</li> <li>Scalable \u2192 works for small files or terabytes.</li> <li>Cost-effective \u2192 pay-per-use.</li> <li>Rich connectors \u2192 100+ sources (DBs, files, APIs).</li> <li>Visual &amp; code-based \u2192 drag-and-drop UI + JSON definitions.</li> <li>Monitoring \u2192 built-in logging, retry, alerts.</li> </ul> <p>\ud83d\udc49 In short: ADF = a data pipeline orchestration tool in Azure. It moves, transforms, and organizes data so that downstream systems (like Synapse, Databricks, Power BI) can use it. Do you want me to go next into ADF architecture (with diagram) or step-by-step how to build your first pipeline?</p>"},{"location":"azure/22-Azure_Data_Factory_vs_Synapse/","title":"Azure Data Factory Vs Synapse","text":"<p>Great question \ud83d\ude80 \u2014 people often confuse Azure Data Factory (ADF) and Azure Synapse Analytics because both deal with data, but they serve different roles.</p> <p>Let\u2019s break it down clearly \ud83d\udc47</p>"},{"location":"azure/22-Azure_Data_Factory_vs_Synapse/#azure-data-factory-adf-vs-azure-synapse-analytics","title":"\ud83d\udd39 Azure Data Factory (ADF) vs Azure Synapse Analytics","text":"Feature Azure Data Factory (ADF) Azure Synapse Analytics Purpose Data integration &amp; orchestration (ETL/ELT). Moves and transforms data. Data storage, querying &amp; analytics (data warehouse). Stores large amounts of structured data for reporting. Type of Service ETL tool (like SSIS in the cloud). Data warehouse (like SQL Server on steroids). Main Role Move data between sources \u2192 clean/transform \u2192 load into storage/warehouse. Store processed data and allow BI tools (Power BI, Tableau) or analysts to query it fast. Data Sources Connects to 100+ sources (SQL, NoSQL, APIs, files, SaaS apps). Queries mainly relational/tabular data stored inside Synapse or external tables. Storage Doesn\u2019t store data (only moves it). Uses staging temporarily. Stores structured, query-ready data. Compute Uses Integration Runtime (IR) for data movement/transformation. Uses Massively Parallel Processing (MPP) engine for fast queries. Transformations - Built-in (mapping data flows) <ul> <li>Orchestration of external compute (Databricks, HDInsight, Synapse pipelines) | - In-database transformations via SQL (T-SQL, stored procs) |   | Best for | - Data pipelines (ETL/ELT)</li> <li>Moving from on-prem/cloud \u2192 Azure</li> <li>Orchestrating Databricks or ML flows | - Business intelligence</li> <li>Reporting dashboards (Power BI)</li> <li>Analyzing terabytes of structured data |   | Pricing | Pay-per-use (based on activities &amp; data movement). | Pay-per-use (on-demand SQL) or reserved (dedicated pools). |</li> </ul>"},{"location":"azure/22-Azure_Data_Factory_vs_Synapse/#how-they-work-together","title":"\ud83d\udd39 How They Work Together","text":"<p>\ud83d\udc49 Typically, you use ADF + Synapse together in a modern data architecture:</p> <ol> <li>ADF \u2192 Extracts raw sales data from SQL Server, API, and Blob storage.</li> <li>ADF \u2192 Cleans/transforms data (orchestrates Databricks/Spark).</li> <li>ADF \u2192 Loads processed data into Synapse Analytics.</li> <li>Synapse \u2192 Analysts query data with SQL or Power BI for dashboards.</li> </ol>"},{"location":"azure/22-Azure_Data_Factory_vs_Synapse/#simple-analogy","title":"\ud83d\udd39 Simple Analogy","text":"<ul> <li>ADF = Data factory workers \ud83c\udfed \u2192 They collect, clean, and deliver the materials (data).</li> <li>Synapse = Warehouse &amp; analysts \ud83c\udfe2 \u2192 They store the materials neatly and let people quickly find/analyze what they need.</li> </ul> <p>\u2705 In short:</p> <ul> <li>Use ADF when you need to move/transform/orchestrate data.</li> <li>Use Synapse when you need to store/analyze/report data.</li> </ul> <p>Do you want me to draw you a step-by-step flow showing ADF + Synapse + Power BI in a pipeline so it\u2019s crystal clear how they connect?</p>"},{"location":"azure/23-Azure_Data_Factory_Architecture/","title":"\ud83c\udfd7 Azure Data Factory Architecture (In Depth)","text":"<p>At a high level, ADF has 5 core building blocks:</p> <ol> <li>Pipelines</li> <li>Activities</li> <li>Datasets</li> <li>Linked Services</li> <li>Integration Runtimes</li> </ol> <p>Let\u2019s explore step by step.</p>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#1-control-plane-vs-data-plane","title":"\ud83d\udd39 1. Control Plane vs Data Plane","text":"<p>ADF runs on a serverless architecture inside Azure. It is split into two planes:</p> <ul> <li> <p>Control Plane:</p> </li> <li> <p>Manages metadata, pipelines, triggers, monitoring.</p> </li> <li>What you see in the ADF Studio (the UI).</li> <li> <p>Stores JSON definitions of pipelines in Azure.</p> </li> <li> <p>Data Plane:</p> </li> <li> <p>Where the actual data movement/processing happens.</p> </li> <li>Uses Integration Runtime (IR) to copy or transform data.</li> <li>Example: Copying a file from On-prem SQL \u2192 Blob storage.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#2-core-components","title":"\ud83d\udd39 2. Core Components","text":""},{"location":"azure/23-Azure_Data_Factory_Architecture/#pipelines","title":"\u2705 Pipelines","text":"<ul> <li>A pipeline = workflow.</li> <li>Groups multiple activities into a sequence/graph.</li> <li> <p>Example:</p> </li> <li> <p>Step 1: Copy sales data from SQL \u2192 Data Lake</p> </li> <li>Step 2: Run Databricks transformation</li> <li>Step 3: Load into Synapse</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#activities","title":"\u2705 Activities","text":"<ul> <li>Steps inside a pipeline.</li> <li> <p>Types:</p> </li> <li> <p>Data Movement \u2192 Copy Activity (move data between stores).</p> </li> <li>Data Transformation \u2192 Mapping Data Flows, Databricks, Synapse SQL, HDInsight.</li> <li>Control Activities \u2192 If/Else, ForEach loops, Web calls, Execute pipeline.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#datasets","title":"\u2705 Datasets","text":"<ul> <li>Definition of data structure you want to read/write.</li> <li>Think of it as a pointer to data inside a storage system.</li> <li> <p>Example:</p> </li> <li> <p>A dataset for \"SalesTable in SQL DB\".</p> </li> <li>A dataset for \"CSV file in Data Lake folder\".</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#linked-services","title":"\u2705 Linked Services","text":"<ul> <li>Connection info (credentials + endpoints).</li> <li>Similar to connection strings.</li> <li> <p>Examples:</p> </li> <li> <p>Linked Service for Azure SQL DB</p> </li> <li>Linked Service for Blob Storage</li> <li>Linked Service for On-prem SQL via Self-hosted IR</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#integration-runtime-ir","title":"\u2705 Integration Runtime (IR)","text":"<p>This is the engine that actually runs ADF activities. Types of IR:</p> <ol> <li>Azure IR \u2192 Managed, serverless compute (default). Used for copying data in the cloud.</li> <li>Self-Hosted IR \u2192 Installed on your on-prem VM. Used for hybrid (on-prem \u2194 cloud).</li> <li>Azure SSIS IR \u2192 Run legacy SSIS packages in Azure.</li> </ol> <p>\ud83d\udccc Example:</p> <ul> <li>If your data is in on-prem SQL Server, you must install Self-hosted IR in your data center to move data to Azure.</li> <li>If your data is in Azure Blob \u2192 Synapse, then Azure IR handles it.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#3-orchestration-layer","title":"\ud83d\udd39 3. Orchestration Layer","text":"<ul> <li> <p>Pipelines are triggered by:</p> </li> <li> <p>Schedule (daily, hourly)</p> </li> <li>Event-based (new file arrives in Blob)</li> <li>Manual/REST API call</li> <li>Pipelines can branch, loop, or run in parallel.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#4-monitoring-layer","title":"\ud83d\udd39 4. Monitoring Layer","text":"<ul> <li>Built-in monitoring in ADF Studio.</li> <li>Shows pipeline runs, activity runs, duration, errors.</li> <li>Integrated with Azure Monitor &amp; Log Analytics for alerts.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#5-security-layer","title":"\ud83d\udd39 5. Security Layer","text":"<ul> <li>Authentication: Managed Identity, Service Principal, Key Vault.</li> <li>Data never passes through control plane \u2192 only through IR.</li> <li>Network isolation possible with VNet integration.</li> </ul>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#6-typical-data-flow-example","title":"\ud83d\udd39 6. Typical Data Flow Example","text":"<p>Scenario: Ingest daily sales data from On-prem SQL to Synapse</p> <ol> <li>Trigger fires daily.</li> <li>Pipeline starts.</li> <li>Copy Activity (using Self-hosted IR) moves data \u2192 Azure Data Lake.</li> <li>Mapping Data Flow Activity cleans &amp; transforms data.</li> <li>Copy Activity loads transformed data \u2192 Synapse DW.</li> <li>Monitoring logs success/failure.</li> </ol>"},{"location":"azure/23-Azure_Data_Factory_Architecture/#architecture-diagram-explained-in-words","title":"\ud83d\udd39 Architecture Diagram (Explained in Words)","text":"<p>Imagine:</p> <ul> <li>Top Layer (UI + Control Plane) \u2192 ADF Studio where you design pipelines.</li> <li>Middle Layer (Orchestration) \u2192 Pipelines + Triggers + Activities.</li> <li>Bottom Layer (Execution via IR) \u2192 Data is copied/transformed by IR across data sources.</li> </ul> <p>So:</p> <ul> <li>Control Plane = Think \"Blueprint + Control Room\".</li> <li>Integration Runtime (Data Plane) = Think \"Workers doing the job\".</li> </ul> <p>\u2705 In short: ADF is an orchestrator + integration engine with pipelines as workflows, activities as tasks, linked services as connections, datasets as pointers, and IR as the worker that executes jobs.</p>"},{"location":"azure/24-ADF_Triggers_Intro/","title":"Azure Data Factory Triggers","text":""},{"location":"azure/24-ADF_Triggers_Intro/#triggers-in-azure-data-factory","title":"Triggers in Azure Data Factory","text":""},{"location":"azure/24-ADF_Triggers_Intro/#schedule-trigger","title":"Schedule Trigger","text":"<ul> <li>Runs on calendar clock</li> <li>Supports periodic and specific times</li> <li>Trigger to pipeline is many to many</li> <li>Can be scheduled only at a future point in time</li> </ul>"},{"location":"azure/24-ADF_Triggers_Intro/#tumbling-window-trigger","title":"Tumbling Window Trigger","text":"<ul> <li>Runs at periodic intervals</li> <li>Windows are fixed size and non overlapping</li> <li>Can be scheduled for past windows/slices</li> <li>Trigger to pipeline is one to one</li> </ul>"},{"location":"azure/24-ADF_Triggers_Intro/#event-trigger","title":"Event Trigger","text":"<ul> <li>Runs in response to events</li> <li>Events can be creation or deletion of blobs/files</li> <li>Trigger to pipeline is many to many.</li> </ul>"},{"location":"azure/25-ADF_Parameters/","title":"Azure Data Factory Parameters","text":""},{"location":"azure/25-ADF_Parameters/#parameters-in-azure-data-factory","title":"Parameters in Azure Data Factory","text":"<p>Paramters are external values that can be passed into pipelines, datasets and linked services. The value cannot be changed inside a pipeline.</p> <p>Variables are internal values set inside a pipeline that can be changed using Set Variable or Append Variable Activity.</p>"},{"location":"azure/25-ADF_Parameters/#steps-to-parameterize-a-pipeline","title":"Steps to Parameterize a Pipeline","text":"<ol> <li>Source Dataset - Set Parameter for <code>relativeURL</code></li> </ol> <ol> <li>Source Dataset - Define the Paramter in Connections</li> </ol> <ol> <li>Sink Dataset - Parameterize File Name</li> </ol> <ol> <li>Pipeline Parameterization - Add two variables</li> </ol> <ol> <li>Pipeline Paramterization - Pass the variables to the activity</li> </ol> <p>It asks us to pass relativeURL paramter which we do via the variables defined above.</p> <p></p> <p>Similarly for the sink as well.</p>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/","title":"Data-format deep dive - Avro, CSV, JSON, Parquet - and how to tune Avro for performance","text":""},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#1-quick-comparison-at-a-glance","title":"1) Quick comparison (at-a-glance)","text":"Format Schema Layout Splittable Best for Pros Cons CSV none (ad-hoc) row text yes (if uncompressed / splittable compression) simple exports, small datasets, ad-hoc loads human readable, simple no types, ambiguous parsing, large on disk, slow for big data, no nested types JSON / NDJSON implicit (can infer) row text (JSON objects) yes (if newline-delimited + splittable compression) logs, event transport, semi-structured data flexible, supports nesting heavier than binary, slow parsing, schema inference costly Avro explicit schema in file row-oriented binary (blocks) yes \u2014 block syncronization markers make it splittable. streaming / message formats, ingest layer, schema evolution compact binary, schema evolution, fast (serialize/deserialize), splittable, compressible row format (not ideal for OLAP), less efficient column pruning than Parquet Parquet schema (stored in file metadata) columnar yes analytics, OLAP, read-heavy workloads excellent compression &amp; IO for columnar reads, predicate pushdown, column pruning heavier write cost, not ideal for single-record streaming"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#2-more-detail-on-each-format","title":"2) More detail on each format","text":""},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#csv","title":"CSV","text":"<ul> <li>Plain text, each row is a line; columns separated by delimiter.</li> <li>No typed schema: everything is a string until you parse it. Date/number parsing is uncertain.</li> <li>Compression: you can gzip/bzip2 files \u2014 gzip is not splittable, so parallel reads on S3/HDFS are limited; bzip2 is splittable but slow.</li> <li>Use when: small/medium tabular exports, exchange with tools that expect CSV. Avoid as a canonical lake format.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#json-and-ndjson-json-lines","title":"JSON (and NDJSON / JSON Lines)","text":"<ul> <li>Supports nested records/arrays; NDJSON (one JSON object per line) is the scalable variant.</li> <li>Schema inference is slower and brittle \u2014 in Spark use an explicit schema to avoid full-file scans.</li> <li>Multiline JSON (arrays) is expensive and should be avoided for large datasets.</li> <li>Good for flexible ingestion / logs / event vocabularies, but heavy for analysis.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#avro-focus-internals-and-strengths","title":"Avro (focus \u2014 internals and strengths)","text":"<ul> <li>Binary, row-oriented format. Each file contains a header with the writer schema, then series of blocks (many records), each block followed by a sync marker. Because compression happens per-block and sync markers mark block boundaries, Avro container files are splittable and suitable for parallel processing.</li> <li>Schema is stored in the file header, which enables robust schema evolution (writer/reader schema resolution, default values, etc.). Great for message buses (Kafka) and ingest pipelines.</li> <li>Compression: Avro supports block-level codecs (null/deflate/snappy/bzip2; many ecosystems also support xz/zstandard). Codec support depends on library/runtime. In Spark you can control codec via <code>compression</code> option or <code>spark.sql.avro.compression.codec</code>. </li> <li>When to use Avro: streaming/ingest, durable event storage, where schema evolution and small record serialization speed matter.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#parquet","title":"Parquet","text":"<ul> <li>Columnar storage: data is stored by column chunks \u2192 huge IO savings when queries touch a few columns. Excellent compression (dictionary, bit-packing) and predicate pushdown. Best format for analytical queries and BI workloads. ([Upsolver][3])</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#3-why-avro-vs-parquet-practical-pattern","title":"3) Why Avro vs Parquet (practical pattern)","text":"<ul> <li>Use Avro for ingestion, event logs, Kafka messages, or when schema evolution and fast row serialization are required.</li> <li>Convert Avro -&gt; Parquet for analytics and dashboards (Parquet\u2019s columnar layout + predicate pushdown + column pruning gives much faster query times and lower IO).</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#4-tuning-avro-for-performance-principles-actionable-steps","title":"4) Tuning Avro for performance \u2014 principles + actionable steps","text":"<p>Below are the levers you can pull when you write/produce Avro files and when you read them (Spark/Hadoop/Java examples included).</p>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#a-choose-the-right-compression-codec","title":"A. Choose the right compression codec","text":"<p>Tradeoffs: CPU cost vs compression ratio vs read speed.</p> <ul> <li>snappy \u2014 fast compression/decompression, moderate ratio \u2192 generally best for throughput-sensitive pipelines.</li> <li>deflate (zlib) \u2014 better compression ratio; adjustable level (slower CPU).</li> <li>zstandard / xz / bzip2 \u2014 higher compression ratio but higher CPU; zstd is often a good space/speed compromise if available.   Recommendation: start with snappy for ingestion pipelines; consider deflate or zstd for archival data where IO cost is important. In Spark you can configure codec via <code>spark.sql.avro.compression.codec</code> or <code>option(\"compression\", \"&lt;codec&gt;\")</code>. ([Apache Spark][5])</li> </ul> <p>Spark example</p> <pre><code>// Option 1: set global conf\nspark.conf.set(\"spark.sql.avro.compression.codec\", \"snappy\")\nspark.conf.set(\"spark.sql.avro.deflate.level\", \"6\")   // if using deflate\n\n// Then write\ndf.write.format(\"avro\").mode(\"overwrite\").save(\"/mnt/data/orders.avro\")\n\n// Or explicitly (option is supported too)\ndf.write.format(\"avro\").option(\"compression\", \"snappy\").save(\"/mnt/data/orders.avro\")\n</code></pre> <p>(If <code>compression</code> option is not set, <code>spark.sql.avro.compression.codec</code> is used.)</p>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#b-tune-block-sync-interval-the-avro-block-size","title":"B. Tune block / sync interval (the Avro \u201cblock size\u201d)","text":"<ul> <li>Avro groups many records into a block and writes a sync marker after each block. Larger blocks = fewer markers, better compression (more data to compress), and fewer IO operations; but larger blocks mean more memory used during compression and potentially larger re-read buffer.</li> <li>The Avro API exposes <code>setSyncInterval(...)</code> on the <code>DataFileWriter</code> so you can adjust bytes per block. Suggested values historically range from a few KB up to a few MB; Avro\u2019s docs recommend values between \\~2KB and 2MB, and a commonly used default was increased to 64KB for better compression. ([Apache Avro][6])</li> </ul> <p>Java/Scala example (low-level writer)</p> <pre><code>import org.apache.avro.file.{CodecFactory, DataFileWriter}\nimport org.apache.avro.generic.{GenericDatumWriter, GenericRecord}\n\n// create writer...\nval datumWriter = new GenericDatumWriter[GenericRecord](schema)\nval dfw = new DataFileWriter(datumWriter)\ndfw.setCodec(CodecFactory.snappyCodec())       // choose codec\ndfw.setSyncInterval(64 * 1024)                 // e.g. 64KB block size\ndfw.create(schema, new java.io.File(\"orders.avro\"))\n// write records...\ndfw.close()\n</code></pre> <p>(Use larger sync interval for higher throughput/space efficiency; use smaller interval for lower memory footprint and faster single-record availability.)</p>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#c-file-size-and-partitioning","title":"C. File size and partitioning","text":"<ul> <li>Avoid tiny files - small-file overhead kills performance on distributed filesystems and object stores. Aim for hundreds of MB per file (common heuristic: 128MB\u20131GB depending on workload and compute). For Avro ingestion, batch/compact files to reasonable sizes.</li> <li>Use partitioning on high-cardinality but selectively chosen columns (date/year/month, region) to enable pruning on read.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#d-schema-design-and-types","title":"D. Schema design and types","text":"<ul> <li>Prefer primitive types where possible (ints/longs/dates) instead of stringly-typed fields. Use Avro logical types (date/timestamp/decimal) appropriately to avoid costly conversions.</li> <li>Avoid very deep nesting or very wide records if your queries often need only a small subset of fields - Avro is row-oriented, so you pay to read full rows. For analytics consider storing the same data in Parquet.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#e-read-time-optimizations-in-spark","title":"E. Read-time optimizations in Spark","text":"<ul> <li>Partition pruning (partition columns) is your friend; Avro does not give the same column pruning benefits as Parquet, so pruning by file layout is essential.</li> <li>Use predicate pushdown only where supported by the engine - Parquet has stronger pushdown/pruning. For Avro rely on partitioning + filter pushdown at file level when possible.</li> <li>Avoid schema inference on read - pass explicit schemas to <code>spark.read.format(\"avro\").schema(mySchema).load(...)</code> to speed parsing.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#f-compaction-lifecycle","title":"F. Compaction + lifecycle","text":"<ul> <li>For continuous ingestion (many small Avro files), run periodic compaction jobs to merge small files into larger Avro files with tuned block sizes and codec settings. This reduces metadata overhead and improves read throughput.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#5-concrete-tuning-checklist-quick-actions-you-can-apply-now","title":"5) Concrete tuning checklist \u2014 quick actions you can apply now","text":"<ol> <li>Codec: use <code>snappy</code> for throughput; <code>deflate</code>/<code>zstd</code> for smaller archive sizes. (Spark: <code>spark.conf.set(\"spark.sql.avro.compression.codec\", \"snappy\")</code>.) </li> <li>Block size / sync interval: increase from default to e.g. 64KB\u20131MB (measure CPU/memory). Use <code>DataFileWriter.setSyncInterval(...)</code> for low-level writers. </li> <li>File size: avoid tiny files \u2014 compact to 128MB\u20131GB per file.</li> <li>Partitioning: partition on query-selective columns so reads can prune files.</li> <li>Schema: provide explicit schemas at read time; use logical types for decimals/timestamps.</li> <li>Convert to Parquet for analytics: ingest with Avro, transform to Parquet for OLAP workloads. Parquet gives column pruning and faster scans. </li> </ol>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#6-small-worked-example-ingest-avro-then-convert-to-parquet-spark","title":"6) Small worked example \u2014 ingest Avro then convert to Parquet (Spark)","text":"<pre><code>// 1) write Avro (ingest)\nspark.conf.set(\"spark.sql.avro.compression.codec\", \"snappy\")\ndfIngest.write.format(\"avro\")\n  .mode(\"append\")\n  .save(\"/lake/landing/orders/\")\n\n// (compaction step can merge small Avro files here)\n\n// 2) convert to Parquet for analytics (optimised)\nval avroDF = spark.read.format(\"avro\").load(\"/lake/landing/orders/\")\navroDF.write.mode(\"overwrite\").partitionBy(\"dt\").parquet(\"/lake/curated/orders_parquet/\")\n</code></pre> <p>This pattern gives you the ingest-friendly Avro layer + analytics-friendly Parquet layer.</p>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#7-limitations-when-avro-tuning-is-not-enough","title":"7) Limitations / when Avro tuning is not enough","text":"<ul> <li>If analytic queries require selecting a few columns from very wide rows, Avro will still read whole rows \u2014 use Parquet/ORC.</li> <li>If your workloads are read-heavy and interactive, convert to columnar formats and use appropriate file sizes and clustering.</li> </ul>"},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#avro-visual","title":"Avro Visual","text":""},{"location":"data-formats/01-Data_Format_Deep_Dive_Pt1/#parquet-visual","title":"Parquet Visual","text":""},{"location":"data-formats/02-Parquet_Format_Internals/","title":"Parquet Format Internals","text":""},{"location":"data-formats/02-Parquet_Format_Internals/#lecture-14-parquet-file-internals","title":"Lecture 14 : Parquet File Internals","text":"<p>There are two types of file formats:</p> <ul> <li>Columnar Based and Row Based</li> </ul>"},{"location":"data-formats/02-Parquet_Format_Internals/#physical-storage-of-data-on-disk","title":"Physical Storage of Data on Disk","text":""},{"location":"data-formats/02-Parquet_Format_Internals/#write-once-read-many","title":"Write Once Read Many","text":"<p>The funda of big data is write once read many.</p> <ul> <li>We dont need all the columns for analytics of big data, so columnar storage is the best.</li> <li>If we store in row based format then we need to jump many memory racks to be able to get the data we need.</li> </ul> <p></p> <ul> <li>OLTP generally use row base4d file format.</li> <li>I/O should be reduced so OLAP uses columnar format.</li> </ul>"},{"location":"data-formats/02-Parquet_Format_Internals/#why-columnar-format-may-not-be-the-best","title":"Why Columnar format may not be the best?","text":"<p>In above case we can get col1 and col2 easily but for col10 we still need to scan the entire file.</p> <p>To tackle this:</p> <p>Let's say we have 100 million total rows.</p> <p>We can store 100,000 records at a time, continuously in one row, then the next 100,000 records in next row and so on in hybrid format.</p> <p></p>"},{"location":"data-formats/02-Parquet_Format_Internals/#logical-partitioning-in-parquet","title":"Logical Partitioning in Parquet","text":"<p>Let's day we have 500mb data, each row group by default has 128 mb data, so we will have 4 row groups. Each row group will have some metadata attached to it.</p> <p>In our example let's say one row group has 100000 records. The column is futher stored as a page.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#runlength-encoding-and-bitpacking","title":"Runlength Encoding and Bitpacking","text":"<p>Suppose we have 10 lakh records but there can be say 4 countries.</p> <p>So parquet actually creates a dictionary of key value pair with key as int starting from 0 to 3 and then in the dictionary encoded data, we can see the keys being used insted of country name.</p> <p></p>"},{"location":"data-formats/02-Parquet_Format_Internals/#more-details-on-parquet","title":"More details on Parquet","text":"<p>The row-wise formats store data as records, one after another. This format works well when accessing entire records frequently. However, it can be inefficient when dealing with analytics, where you often only need specific columns from a large dataset.</p> <p></p> <p>Imagine a table with 50 columns and millions of rows. If you\u2019re only interested in analyzing 3 of those columns, a row-wise format would still require you to read all 50 columns for each row.</p> <p>Columnar formats address this issue by storing data in columns instead of rows. This means that when you need specific columns, you can read only the columnsdata you need, significantly reducing the amount of data scanned.</p> <p></p> <p>However, storing data in a columnar format has some downsides. The record write or update operation requires touching multiple column segments, resulting in numerous I/O operations. This can significantly slow the write performance, especially when dealing with high-throughput writes.</p> <p>When queries involve multiple columns, the database system must reconstruct the records from separate columns. The cost of this reconstruction increases with the number of columns involved in the query.</p> <p>The hybrid format combines the best of both worlds. The format groups data into \"row groups,\" each containing a subset of rows (aka horizontal partition). Within each row group, data for each column is called a \u201ccolumn chunk\" (aka vertical partition).</p> <p></p> <p>In the row group, these chunks are guaranteed to be stored contiguously on disk.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#terminologies","title":"Terminologies","text":"<p>A Parquet file is composed of:</p> <p>Row Groups: Each row group contains a subset of the rows in the dataset. Data is organized into columns within each row group, each stored in a column chunk.</p> <p>Column Chunk: A chunk is the data for a particular column in the row group. Column chunk is further divided into pages.</p> <p>Pages: A page is the smallest data unit in Parquet. There are several types of pages, including data pages (actual data), dictionary pages (dictionary-encoded values), and index pages (used for faster data lookup).</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#metadata-types-in-parquet","title":"Metadata Types in Parquet","text":"<p>Magic number: The magic number is a specific sequence of bytes (PAR1) located at the beginning and end of the file. It is used to verify whether it is a valid Parquet file.</p> <p>FileMetadata: Parquet stores FileMetadata in the footer of the file. This metadata provides information like the number of rows, data schema, and row group metadata. Each row group metadata contains information about its column chunks (ColumnMetadata), such as the encoding and compression scheme, the size, the page offset, the min/max value of the column chunk, etc. The application can use information in this metadata to prune unnecessary data.</p> <p>PageHeader: The page header metadata is stored with the page data and includes information such as value, definition, and repetition encoding. Parquet also stores definition and repetition levels to handle nested data. The application uses the header to read and decode the data.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#how-is-data-written-into-parquet","title":"How is data written into Parquet?","text":"<ul> <li> <p>The application issues a written request with parameters like the data, the compression and encoding scheme for each column (optional), the file scheme (write to one or multiple files), etc.</p> </li> <li> <p>The Parquet Writer first collects information, such as the data schema, the null appearance, the encoding scheme, and all the column types recorded in FileMetadata.</p> </li> <li> <p>The Writer writes the magic number at the beginning of the file.</p> </li> <li> <p>Then, it calculates the number of row groups based on the row group\u2019s max size (configurable) and the data\u2019s size. This step also determines which subset of data belongs to which row group.</p> </li> <li> <p>For each row group, it iterates through the column list to write each column chunk for the row group. This step will use the compression scheme specified by the user (the default is none) to compress the data when writing the chunks.</p> </li> <li> <p>The chunk writing process begins by calculating the number of rows per page using the max page size and the chunk size. Next, it will try to calculate the column's min/max statistic. (This calculation is only applied to columns with a measurable type, such as integer or float.)</p> </li> <li> <p>Then, the column chunk is written page by page sequentially. Each page has a header that includes the page\u2019s number of rows, the page\u2019s encoding for data, repetition, and definition. The dictionary page is stored with its header before the data page if dictionary encoding is used.</p> </li> <li> <p>After writing all the pages for the column chunk, the Parquet Writer constructs the metadata for that chunk, which includes information like the column's min/max, the uncompressed/compressed size, the first data page offset, and the first dictionary page offset.</p> </li> <li> <p>The column chunk writing process continues until all columns in the row group are written to disk contiguously. The metadata for each column chunk is recorded in the row group metadata.</p> </li> <li> <p>After writing all the row groups, all row groups\u2019 metadata is recorded in the FileMetadata.</p> </li> <li> <p>The FileMetadata is written to the footer.</p> </li> <li> <p>The process finishes by writing the magic number at the end of the file.</p> </li> </ul>"},{"location":"data-formats/02-Parquet_Format_Internals/#how-is-data-read-from-parquet","title":"How is data read from Parquet?","text":"<ul> <li> <p>The application issues a read request with parameters such as the input file, filters to limit the number of read row groups, the set of desired columns, etc.</p> </li> <li> <p>If the application requires verification that it\u2019s reading a valid Parquet file, the reader will check if there is a magic number at the beginning and end of the file by seeking the first and last four bytes.</p> </li> <li> <p>It then tries to read the FileMetadata from the footer. It extracts information for later use, such as the file schema and the row group metadata.</p> </li> <li> <p>If filters are specified, they will limit the scanned row groups by iterating over every row group and checking the filters against each chunk\u2019s statistics. If it satisfies the filters, this row group is appended to the list, which is later used to read.</p> </li> <li> <p>The reader defines the column list to read. If the application specifies a subset of columns it wants to read, the list only contains these columns.</p> </li> <li> <p>The next step is reading the row groups. The reader will iterate through the row group list and read each row group.</p> </li> <li> <p>The reader will read the column chunks for each row group based on the column list. It used ColumnMetadata to read the chunk.</p> </li> <li> <p>When reading the column chunk for the first time, the reader locates the position of the first data page (or dictionary page if dictionary encoding is used) using the first page offset in the column metadata. From this position, the reader reads the pages sequentially until no pages are left.</p> </li> <li> <p>To determine whether any data remains, the reader tracks the current number of read rows and compares it to the chunk\u2019s total number of rows. If the two numbers are equal, the reader has read all the chunk data.</p> </li> <li> <p>To read and decode each data page, the reader visits the page header to collect information like the value encoding, the definition, and the repetition level encoding.</p> </li> <li> <p>After reading all the row groups\u2019 column chunks, the reader moves to read the following row groups.</p> </li> <li> <p>The process continues until all the row groups in the row group list are read.</p> </li> </ul> <p>Because the Parquet file can be stored in multiple files, the application can read them simultaneously.</p> <p>In addition, a single Parquet file is partitioned horizontally (row groups) and vertically (column chunks), which allows the application to read data in parallel at the row group or column level.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#olap-workload-example","title":"OLAP Workload Example","text":""},{"location":"data-formats/02-Parquet_Format_Internals/#demo","title":"Demo","text":"<p><code>parquet-tools inspect &lt;filename&gt;</code></p> <p> </p> <p>Gives some file and brief column level metadata.</p> <p><code>parquet_file.metadata.row_group(0).column_group(0)</code> </p> <p>Compression is GZIP </p> <p>Encoding is explained on top.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#bitpacking-advantage","title":"Bitpacking Advantage","text":"<ul> <li>Bitpacking helps in compressing the bits so in above case we just have 4 unique values and hence we need just 2 bytes.</li> <li>Query in seconds for running select on csv,parquet etc..</li> </ul>"},{"location":"data-formats/02-Parquet_Format_Internals/#summary","title":"Summary","text":"<ul> <li> <p>Here the actual data is stored in the pages and it has metadata like min,max and count.</p> </li> <li> <p>Let's say we need to find out people less than 18 years age</p> </li> </ul> <p></p> <p>Here when we divide data into row groups, we dont need to do any IO read operation on Row group 2, it saves lot of time and optimize performance.</p> <p>The above concept is called Predicate Pushdown.</p>"},{"location":"data-formats/02-Parquet_Format_Internals/#projection-pruning","title":"Projection Pruning","text":"<p>Projection Pruning means we dont read IO from columns that are not part of the select query or that arent required for any join.</p>"},{"location":"databricks/","title":"Databricks","text":"<p>This is the overview page for Databricks.</p>"},{"location":"databricks/01-Azure_Databricks_UC_Creation/","title":"Azure Databricks Uc Creation","text":""},{"location":"databricks/01-Azure_Databricks_UC_Creation/#how-to-configure-and-create-unity-catalog-in-azure","title":"How to configure and create Unity Catalog in Azure","text":"<p>We need to setup Storage Account under the same RG where Dsatabricks Workspace is deployed.</p> <p>Also we need to create a UC Databricks Connector to connect to the resource group.</p> <p>Then give Storage Data Contributor access to the UC Databrticks Connector Managed Identity.</p> <p>Go to Databricks Login Page and select the account that is the admin.</p> <p>Go to catalog and create a new metastore.  </p> <p>Fill in the details</p> <p></p> <p></p> <p>Imp : Login from your admin account</p> <p>We can now see more options like Add Catalog, Add Credentials, Create Volume after UC is enabled.</p> <p></p>"},{"location":"databricks/02-Databricks_UC_Introduction/","title":"Databricks Uc Introduction","text":""},{"location":"databricks/02-Databricks_UC_Introduction/#databricks-unity-catalog-introduction","title":"Databricks Unity Catalog Introduction","text":"<p>\u26a0\ufe0f Metadata in Metastore is stored on control plane and actual data is on data plane.</p> <p></p> <p>Catalog is called data securable object because without having access to <code>USE CATALOG</code> we cannot query any data objects.</p>"},{"location":"databricks/02-Databricks_UC_Introduction/#securable-data-objects-that-we-can-use-to-manage-external-data-sources","title":"Securable Data Objects that we can use to manage external data sources","text":"<p>In addition to the database objects and AI assets that are contained in schemas, Unity Catalog also uses the following securable objects to manage access to cloud storage and other external data sources and services:</p> <p>Storage credentials, which encapsulate a long-term cloud credential that provides access to cloud storage.</p> <p>External locations, which reference both a cloud storage path and the storage credential required to access it. External locations can be used to create external tables or to assign a managed storage location for managed tables and volumes. </p> <p>Connections, which represent credentials that give read-only access to an external database in a database system like MySQL using Lakehouse Federation. </p> <p>Service credentials, which encapsulate a long-term cloud credential that provides access to an external service. See Create service credentials.</p>"},{"location":"databricks/02-Databricks_UC_Introduction/#admin-roles","title":"Admin Roles","text":"<p>Account admins: can create metastores, link workspaces to metastores, add users, and assign privileges on metastores.</p> <p>Workspace admins: can add users to a workspace, and manage many workspace-specific objects like jobs and notebooks. Depending on the workspace, workspace admins can also have many privileges on the metastore that is attached to the workspace.</p> <p>Metastore admins: This optional role is required if you want to manage table and volume storage at the metastore level. It is also convenient if you want to manage data centrally across multiple workspaces in a region.</p>"},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/","title":"Databricks Managed External Tables Hive","text":""},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#managed-and-external-tables-in-hive-metastore","title":"Managed and External Tables in Hive Metastore","text":""},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#1-your-command","title":"1\ufe0f\u20e3 Your command","text":"<pre><code>CREATE TABLE IF NOT EXISTS dev.bronze.table_1\nLOCATION 'dbfs:/tmp/tmp_ext'\n</code></pre> <ul> <li>You are explicitly setting a LOCATION.</li> <li>You are creating the table in the Hive Metastore (because you didn\u2019t specify Unity Catalog).</li> <li>That means: this is an external table, not a managed one.</li> </ul>"},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#2-where-does-the-data-go","title":"2\ufe0f\u20e3 Where does the data go?","text":"<ul> <li>If you don\u2019t specify a LOCATION and use Hive Metastore \u2192 the data is stored in the Databricks-managed DBFS root (a hidden storage account in the Databricks Managed Resource Group). That\u2019s the \u201cmanaged table\u201d case.</li> <li>Since you did specify <code>LOCATION dbfs:/tmp/tmp_ext</code> \u2192 the data is stored in DBFS, which itself is backed by the same hidden storage account unless you mounted another storage.</li> <li>So in your case, yes \u2014 it is still in the Databricks-managed storage account, but specifically under the DBFS <code>/tmp</code> folder, not the default warehouse root (<code>/user/hive/warehouse</code>).</li> </ul>"},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#3-important-distinction-hive-metastore-vs-unity-catalog","title":"3\ufe0f\u20e3 Important distinction: Hive Metastore vs Unity Catalog","text":"<ul> <li> <p>Hive Metastore tables:</p> </li> <li> <p>Data goes to DBFS (Databricks-managed storage account).</p> </li> <li> <p>Security is weaker (workspace-scoped, not account-scoped).</p> </li> <li> <p>Unity Catalog tables:</p> </li> <li> <p>Data must go to a customer-managed external storage account (ADLS Gen2 / S3 / GCS).</p> </li> <li>Databricks does not put UC data into its hidden resource group storage.</li> </ul>"},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#4-managed-vs-external-recap","title":"4\ufe0f\u20e3 Managed vs External recap","text":"<ul> <li>Managed table (no LOCATION) \u2192 Hive Metastore puts data in Databricks-managed DBFS root.</li> <li>External table (with LOCATION) \u2192 Data goes exactly where you point it (<code>dbfs:/...</code> \u2192 Databricks storage, <code>abfss:/...</code> \u2192 your ADLS).</li> </ul> <p>\u2705 So, in your example: Yes \u2014 the data is still stored in the Databricks managed resource group storage account, because <code>dbfs:/tmp/...</code> points to DBFS root, which is Databricks-managed storage.</p>"},{"location":"databricks/03-Databricks_Managed_External_Tables_Hive/#data-retention","title":"Data Retention?","text":"<ul> <li>In Hive Metastore once a managed table is deleted its whole data is also purged forever.</li> <li>This is not the case with UC, unless we do VACUUM even if we drop the table we still can see data in storage account for 7 days by default.</li> </ul>"},{"location":"databricks/03-UC_Managed_External_Tables/","title":"03 UC Managed External Tables","text":""},{"location":"databricks/03-UC_Managed_External_Tables/#managed-vs-external-tables-in-uc","title":"Managed vs External Tables in UC","text":"<p>Managed tables are fully managed by Unity Catalog, which means that Unity Catalog manages both the governance and the underlying data files for each managed table. Managed tables are stored in a Unity Catalog-managed location in your cloud storage. Managed tables always use the Delta Lake format. You can store managed tables at the metastore, catalog, or schema levels.</p> <p>External tables are tables whose access from Databricks is managed by Unity Catalog, but whose data lifecycle and file layout are managed using your cloud provider and other data platforms. Typically you use external tables to register large amounts of your existing data in Databricks, or if you also require write access to the data using tools outside of Databricks. External tables are supported in multiple data formats. Once an external table is registered in a Unity Catalog metastore, you can manage and audit Databricks access to it---and work with it---just like you can with managed tables.</p> <p>Managed volumes are fully managed by Unity Catalog, which means that Unity Catalog manages access to the volume's storage location in your cloud provider account. When you create a managed volume, it is automatically stored in the managed storage location assigned to the containing schema.</p> <p>External volumes represent existing data in storage locations that are managed outside of Databricks, but registered in Unity Catalog to control and audit access from within Databricks. When you create an external volume in Databricks, you specify its location, which must be on a path that is defined in a Unity Catalog external location.</p>"},{"location":"databricks/03-UC_Managed_External_Tables/#cloud-storage-and-data-isolation","title":"Cloud Storage and Data Isolation","text":"<p>Managed storage: default locations for managed tables and managed volumes (unstructured, non-tabular data) that you create in Databricks. These managed storage locations can be defined at the metastore, catalog, or schema level. You create managed storage locations in your cloud provider, but their lifecycle is fully managed by Unity Catalog.</p> <p>Storage locations where external tables and volumes are stored. These are tables and volumes whose access from Databricks is managed by Unity Catalog, but whose data lifecycle and file layout are managed using your cloud provider and other data platforms. Typically you use external tables or volumes to register large amounts of your existing data in Databricks, or if you also require write access to the data using tools outside of Databricks.</p>"},{"location":"databricks/04-UC_External_Location_Storage_Credentials/","title":"Uc External Location Storage Credentials","text":""},{"location":"databricks/04-UC_External_Location_Storage_Credentials/#managed-table-locations-in-unity-catalog","title":"Managed Table Locations in Unity Catalog","text":"<ol> <li>External Metastore Location is provided.</li> </ol> <p>If the metastore storage account is provided by us, then all the catalog, schema and tables is created under this location. At catalog level the location doesnt matter its optional.</p> <ol> <li>If Metastore location is not provided.</li> </ol> <p>In this case we need to provide the location at catalog level to create tables. This is mandatory.</p> <p></p>"},{"location":"databricks/04-UC_External_Location_Storage_Credentials/#creating-a-container-for-our-catalog","title":"Creating a container for our catalog","text":"<p>All data for the managed tables will be stored under this external location if we dont specify it at the schema level.</p> <p></p>"},{"location":"databricks/04-UC_External_Location_Storage_Credentials/#how-things-get-affected-when-providing-location-at-different-levels-in-uc","title":"How things get affected when providing location at different levels in UC?","text":"<p>1\ufe0f\u20e3 External location at Metastore level</p> <p>When you create a Metastore, you must give it a storage root. Example:</p> <pre><code>CREATE METASTORE my_metastore\n  LOCATION 'abfss://uc-metastore@&lt;storageaccount&gt;.dfs.core.windows.net/'\n</code></pre> <p>This location is the default root for managed tables if no other path is specified.</p> <p>Any catalog/schema/table created as managed without its own external location will fall back here.</p> <p>\ud83d\udc49 So think of it like a global default storage for all managed tables across catalogs.</p> <p>2\ufe0f\u20e3 External location at Catalog level</p> <p>When you create a catalog, you can optionally give it its own external location:</p> <pre><code>CREATE CATALOG raw_data\n  USING MANAGED LOCATION my_external_location;\n</code></pre> <p>Now, managed tables inside this catalog will go under this catalog-specific external location (instead of the metastore root).</p> <p>This is useful for separating zones (raw, curated, gold) into their own ADLS containers/folders.</p> <p>\ud83d\udc49 So catalogs can \u201coverride\u201d the metastore root location.</p> <p>3\ufe0f\u20e3 External Tables vs Managed Tables</p> <p>Managed Table: Unity Catalog controls the data lifecycle. </p> <p>Data Deletion Timeline: While the data deletion is initiated immediately, the actual purging of the data files from cloud storage may not be instantaneous. Databricks documentation indicates that the files are typically deleted within a retention period, often around 7 days, though this can sometimes be influenced by Delta Lake's default log retention (e.g., 30 days). Running a VACUUM command can also accelerate the deletion of unreferenced data files.</p> <p>Data lives in either: The Metastore storage root, if no catalog location is set. The Catalog\u2019s external location, if provided.</p> <p>External Table: You explicitly provide a LOCATION when creating the table.</p> <p>Example</p> <pre><code>-- Metastore root\nCREATE METASTORE main_metastore\n  LOCATION 'abfss://uc-metastore@stacc1.dfs.core.windows.net/';\n\n-- Catalog with its own location\nCREATE CATALOG raw_data\n  USING MANAGED LOCATION raw_external;\n\n-- Managed table in raw_data (goes to raw_external)\nUSE CATALOG raw_data;\nCREATE TABLE users_managed (id INT, name STRING) USING DELTA;\n\n-- External table (explicit path overrides everything)\nCREATE TABLE users_external (id INT, name STRING)\nUSING DELTA\nLOCATION 'abfss://special@stacc2.dfs.core.windows.net/custom_path/';\n</code></pre> <p></p>"},{"location":"databricks/04-UC_External_Location_Storage_Credentials/#creating-external-location-for-catalog","title":"Creating External Location For Catalog","text":"<ol> <li>Creating the Storage Credential</li> </ol> <p>This is basically using previous UC Connector to connect to our new storage container for the catalog data.</p> <p></p> <ol> <li>Create the external location</li> </ol> <pre><code>CREATE EXTERNAL LOCATION ext_catalog_dev\nURL 'abfss://data@adbvedanthnew.dfs.core.windows.net/data/catalog'\nWITH (STORAGE CREDENTIAL `uc-data-storage`);\n</code></pre> <pre><code>CREATE CATALOG dev_ext MANAGED LOCATION 'abfss://data@adbvedanthnew.dfs.core.windows.net/data/catalog' COMMENT 'This is external storage catalog'\n</code></pre>"},{"location":"databricks/04-UC_External_Location_Storage_Credentials/#visual-architecture-of-external-location-storage-credential-and-volume-access","title":"Visual Architecture of External Location, Storage Credential and Volume Access","text":"<p>Both managed storage locations and storage locations where external tables and volumes are stored use external location securable objects to manage access from Databricks. External location objects reference a cloud storage path and the storage credential required to access it. Storage credentials are themselves Unity Catalog securable objects that register the credentials required to access a particular storage path. Together, these securables ensure that access to storage is controlled and tracked by Unity Catalog.</p> <p></p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/","title":"Databricks Managed Location Catalog Schema Level","text":""},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#unity-catalog-setup-from-scratch-azure-databricks","title":"\ud83d\ude80 Unity Catalog Setup from Scratch (Azure + Databricks)","text":"<p>Using UC Connector as Managed Identity</p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#part-1-azure-setup","title":"\ud83d\udd39 Part 1: Azure Setup","text":"<p>1. Create a Storage Account</p> <p>In Azure Portal, search Storage Accounts \u2192 Create.</p> <p>Important settings:</p> <p>Performance: Standard Redundancy: LRS (for testing) Enable Hierarchical Namespace (HNS) \u2705 (must be ON for Unity Catalog). Create a container inside it (e.g. uc-data).</p> <p>2. Assign Permissions to UC Managed Identity</p> <p>In Azure Portal \u2192 Storage Account \u2192 Access Control (IAM) \u2192 Add Role Assignment. Assign these roles to the UC Managed Identity (the connector): - Storage Blob Data Owner (read/write access). - Storage Blob Delegator (needed for ABFS driver).</p> <p>Scope: Storage Account level (recommended).</p> <p>\ud83d\udc49 Tip: You can find the UC managed identity name in Databricks \u2192 Admin Console \u2192 Identity Federation.</p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#part-2-databricks-setup-ui-sql","title":"\ud83d\udd39 Part 2: Databricks Setup (UI + SQL)","text":"<p>3. Verify Metastore</p> <p>Go to Databricks Admin Console \u2192 Unity Catalog \u2192 Metastores.</p> <p>If you don\u2019t have one, click Create Metastore. Region = same as Storage Account. Assign this metastore to your workspace.</p> <p>\ud83d\udc49 Do not skip this. Unity Catalog won\u2019t work without a metastore.</p> <p>4. Create Storage Credential (UI or SQL)</p> <p>This ties the Azure Managed Identity to Unity Catalog.</p> <p>UI: Go to Catalog \u2192 Storage Credentials \u2192 Create. Select Azure Managed Identity. Enter a name (e.g., uc-cred). Paste the UC Connector Managed Identity ID.</p> <pre><code>CREATE STORAGE CREDENTIAL uc_cred\nWITH AZURE_MANAGED_IDENTITY 'your-managed-identity-client-id'\nCOMMENT 'UC Connector credential for ADLS';\n</code></pre> <p>5. Register an External Location</p> <p>This points Unity Catalog to your ADLS container/folder.</p> <p>UI: Go to Catalog \u2192 External Locations \u2192 Create. Name: ext_loc_dev Path: abfss://uc-data@.dfs.core.windows.net/ Storage credential: uc-cred (Storage Credential is at Storage Account level so for one cred is enough) <pre><code>CREATE EXTERNAL LOCATION ext_loc_dev\nURL 'abfss://uc-data@&lt;storageaccount&gt;.dfs.core.windows.net/'\nWITH (STORAGE CREDENTIAL uc_cred)\nCOMMENT 'External location for dev data';\n</code></pre> <p>6. Create Catalog</p> <p>Decide whether it will use:</p> <p>Metastore root storage (if defined), OR Catalog-level managed location (recommended).</p> <p>UI:</p> <p>Go to Catalog Explorer \u2192 Create Catalog. Name: dev Managed Location: <pre><code>abfss://uc-data@&lt;storageaccount&gt;.dfs.core.windows.net/dev\n</code></pre> Storage credential: uc-cred.</p> <p>SQL equivalent:</p> <pre><code>CREATE CATALOG dev\nMANAGED LOCATION 'abfss://uc-data@&lt;storageaccount&gt;.dfs.core.windows.net/dev';\n</code></pre> <p></p> <p>7. Create Schema</p> <p>Schemas can also have their own managed locations (if needed).</p> <pre><code>CREATE SCHEMA dev.bronze\nMANAGED LOCATION 'abfss://uc-data@&lt;storageaccount&gt;.dfs.core.windows.net/dev/bronze';\n</code></pre> <p>9. Grant Permissions</p> <p>For yourself or a group (like account users):</p> <pre><code>-- Catalog usage\nGRANT USAGE ON CATALOG dev TO `account users`;\n\n-- Schema usage\nGRANT USAGE ON SCHEMA dev.bronze TO `account users`;\n\n-- External location permissions\nGRANT READ FILES, WRITE FILES ON EXTERNAL LOCATION ext_loc_dev TO `account users`;\n\n-- Table level\nGRANT SELECT, MODIFY ON TABLE dev.bronze.trades TO `account users`;\n</code></pre> <p>10. Create Tables at Different Levels</p> <pre><code>CREATE SCHEMA dev.bronze\nCOMMENT 'This is schema in dev catalog without external location'\n</code></pre> <p>This gets created in metastore level container because its managed and we havent specified external location at catalog level.</p> <pre><code>-- CREATE A TABLE UNDER ALL THREE SCHEMA\nCREATE TABLE IF NOT EXISTS dev.bronze.raw_sales (\n  id INT,\n  name STRING,\n  invoice_no INT,\n  price double\n);\n\nINSERT INTO dev_ext.bronze.raw_sales VALUES (1,'Cookies',1,200.50);\n</code></pre> <p>This gets created in catalog level container because the catalog associated to the schema is external.</p> <pre><code>-- CREATE A TABLE UNDER ALL THREE SCHEMA\nCREATE TABLE IF NOT EXISTS dev_ext.bronze.raw_sales (\n  id INT,\n  name STRING,\n  invoice_no INT,\n  price double\n);\n\nINSERT INTO dev_ext.bronze.raw_sales VALUES (1,'Cookies',1,200.50);\n</code></pre> <p>Creating schema in external location.</p> <pre><code>CREATE EXTERNAL LOCATION 'ext_schema'\nMANAGED LOCATION 'https://adbvedanthnew.databricks.net/adb/schema/bronze_ext'\n</code></pre> <p>This gets created in schema level since we specified external location at schema level.</p> <pre><code>-- CREATE A TABLE UNDER ALL THREE SCHEMA\nCREATE TABLE IF NOT EXISTS dev_ext.bronze_ext.raw_sales (\n  id INT,\n  name STRING,\n  invoice_no INT,\n  price double\n);\n\nINSERT INTO dev_ext.bronze_Ext.raw_sales VALUES (1,'Cookies',1,200.50);\n</code></pre> <p></p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#where-data-is-stored","title":"Where data is stored?","text":"<ol> <li>Stored in metastore root location</li> </ol> <ol> <li>Store in Catalog Level Ext Location</li> </ol> <pre><code>DESC EXTENDED DEV_EXT.BRONZE.RAW_SALE;\n</code></pre> <ol> <li>Store in Schema Level External Location</li> </ol> <pre><code>DESC EXTENDED DEV_EXT.BRONZE_EXT.RAW_SALE\n</code></pre>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#summary","title":"Summary","text":""},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#1-managed-table-no-location-specified","title":"\ud83d\udd39 1. Managed Table (no LOCATION specified)","text":"<pre><code>CREATE SCHEMA finance\nMANAGED LOCATION 'abfss://finance@companydatalake.dfs.core.windows.net/schemas/finance';\n\nCREATE TABLE finance.transactions (\n  id INT,\n  amount DECIMAL(10,2)\n);\n</code></pre> <ul> <li> <p>Since you didn\u2019t give a <code>LOCATION</code> for the table:</p> </li> <li> <p>Unity Catalog treats this as a managed table.</p> </li> <li>UC stores the data under the schema\u2019s managed location.</li> </ul> <p>\u2705 Path = <code>abfss://finance@companydatalake.dfs.core.windows.net/schemas/finance/transactions/</code></p> <p>So in your example, you are exactly right.</p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#2-external-table-explicit-location-specified","title":"\ud83d\udd39 2. External Table (explicit LOCATION specified)","text":"<pre><code>CREATE TABLE finance.transactions_ext (\n  id INT,\n  amount DECIMAL(10,2)\n)\nLOCATION 'abfss://raw@companydatalake.dfs.core.windows.net/landing/transactions/';\n</code></pre> <ul> <li>Here you told UC exactly where the data lives.</li> <li>This is an external table.</li> <li>UC does not move or manage the files \u2014 it just registers metadata pointing at that path.</li> </ul> <p>\u2705 Path = <code>abfss://raw@companydatalake.dfs.core.windows.net/landing/transactions/</code></p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#what-if-schema-has-no-managed-location","title":"\ud83d\udd39 What If Schema Has No Managed Location?","text":"<p>If you do:</p> <pre><code>CREATE SCHEMA finance;\n</code></pre> <ul> <li>No schema location defined.</li> <li>A managed table like:</li> </ul> <pre><code>CREATE TABLE finance.transactions (id INT, amount DECIMAL(10,2));\n</code></pre> <p>will fall back to the metastore\u2019s root storage location (the one you defined when setting up Unity Catalog).</p> <p>\u2705 Path example: <code>abfss://uc-metastore@companydatalake.dfs.core.windows.net/finance/transactions/</code></p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#summary-table","title":"\ud83d\udd39 Summary Table","text":"Case Table Type Storage Path Schema has <code>MANAGED LOCATION</code>, table has no <code>LOCATION</code> Managed table Inside schema\u2019s managed location (e.g., <code>/schemas/finance/transactions/</code>) Schema has <code>MANAGED LOCATION</code>, table has explicit <code>LOCATION</code> External table Exact path you provided Schema has no location, table has no <code>LOCATION</code> Managed table Falls back to metastore root storage <p>\u2705 So in your example (<code>schema has managed location</code> + <code>table no LOCATION</code>) \u2192 yes, data files are stored under schema\u2019s storage location.</p> <p></p>"},{"location":"databricks/05-Databricks_Managed_Location_Catalog_Schema_Level/#real-case-study-employee-data-sensitivity","title":"Real Case Study : Employee Data Sensitivity","text":"<p>For example, let's say your organization has a company compliance policy that requires production data relating to human resources to reside in the bucket s3://mycompany-hr-prod. In Unity Catalog, you can achieve this requirement by setting a location on a catalog level, creating a catalog called, for example hr_prod, and assigning the location s3://mycompany-hr-prod/unity-catalog to it. This means that managed tables or volumes created in the hr_prod catalog (for example, using CREATE TABLE hr_prod.default.table \u2026) store their data in s3://mycompany-hr-prod/unity-catalog. Optionally, you can choose to provide schema-level locations to organize data within the hr_prod catalog at a more granular level.</p> <p>If storage isolation is not required for some catalogs, you can optionally set a storage location at the metastore level. This location serves as a default location for managed tables and volumes in catalogs and schemas that don't have assigned storage. Typically, however, Databricks recommends that you assign separate managed storage locations for each catalog.</p>"},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/","title":"Ctas Deep Clone Shallow Clone Databricks","text":""},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/#ctas-deep-clone-shallow-clone","title":"CTAS | Deep Clone | Shallow Clone","text":""},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/#create-table-as-select-ctas","title":"Create Table As Select (CTAS)","text":"<pre><code>CREATE TABLE dev.bronze.sales_ctas \nAS \nSELECT * FROM dev.bronze.sales_managed\n</code></pre> <p>A new physical location is created and data is now stored in that for the new table.</p> <pre><code>-- location is different from original table, the physical location is changed.\ndesc extended dev.bronze.sales_ctas\n</code></pre>"},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/#deep-clone","title":"Deep Clone","text":"<pre><code>CREATE TABLE dev.bronze.sales_deep_clone DEEP CLONE dev.bronze.sales_managed;\nselect * from dev.bronze.sales_deep_clone;\ndesc extended dev.bronze.sales_deep_clone;\n</code></pre> <p>This copies both table data and metadata.</p> <p>The data is created in new locaiton.</p> <p></p> <p>Both delta logs and data copied over</p> <p></p> <p>We can see latest version of source from where data from table is copied.</p> <p></p>"},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/#shallow-clone","title":"Shallow Clone","text":"<p>The table is created at a new location</p> <p></p> <p>Only Delta Log copied over the physical data stored at original location.</p> <p></p>"},{"location":"databricks/06-CTAS_Deep_Clone_Shallow_Clone_Databricks/#inserting-data-into-original-table-and-see-if-it-reflects-in-shallow-clone","title":"Inserting Data into original table and see if it reflects in Shallow Clone","text":"<pre><code>select * from dev.bronze.sales_shallow_clone\n</code></pre> <p>Insert data into main table</p> <p></p> <p>Requery shallow clone</p> <p></p> <p>We can see only one row the 2nd row inserted is not visible.</p> <p>Shallow Clone points to the source version of original table when created and does not update the data.</p> <p>When we do the vice versa and insert new record in shallow table, it does not impact the original table. The new data is stored in the shallow table's own location.</p> <p>Only when we VACUUM the main table, all records from shallow table also gets deleted.</p>"},{"location":"databricks/07-RBAC_Custom_Roles_ServicePrincipals/","title":"Rbac Custom Roles Serviceprincipals","text":""},{"location":"databricks/07-RBAC_Custom_Roles_ServicePrincipals/#role-based-access-control-custom-role-definitions-service-principals","title":"Role Based Access Control | Custom Role Definitions | Service Principals","text":"<p>1\ufe0f\u20e3 Azure RBAC Roles (Role-Based Access Control)</p> <p>Purpose: RBAC in Azure controls who can do what on which resource.</p> <p>Key Concepts:</p> <p>Scope: Defines where the role applies (Subscription \u2192 Resource Group \u2192 Resource).</p> <p>Role: Defines what actions can be performed.</p> <p>Principal: The identity (user, group, or application) assigned the role.</p> <p></p> <p>2\ufe0f\u20e3 Custom Roles</p> <p>Purpose: When built-in roles are too broad or restrictive, you can create custom roles with exactly the permissions you need.</p> <p>How it works:</p> <ul> <li> <p>Define a JSON file with allowed actions (Microsoft.Storage/storageAccounts/blobServices/containers/read, etc.)</p> </li> <li> <p>Assign it to users/groups/service principals.</p> </li> </ul> <p>Example JSON for a custom role (ADLS read-only access):</p> <p><pre><code>{\n  \"Name\": \"ADLS ReadOnly\",\n  \"IsCustom\": true,\n  \"Description\": \"Read-only access to ADLS Gen2 containers\",\n  \"Actions\": [\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/read\",\n    \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\"\n  ],\n  \"NotActions\": [],\n  \"AssignableScopes\": [\"/subscriptions/&lt;subscription-id&gt;\"]\n}\n</code></pre> When to use:</p> <p>You want a minimal-privilege principle, e.g., a BI service can only read blobs, not delete them.</p> <p>3\ufe0f\u20e3 Service Principals</p> <p>Purpose: A service principal is like a \u201cuser identity\u201d for applications, scripts, or automated services.</p> <p>Why needed:</p> <ul> <li>Azure RBAC requires an identity for access.</li> <li>You don\u2019t want to use your personal account for automated tasks.</li> </ul> <p>Example: Databricks accessing ADLS via a service principal.</p> <p>Types of Service Principals Authentication:</p> <ul> <li>Client Secret: Password-like string.</li> <li>Certificate: Secure certificate authentication.</li> <li>Managed Identity (Recommended for Databricks UC Connector): Azure handles the credentials for you.</li> </ul> <p>How it works in practice (Databricks + ADLS)</p> <ul> <li>Create a service principal in Azure AD.</li> <li>Assign RBAC (e.g., Storage Blob Data Contributor) on the storage account/container.</li> <li>Use this SP to create a Databricks Storage Credential (or UC connector).</li> <li>Unity Catalog or your clusters use the SP to access storage without exposing your personal account.</li> </ul>"},{"location":"databricks/07-RBAC_Custom_Roles_ServicePrincipals/#azure-ad-interview-questions-for-data-engineers","title":"\ud83d\udd39 Azure AD Interview Questions (for Data Engineers)","text":"<p>Q1. What is Azure Active Directory (Azure AD)?</p> <p>Answer: Azure AD is Microsoft\u2019s cloud-based identity and access management service. It authenticates users, applications, and services, and authorizes them to access Azure resources. Unlike on-prem AD, Azure AD is designed for cloud-first apps, RBAC, and SSO.</p> <p>Q2. What is the difference between Azure AD Users, Groups, Service Principals, and Managed Identities?</p> <p>Answer:</p> <p>Users \u2192 Human identities (employees, admins).</p> <p>Groups \u2192 Collection of users/SPs for easier role assignment.</p> <p>Service Principal (SP) \u2192 Non-human identity for applications to access resources.</p> <p>Managed Identity \u2192 A special type of SP managed automatically by Azure, used by Azure services (like Databricks, ADF) to access resources without credentials.</p> <p>Q3. What\u2019s the difference between Service Principal and Managed Identity?</p> <p>Answer:</p> <p>Service Principal \u2192 You create it manually, assign roles, and manage secrets/certs.</p> <p>Managed Identity \u2192 Azure creates/rotates credentials automatically, no secrets to manage. Example: Databricks UC Connector \u2192 Managed Identity (no secrets). A legacy pipeline using Python SDK \u2192 Service Principal (with client secret).</p> <p>Q4. How does RBAC work in Azure?</p> <p>Answer:</p> <p>RBAC (Role-Based Access Control) grants permissions at scope levels: Subscription \u2192 Resource Group \u2192 Resource.</p> <p>Roles are assigned to principals (user, group, SP, or MI).</p> <p>Built-in roles include: Owner, Contributor, Reader, Storage Blob Data Reader/Contributor. Example: Assign Storage Blob Data Contributor to a Databricks SP so it can read/write to ADLS.</p> <p>Q5. How is Azure AD different from On-prem Active Directory?</p> <p>Answer:</p> <p>AD (On-prem) \u2192 Kerberos/NTLM, domain-joined machines, Windows environments.</p> <p>Azure AD \u2192 OAuth2, SAML, OpenID Connect, cloud-first, SSO, SaaS app integration.</p> <p>Azure AD cannot join servers to a domain but can integrate with ADDS (hybrid).</p> <p>Q6. What is Conditional Access in Azure AD?</p> <p>Answer: It enforces policies like MFA, location restrictions, or device compliance before granting access. Example: Require MFA for accessing Databricks workspace from outside corporate network.</p> <p>Q7. What is a Custom Role in Azure AD?</p> <p>Answer:</p> <p>Built-in roles may not cover all needs.</p> <p>Custom roles let you define granular actions (e.g., \u201cread blobs, but not delete\u201d). Example: A custom role for analysts \u2192 can read raw/curated ADLS folders but not write/delete.</p> <p>Q8. What are the authentication protocols supported by Azure AD?</p> <p>Answer:</p> <p>OAuth 2.0 \u2192 App-to-app access (SPs, APIs)</p> <p>OpenID Connect (OIDC) \u2192 User authentication + SSO</p> <p>SAML 2.0 \u2192 Enterprise SSO with third-party apps</p> <p>SCIM \u2192 User/group provisioning</p> <p>Example: Databricks notebooks \u2192 ADLS (OAuth 2.0 via SP/MI).</p> <p>Q9. Explain a real-world flow of Databricks accessing ADLS with Unity Catalog and Azure AD.</p> <p>Answer:</p> <p>User runs a query in Databricks notebook.</p> <p>Unity Catalog enforces permissions (does user have SELECT?).</p> <p>Databricks uses storage credential (SP or Managed Identity) registered in UC.</p> <p>Azure AD authenticates the SP/MI.</p> <p>ADLS authorizes via RBAC role (Storage Blob Data Contributor).</p> <p>Data is read/written securely, no secrets exposed.</p> <p>Q10. What is the difference between Directory Roles vs Azure RBAC Roles?</p> <p>Answer:</p> <p>Directory Roles \u2192 Control Azure AD objects (users, groups, SPs). Example: Global Admin, User Administrator.</p> <p>RBAC Roles \u2192 Control access to Azure resources (storage, VMs, databases). Example: Storage Blob Data Contributor.</p> <p>Q11. How would you give different levels of access to Finance vs Data Science teams on the same ADLS account?</p> <p>Answer:</p> <p>Create two groups in Azure AD \u2192 finance-users, ds-users.</p> <p>Assign RBAC roles:</p> <p>Finance \u2192 Read access only (custom role or Blob Data Reader).</p> <p>DS \u2192 Read/Write on curated container (Blob Data Contributor).</p> <p>In Unity Catalog, assign table permissions \u2192 Finance: SELECT, DS: SELECT/INSERT/UPDATE.</p> <p>Q12. What are some common security best practices with Azure AD in Data Engineering?</p> <p>Answer:</p> <p>Use Managed Identities instead of secrets.</p> <p>Use Groups for access, not direct user assignments.</p> <p>Use Conditional Access (MFA, network restrictions).</p> <p>Follow least privilege principle with custom roles.</p> <p>Enable logging (Azure AD logs, storage logs, Databricks audit logs) for compliance.</p>"},{"location":"databricks/07-RBAC_Custom_Roles_ServicePrincipals/#openid-connect-oidc","title":"OpenID Connect (OIDC)","text":"<p>OpenID Connect is an identity layer built on top of OAuth 2.0.</p> <p>OAuth 2.0 \u2192 Handles authorization (what an app can do on your behalf).</p> <p>OIDC \u2192 Adds authentication (who you are, your identity).</p> <p>How it works (simple flow):</p> <p>A user tries to log in to an app (e.g., Databricks). The app redirects them to Azure AD (the identity provider) using OIDC. Azure AD authenticates the user (password, MFA, etc.).</p> <p>Azure AD returns tokens:</p> <p>ID Token (JWT) \u2192 contains identity info (username, email, groups). Access Token \u2192 lets the app call APIs on user\u2019s behalf. The app trusts the ID token and logs the user in.</p> <p>Example in Azure AD + Databricks</p> <p>You open Databricks workspace in browser. Databricks uses OIDC with Azure AD to authenticate you. Azure AD issues an ID token with your email + groups. Databricks checks your group \u2192 grants access based on Unity Catalog permissions.</p> <p>Why OIDC is important?</p> <p>It enables SSO (Single Sign-On) across cloud apps. Works with MFA and Conditional Access. Uses JWT tokens that are stateless and easy to validate.</p> <p>\u2705 Summary for interview: OpenID Connect is an authentication protocol built on OAuth 2.0. It issues ID tokens (JWTs) that allow apps to verify a user\u2019s identity and support SSO. In Azure AD, OIDC is used when logging into cloud apps like Databricks, Power BI, or ADF.</p> <p>Imagine this:</p> <p>You want to enter a party \ud83c\udf89. The party organizer is the app (like Databricks). At the door, they don\u2019t know you\u2026 so they send you to the government office (Azure AD).</p> <p>What happens:</p> <p>You go to the government office (Azure AD). You show your ID card, fingerprint, maybe even OTP \u2192 they confirm you are really you \u2705. They give you a badge (the ID token). You take that badge back to the party \ud83c\udf89.</p> <p>The party organizer looks at the badge \u2192 \u201cOkay, you are Vedanth, you\u2019re allowed in.\u201d</p> <p>If you want to get food \ud83c\udf55 or drinks \ud83e\udd64 inside, the badge can also have permissions (access token) telling the staff what you\u2019re allowed to do.</p> <p>Difference:</p> <ul> <li>OAuth 2.0 \u2192 Badge only says what you can do inside the party.</li> <li>OIDC \u2192 Badge also says who you are.</li> </ul>"},{"location":"databricks/08-Deletion_Vectors_Delta_lake/","title":"Deletion Vectors Delta Lake","text":""},{"location":"databricks/08-Deletion_Vectors_Delta_lake/#deletion-vectors-in-delta-lake","title":"Deletion Vectors in Delta Lake","text":""},{"location":"databricks/08-Deletion_Vectors_Delta_lake/#what-are-deletion-vectors","title":"\ud83d\udd39 What are Deletion Vectors?","text":"<p>Normally, when you delete rows in a Delta table, Delta rewrites entire Parquet files without those rows.</p> <p>This is called copy-on-write \u2192 expensive for big tables.</p> <p>Deletion Vectors (DVs) are a new optimization:</p> <p>Instead of rewriting files, Delta just marks the deleted rows with a bitmap (a lightweight \u201cmask\u201d). The data is still physically there, but readers skip the \u201cdeleted\u201d rows.</p> <p>Think of it like putting a red X mark \u274c on rows instead of erasing them immediately.</p>"},{"location":"databricks/08-Deletion_Vectors_Delta_lake/#why-are-they-useful","title":"\ud83d\udd39 Why are they useful?","text":"<p>\ud83d\ude80 Much faster deletes/updates/merges (because files aren\u2019t rewritten).</p> <p>\u26a1 Less I/O \u2192 good for big data tables.</p> <p>\u2705 Efficient for streaming + time travel.</p>"},{"location":"databricks/08-Deletion_Vectors_Delta_lake/#example-without-deletion-vectors","title":"Example Without deletion vectors","text":"<ol> <li>Create a sales table</li> </ol> <pre><code>CREATE TABLE dev.bronze.sales as \nselect * from \nread_files(\n  'dbfs:/databricks-datasets/online_retail/data-001/data.csv',\n  header =&gt; true,\n  format =&gt; 'csv'\n)\n</code></pre> <ol> <li>Set Deletion Vectors false</li> </ol> <pre><code>ALTER TABLE dev.bronze.sales SET TBLPROPERTIES (delta.enableDeletionVectors = false);\n</code></pre> <ol> <li>Delete some rows</li> </ol> <pre><code>-- delete InvoiceNo = '540644'\ndelete from dev.bronze.sales\nwhere InvoiceNo = '540644'\n</code></pre> <ol> <li>Describe history</li> </ol> <p>Observe that all rows (65000+) are removed and rewritten.</p>"},{"location":"databricks/08-Deletion_Vectors_Delta_lake/#example-with-deletion-vectors","title":"Example with deletion vectors","text":"<p>We can see that one deletion vector is added no files are rewritten.</p> <p>Running optimize would remove those files / records.</p>"},{"location":"databricks/09-Liquid_Clustering_Delta_Lake/","title":"Liquid Clustering Delta Lake","text":""},{"location":"databricks/09-Liquid_Clustering_Delta_Lake/#liquid-clustering-in-delta-lake-and-databricks","title":"Liquid Clustering in Delta Lake and Databricks","text":""},{"location":"databricks/09-Liquid_Clustering_Delta_Lake/#traditional-partitioning-the-old-way","title":"\ud83d\udd39 Traditional Partitioning (the old way)","text":"<p>When you create a Delta table, you pick a partition column (e.g., date).</p> <p>Data is physically stored in folders like:</p> <pre><code>/table/date=2025-08-25/\n/table/date=2025-08-26/\n</code></pre> <p>Queries on date are very fast (partition pruning).</p> <p>But\u2026 problems:</p> <p>You must choose the partition column upfront (hard to change later). Skew \u2192 some partitions get huge, others tiny. If you query on a different column (say country), partitioning doesn\u2019t help.</p>"},{"location":"databricks/09-Liquid_Clustering_Delta_Lake/#what-is-liquid-clustering","title":"\ud83d\udd39 What is Liquid Clustering?","text":"<p>Liquid Clustering is next-gen partitioning without rigid partitions.</p> <p>Instead of fixed folder partitions, Delta uses clustering columns.</p> <p>Data is automatically organized into files that are co-located based on clustering keys.</p> <p>No fixed directories \u2014 clustering boundaries are \u201cliquid,\u201d meaning they can shift over time.</p> <p>Think of it like:</p> <p>Partitioning = chopping the cake into fixed slices \ud83c\udf70. Liquid Clustering = marbling the cake so flavors are naturally grouped but flexible \ud83c\udf00.</p> <p></p> <p>\ud83e\udde9 The root problem: concurrent writes</p> <p>In a traditional partitioned Delta table:</p> <p>If two jobs write to the same partition folder (say date=2025-08-25), they may overwrite each other\u2019s files, create tons of small files, or cause conflicts.</p> <p>Delta\u2019s transaction log ( _delta_log ) prevents corruption, but still you can get:</p> <ul> <li>Write conflicts</li> <li>Compaction/reorg problems</li> <li>Skewed partitions</li> </ul> <p>\ud83c\udf00 What Liquid Clustering does differently</p> <p>Liquid Clustering removes the dependency on static partition folders.</p> <p>There is no date=2025-08-25/ folder.</p> <p>Instead, data is stored in files spread across the table storage, tagged internally with clustering metadata.</p> <p>When multiple jobs write:</p> <ul> <li>The Delta transaction log coordinates atomic commits.</li> <li>Writers don\u2019t fight for the same fixed folder (no \"hotspot\").</li> <li>Databricks automatically distributes new rows into the right clustering ranges.</li> </ul> <p>\u26a1 How concurrent writes are prevented</p> <p>Transaction log serialization</p> <p>Every write creates a new JSON transaction in _delta_log. If two jobs conflict, Delta retries or errors out gracefully \u2014 no corruption.</p> <p>No rigid partitions</p> <p>Since clustering is \"liquid\", two writers can both insert data with the same date or country values. Delta decides file placement dynamically (not tied to a single folder).</p> <p>Background clustering</p> <p>Databricks runs auto-optimization jobs to maintain clustering quality. Even if concurrent writes scatter data, the optimizer later reorganizes files.</p> <p>Reduced small files problem</p> <p>With partitions, concurrent writers often create many tiny files in the same folder. With Liquid Clustering, writers spread load across cluster ranges \u2192 fewer hotspots.</p> <p>\ud83c\udf55 Pizza Shop Analogy</p> <p>Imagine you and your friends are delivering pizzas to an office building.</p> <p>Old Way (Partitions)</p> <p>The building has one mailbox per floor. If two delivery guys (writers) come to the same floor mailbox at the same time, they fight for space. The mailbox gets messy, pizzas overlap, and sometimes one delivery overwrites the other. This is like partitioned Delta tables \u2192 if two jobs write to the same partition folder, conflicts happen.</p> <p>New Way (Liquid Clustering)</p> <p>Now the building switches to smart lockers (clustering ranges).</p> <p>When a delivery comes in, the system automatically assigns any free locker on that floor.</p> <p>Two delivery guys can deliver pizzas for the same floor at the same time, but the system spreads them across different lockers.</p> <p>Later, the building staff reorganizes lockers (background clustering) so pizzas for the same person are grouped together neatly.</p> <p>This is like Liquid Clustering \u2192 no fixed folders, data is dynamically placed, and reorganized in the background.</p> <p>The Delta log is like the building\u2019s register that records every pizza delivered \u2192 so no one loses track.</p>"},{"location":"databricks/09-Liquid_Clustering_Delta_Lake/#are-there-trade-offs","title":"Are there Trade Offs?","text":"<p>With partitions (mailboxes), if you know the floor (partition key), you go directly to that mailbox \u2014 super fast \ud83d\ude80 for point lookups.</p> <p>With liquid clustering (smart lockers), pizzas for the same floor (or customer) might be spread across multiple lockers. To find all pizzas for \u201cfloor 5,\u201d you may have to open  several lockers instead of one \u2192 sounds slower, right?</p> <p>Why it\u2019s not actually that slow in practice:</p> <p>Clustering index in metadata</p> <p>Delta keeps track of where rows are stored (think: a digital map of which lockers hold floor 5 pizzas). Readers don\u2019t randomly scan every file; they check the index and skip irrelevant files.</p> <p>File skipping + statistics</p> <p>Each data file stores min/max values of the clustering column. So if you query \u201ccustomer_id = 123,\u201d Delta can skip 90% of files if their min/max range doesn\u2019t cover 123.</p> <p>Background reclustering</p> <p>Liquid clustering reorganizes lockers in the background, so \u201csimilar pizzas\u201d get grouped closer over time. This means queries get faster the more the system reclusters.</p> <p>Trade-off (balanced)</p> <p>Old partitions \u2192 fast for single key lookups, but slow for big aggregations (because partitions may be uneven/skewed). Liquid clustering \u2192 slightly slower for tiny point lookups, but much faster and balanced for mixed workloads (point lookups + large scans).</p>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/","title":"Concurrency Liquid Clustering","text":""},{"location":"databricks/10-Concurrency_Liquid_Clustering/#multi-version-concurrency-control-in-liquid-clustering","title":"Multi Version Concurrency Control in Liquid Clustering","text":"<p>\ud83d\udd0e Why concurrent write failures still happen</p> <p>Delta uses Optimistic Concurrency Control (OCC) for all writes, even with Liquid Clustering:</p> <p>Each writer reads a snapshot of the table (say version 5).</p> <p>Both writers prepare their changes.</p> <p>When committing:</p> <p>Delta checks if the table\u2019s latest version is still 5.</p> <p>If yes \u2192 commit succeeds (new version 6).</p> <p>If another writer already committed version 6 \u2192 your commit fails with a ConcurrentWriteException.</p> <p>This ensures consistency. Without this, two writers could overwrite each other\u2019s updates silently.</p> <p>\ud83d\udca1 Liquid Clustering helps with performance, not concurrency</p> <p>Normally, clustering/partitioning means two writers updating the same partition can easily conflict (both touching the same small set of files). With Liquid Clustering, rows are dynamically redistributed across files, so writers are less likely to clash on the exact same files.</p> <p>But if two jobs still update overlapping rows (or even metadata) \u2192 OCC detects the conflict and one fails.</p> <p>\ud83d\udc49 So: Liquid reduces probability of collisions but does not eliminate them.</p>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#multi-version-concurrency-control","title":"Multi Version Concurrency Control","text":"<p>Two delivery guys arrive at the same time</p> <p>Job A delivers pizzas for floor 5</p> <p>Job B delivers pizzas for floor 5</p> <p>With old mailboxes \u2192 they\u2019d fight for the same mailbox. Chaos \ud83d\ude35</p> <p>With liquid clustering + MVCC \u2192</p> <p>Each delivery guy puts pizzas into their own lockers (new files).</p> <p>No overwrites. No conflicts.</p> <p>The building\u2019s delivery register (Delta Log)</p> <p>Every delivery is recorded in the logbook at reception (Delta transaction log).</p> <p>The log has versions:</p> <p>Version 1: Deliveries from Job A</p> <p>Version 2: Deliveries from Job B</p> <p>So if you \u201creplay the log,\u201d you see all deliveries, in order.</p> <p>Readers never see half-finished deliveries</p> <p>If someone checks the log while Job A is writing, they still only see the previous version (Version 0).</p> <p>Once Job A finishes, the log moves to Version 1.</p> <p>Then readers see all of Job A\u2019s pizzas atomically.</p> <p>\u279d This guarantees snapshot isolation = you only ever see a consistent view.</p> <p>Concurrent jobs don\u2019t lose pizzas</p> <p>Even if Job A and Job B write at the same time, MVCC ensures:</p> <p>Job A\u2019s new files \u2192 recorded in Version 1</p> <p>Job B\u2019s new files \u2192 recorded in Version 2</p> <p>Both sets of deliveries are preserved. \u2705</p> <p>\ud83e\uddfe Real Delta Lake terms:</p> <p>Log = _delta_log JSON + Parquet files (transaction history).</p> <p>New version = commit when a write finishes.</p> <p>Readers always query a stable snapshot version, not files mid-write.</p> <p>Concurrent writers: no overwrite, because each write creates new files, and old files are marked as removed in the log.</p> <p>\ud83d\udca1 Takeaway:</p> <p>MVCC in Delta is like a time machine + logbook \u2014 every write creates a new version of the table, so no data is lost, no half-baked updates are visible, and readers/writers can happily work in parallel.</p> <p>New files are created everytime we write to clustered delta table.</p> <p>Delta Lake never updates files in-place (because they\u2019re immutable in cloud storage).</p> <p>Instead, on every write (insert, update, merge, etc.):</p> <p>Delta writes new Parquet files with the updated data.</p> <p>The Delta log (_delta_log) is updated with JSON/Checkpoint metadata pointing to the new set of files.</p> <p>Old files are marked as removed, but not physically deleted until <code>VACUUM</code></p> <p>\ud83e\udde9 With Liquid Clustering</p> <p>Liquid Clustering\u2019s job is to keep files balanced by row count (not by fixed partition values).</p> <p>When you insert \u2192 Delta writes new files sized according to Liquid\u2019s clustering strategy (e.g., ~1M rows per file).</p> <p>When you update/merge/delete \u2192 Delta rewrites the affected rows into new files, distributed across existing clustering ranges.</p> <p>The old files are marked as deleted in the log.</p> <p>\ud83d\udc49 Every commit adds new files and retires old ones.</p>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#demo","title":"Demo","text":"<p>\ud83d\udd39 Setup</p> <p>We\u2019ll assume:</p> <p>A Delta table with Liquid clustering enabled.</p> <p>Target file size ~1M rows per file (for simplicity).</p>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#step-1-create-empty-table","title":"Step 1 : Create Empty Table","text":"<pre><code>CREATE TABLE sales_liquid (\n  order_id STRING,\n  customer_id STRING,\n  amount DECIMAL(10,2),\n  date DATE\n)\nUSING DELTA\nCLUSTER BY (date);  -- Liquid Clustering on \"date\"\n</code></pre> <p>\ud83d\udcc2 At this point:</p> <p>_delta_log/ has version 000000.json (empty schema).</p> <p>No data files yet.</p>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#step-2-insert-first-batch","title":"Step 2 : Insert First Batch","text":"<pre><code>INSERT INTO sales_liquid SELECT ... 2M rows ...\n</code></pre> <p>\ud83d\udcc2 What happens:</p> <p>Liquid clustering creates ~2 files of ~1M rows each.</p> <p>_delta_log/000001.json records addFile for these.</p> <p>File status:</p> <pre><code>file_0001.parquet (~1M)\nfile_0002.parquet (~1M)\n</code></pre>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#step-3-insert-second-batch","title":"Step 3 : Insert Second Batch","text":"<pre><code>INSERT INTO sales_liquid SELECT ... 0.5M rows ...\n</code></pre> <p>\ud83d\udcc2 What happens:</p> <p>New data \u2192 1 new file of ~500k rows.</p> <p>No rewrite of old files.</p> <p>_delta_log/000002.json adds metadata.</p> <pre><code>file_0001.parquet (~1M)\nfile_0002.parquet (~1M)\nfile_0003.parquet (~0.5M)\n</code></pre>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#step-4-update-800k-rows-spread-over-files-1-and-2","title":"Step 4 : Update 800k rows spread over files 1 and 2","text":"<pre><code>UPDATE sales_liquid SET amount = amount * 1.05 WHERE date BETWEEN '2023-01-01' AND '2023-03-01';\n</code></pre> <p>\ud83d\udcc2 What happens:</p> <p>Delta does not edit file_0001/0002 \u2192 marks them as removed.</p> <p>Writes new replacement files (~800k rows redistributed).</p> <pre><code>file_0001 \u274c removed\nfile_0002 \u274c removed\nfile_0003.parquet (~0.5M)\nfile_0004.parquet (~0.4M)\nfile_0005.parquet (~0.4M)\n</code></pre>"},{"location":"databricks/10-Concurrency_Liquid_Clustering/#step-5-delete-100k-rows","title":"Step 5 : Delete 100k rows","text":"<pre><code>DELETE FROM sales_liquid WHERE customer_id = 'C123';\n</code></pre> <p>\ud83d\udcc2 What happens:</p> <p>Affected rows come from file_0005.</p> <p>File_0005 is removed, replaced with smaller rewritten file.</p> <pre><code>file_0003.parquet (~0.5M)\nfile_0004.parquet (~0.4M)\nfile_0006.parquet (~0.3M)\n</code></pre>"},{"location":"databricks/11-Copy_Into_Databricks/","title":"Copy Into Databricks","text":""},{"location":"databricks/11-Copy_Into_Databricks/#copy-into-in-databricks","title":"<code>COPY INTO</code> in Databricks","text":"<p>COPY INTO feature is used to load files from volumes to Databricks tables and has feature of idempotency ie the data does not get duplicated in the table.</p>"},{"location":"databricks/11-Copy_Into_Databricks/#steps","title":"Steps","text":"<ol> <li>Create Volume</li> </ol> <pre><code>CREATE VOLUME dev.bronze.landing\n</code></pre> <ol> <li>Create folder inside volume</li> </ol> <pre><code>dbutils.fs.mkdirs(\"/Volumes/dev/bronze/landing/input\")\n</code></pre> <ol> <li>Copy sample dataset into volume</li> </ol> <pre><code>dbutils.fs.cp(\"/databricks-datasets/definitive-guide/data/retail-data/by-day/2010-12-01.csv\",\"/Volumes/dev/bronze/landing/input\")\n\ndbutils.fs.cp(\"/databricks-datasets/definitive-guide/data/retail-data/by-day/2010-12-02.csv\",\"/Volumes/dev/bronze/landing/input\")\n</code></pre> <ol> <li>Create bronze table</li> </ol> <pre><code>COPY INTO dev.bronze.invoice_cp\nFROM '/Volumes/dev/bronze/landing/input'\nFILEFORMAT = CSV\nPATTERN = '*.csv'\nFORMAT_OPTIONS ( -- for the files, if they are different format one has 3 cols other has 5 then merge them\n  'mergeSchema' = 'true',\n  'header' = 'true'\n)\nCOPY_OPTIONS ( -- at table level meerge Schema\n  'mergeSchema' = 'true'\n)\n</code></pre> <ol> <li>Select from table</li> </ol> <p>We can see 5217 rows.</p> <p></p> <ol> <li>Run COPY INTO again</li> </ol> <p></p> <p>No affected rows so copy into does not duplicate. Its idempotent.</p>"},{"location":"databricks/11-Copy_Into_Databricks/#how-does-copy-into-maintain-the-log-of-data-files-ingested","title":"How does copy into maintain the log of data files ingested?","text":"<p>The delta log maintains json version tracking that has information and path of files processed.</p> <p></p>"},{"location":"databricks/11-Copy_Into_Databricks/#custom-transformations-while-loading","title":"Custom Transformations while loading","text":"<pre><code>COPY INTO dev.bronze.invoice_cp_alt\nFROM \n(\n  SELECT InvoiceNo,StockCode,cast(Quantity as DOUBLE),current_timestamp() as _insert_date \n  FROM \n  '/Volumes/dev/bronze/landing/input'\n)\n\nFILEFORMAT = CSV\nPATTERN = '*.csv'\nFORMAT_OPTIONS ( -- for the files, if they are different format one has 3 cols other has 5 then merge them\n  'mergeSchema' = 'true',\n  'header' = 'true'\n)\nCOPY_OPTIONS ( -- at table level meerge Schema\n  'mergeSchema' = 'true'\n)\n</code></pre>"},{"location":"databricks/12-AutoLoader_Databricks/","title":"Autoloader Databricks","text":""},{"location":"databricks/12-AutoLoader_Databricks/#auto-loader-concept-in-databricks","title":"Auto Loader Concept in Databricks","text":"<p>Auto Loader is a Databricks feature for incrementally and efficiently ingesting new data files from cloud storage (S3, ADLS, GCS) into Delta Lake tables.</p> <p>It solves the problem of:</p> <p>\u201cHow do I continuously load only the new files that arrive in my data lake, without reprocessing old files every time?\u201d</p> <p>\u2699\ufe0f How it works</p> <p>New files detection</p> <p>Auto Loader uses file notification or directory listing to detect new files in cloud storage.</p> <p>Each file is processed exactly once.</p> <p>Schema handling</p> <p>Auto Loader can infer schemas automatically and evolve them as new fields appear.</p> <p>Supports schema evolution modes like:</p> <p>addNewColumns \u2192 automatically adds new columns.</p> <p>rescue \u2192 unexpected fields are captured in _rescued_data column instead of failing.</p> <p>Incremental state tracking</p> <p>Auto Loader stores state in a schema location checkpoint directory, so it knows which files are already ingested.</p> <p>Streaming or batch</p> <p>Auto Loader works as a Structured Streaming source but can also be triggered in a batch-like mode.</p>"},{"location":"databricks/12-AutoLoader_Databricks/#key-features","title":"\ud83d\udd11 Key Features","text":"<p>Scalable ingestion: Handles billions of files.</p> <p>Efficient: Processes only new/changed files, no need for full scans.</p> <p>Schema evolution: Adapts to changing data over time.</p> <p>Rescue data: Keeps unrecognized/mismatched fields safe for later analysis.</p> <p>Integration: Works seamlessly with Delta Lake, Structured Streaming, and Databricks Workflows.</p>"},{"location":"databricks/12-AutoLoader_Databricks/#modes-of-schema-evolution","title":"\ud83d\udcca Modes of schema evolution","text":"<p>none \u2192 no schema changes allowed.</p> <p>addNewColumns \u2192 automatically add new columns to the table.</p> <p>rescue \u2192 unexpected fields go into _rescued_data.</p> <p>Manual \u2192 you evolve schema explicitly using ALTER TABLE.</p> <p>When we run readStream cell for first time, then run writeStream it fails</p> <p></p> <p>But when it fails, the new column State is added to the schemaLocation folder.</p> <p>The next time we run readStream it reads using the new schema and now after running writeStream it runs.</p> <p></p> <p>In rescue mode we have a new column added called _rescue_data that holds the records that dont match the schema.</p> <p></p> <p>In None mode, we dont see a rescue_column option, </p> <p></p> <p>There is no failure but the new column data does not get added. There is no schema evolution.</p> <p>\ud83d\udd12 Why use Auto Loader instead of plain Structured Streaming?</p> <p>Without Auto Loader: you\u2019d have to rescan directories and manually deduplicate files.</p> <p>With Auto Loader: file discovery and state management are built-in \u2192 scalable &amp; cost-efficient.</p>"},{"location":"databricks/12-AutoLoader_Databricks/#1-what-checkpointing-is-in-autoloader","title":"\ud83d\udd39 1. What checkpointing is in Autoloader","text":"<p>When you use Autoloader (<code>cloudFiles</code>), it\u2019s powered by Structured Streaming.</p> <ul> <li>Spark Structured Streaming needs to remember progress (which files have been processed, offsets, watermark, etc.).</li> <li>That \u201cmemory\u201d is kept in the checkpoint location (usually in cloud storage like <code>dbfs:/checkpoints/...</code>).</li> <li>Without checkpoints, a restart would re-read the same files.</li> </ul>"},{"location":"databricks/12-AutoLoader_Databricks/#2-state-storage-in-structured-streaming","title":"\ud83d\udd39 2. State storage in Structured Streaming","text":"<p>When you do stateful operations (e.g., <code>dropDuplicates</code>, aggregations, joins with watermarks, etc.), Spark must maintain a state store.</p> <ul> <li>By default, Spark stores this in HDFS-compatible storage under your checkpoint directory.</li> <li>However, Spark also supports RocksDB as the backend for state storage. RocksDB is faster and memory-efficient because it keeps state on local disk (per executor) rather than only on JVM heap.</li> </ul>"},{"location":"databricks/12-AutoLoader_Databricks/#3-using-rocksdb-with-autoloader","title":"\ud83d\udd39 3. Using RocksDB with Autoloader","text":"<p>In Databricks, you can enable RocksDB for Autoloader pipelines with stateful ops:</p> <pre><code>(spark.readStream\n  .format(\"cloudFiles\")\n  .option(\"cloudFiles.format\", \"json\")\n  .load(\"/mnt/raw/crypto\")  # Autoloader source\n  .withWatermark(\"event_time\", \"10 minutes\")\n  .groupBy(\"symbol\", window(\"event_time\", \"5 minutes\"))\n  .agg(F.avg(\"price\").alias(\"avg_price\"))\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", \"dbfs:/checkpoints/crypto_autoloader\")\n  .outputMode(\"append\")\n  .option(\"stateStore.rocksdb.enabled\", \"true\")  # enable RocksDB\n  .start(\"/mnt/bronze/crypto\"))\n</code></pre>"},{"location":"databricks/12-AutoLoader_Databricks/#4-how-it-works","title":"\ud83d\udd39 4. How it works","text":"<ul> <li>Checkpoint location: still needed (to store metadata, offsets, etc.).</li> <li> <p>State store: if RocksDB enabled \u2192 state is stored on local disk of each executor (backed by RocksDB), instead of pure JVM memory.</p> </li> <li> <p>Each executor writes RocksDB files under its working directory.</p> </li> <li>Spark will still write state snapshots to the checkpoint location for recovery.</li> </ul>"},{"location":"databricks/12-AutoLoader_Databricks/#5-why-rocksdb-helps","title":"\ud83d\udd39 5. Why RocksDB helps","text":"<ul> <li>Without RocksDB: state is stored in hash maps in JVM heap \u2192 can cause OOM for large joins/aggregations.</li> <li>With RocksDB: state spills efficiently to disk, uses compression, and avoids large JVM GC pressure.</li> <li>Works especially well in Autoloader pipelines with deduplication or watermark joins.</li> </ul>"},{"location":"databricks/12-AutoLoader_Databricks/#6-things-to-remember","title":"\ud83d\udd39 6. Things to remember","text":"<ul> <li>You must use Databricks Runtime 10.4+ for RocksDB state store.</li> <li>The option is:</li> </ul> <pre><code>spark.sql.streaming.stateStore.providerClass=org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\n</code></pre> <p>(Databricks simplifies this with <code>stateStore.rocksdb.enabled</code>). * Still need cloud checkpointLocation \u2192 otherwise recovery after restart won\u2019t work.</p> <p>\u2705 So in summary:</p> <ul> <li>Autoloader always needs a checkpoint location.</li> <li>If you\u2019re doing stateful ops, you can choose between default HDFS state store vs RocksDB.</li> <li>With RocksDB, state is on executor local disk, but still backed up into the checkpoint folder for recovery.</li> </ul> <p>For file notification mode use <code>option(cloudFiles.useNotifications,True)</code></p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/","title":"Autoloader Schema Inference and Evolution","text":""},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#schema-inference-and-evolution-in-auto-loader","title":"Schema Inference and Evolution in Auto Loader","text":"<p>Auto Loader first samples first 50GB or 1000 files whichever limit is crossed first.</p> <p>Auto Loader stores the schema information in a directory _schemas at the configured cloudFiles.schemaLocation to track schema changes to the input data over time.</p> <p><code>spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes</code> and <code>spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles</code> is used to change default configuration.</p> <p>By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don't encode data types (JSON, CSV, and XML), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files.</p> <p>This is different from Spark DataFrameReader that infers the datatypes based on the sample data even if its JSON / CSV.</p> <p>If we want to replicate the DataFrameReader schema evolution use <code>cloudFiles.inferColumnTypes</code> to true.</p> <p>When inferring the schema for CSV data, Auto Loader assumes that the files contain headers. If your CSV files do not contain headers, provide the option .option(\"header\", \"false\"). In addition, Auto Loader merges the schemas of all the files in the sample to come up with a global schema. Auto Loader can then read each file according to its header and parse the CSV correctly.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#what-happens-when-multiple-parquet-files-have-different-types-for-same-column","title":"What Happens when multiple parquet files have different types for same column?","text":"<p>When a column has different data types in two Parquet files, Auto Loader chooses the widest type. You can use schemaHints to override this choice. When you specify schema hints, Auto Loader doesn't cast the column to the specified type, but rather tells the Parquet reader to read the column as the specified type. In the case of a mismatch, the column is rescued in the rescued data column.</p> <p>When a column has different data types in two Parquet files, Auto Loader chooses the widest type. You can use schemaHints to override this choice. When you specify schema hints, Auto Loader doesn't cast the column to the specified type, but rather tells the Parquet reader to read the column as the specified type. In the case of a mismatch, the column is rescued in the rescued data column.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#schema-evolution-in-auto-loader","title":"Schema Evolution in Auto Loader","text":"<p>Auto Loader detects the addition of new columns as it processes your data. When Auto Loader detects a new column, the stream stops with an UnknownFieldException. Before your stream throws this error, Auto Loader performs schema inference on the latest micro-batch of data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#other-modes","title":"Other Modes","text":"<p><code>rescue</code> - Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the rescued data column.</p> <p><code>failOnNewColumns</code> - Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed.</p> <p><code>none</code> - Does not evolve the schema, new columns are ignored, and data is not rescued unless the rescuedDataColumn option is set. Stream does not fail due to schema changes.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#default-mode","title":"Default Mode","text":"<p><code>addNewColumns</code> mode is the default when a schema is not provided, but none is the default when you provide a schema. addNewColumns is not allowed when the schema of the stream is provided, but does work if you provide your schema as a schema hint.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#how-do-partitions-work-in-auto-loader","title":"How Do Partitions Work in Auto Loader?","text":"<p>Auto Loader attempts to infer partition columns from the underlying directory structure of the data if the data is laid out in Hive style partitioning. For example, the file path base_path/event=click/date=2021-04-01/f0.json results in the inference of date and event as partition columns. If the underlying directory structure contains conflicting Hive partitions or doesn't contain Hive style partitioning, partition columns are ignored.</p> <p>Binary file (binaryFile) and text file formats have fixed data schemas, but support partition column inference. Databricks recommends setting cloudFiles.schemaLocation for these file formats. This avoids any potential errors or information loss and prevents inference of partitions columns each time an Auto Loader begins.</p> <p>Partition columns are not considered for schema evolution. If you had an initial directory structure like <code>base_path/event=click/date=2021-04-01/f0.json</code>, and then start receiving new files as <code>base_path/event=click/date=2021-04-01/hour=01/f1.json</code>, Auto Loader ignores the hour column. To capture information for new partition columns, set <code>cloudFiles.partitionColumns</code> to event,date,hour.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#some-caveats-in-rescue-data-mode","title":"Some Caveats in Rescue Data Mode","text":"<p>The JSON and CSV parsers support three modes when parsing records: <code>PERMISSIVE, DROPMALFORMED, and FAILFAST</code>. When used together with rescuedDataColumn, data type mismatches do not cause records to be dropped in DROPMALFORMED mode or throw an error in FAILFAST mode. Only corrupt records are dropped or throw errors, such as incomplete or malformed JSON or CSV. If you use badRecordsPath when parsing JSON or CSV, data type mismatches are not considered as bad records when using the rescuedDataColumn. Only incomplete and malformed JSON or CSV records are stored in <code>badRecordsPath</code>.</p>"},{"location":"databricks/12.1-AutoLoader_Databricks_Schema_Inference/#defining-schema-hints-in-auto-loader","title":"Defining Schema Hints in Auto Loader","text":"<p>Inferred Schema:</p> <pre><code>|-- date: string\n|-- quantity: int\n|-- user_info: struct\n|\u00a0\u00a0\u00a0\u00a0|-- id: string\n|\u00a0\u00a0\u00a0\u00a0|-- name: string\n|\u00a0\u00a0\u00a0\u00a0|-- dob: string\n|-- purchase_options: struct\n|\u00a0\u00a0\u00a0\u00a0|-- delivery_address: string\n</code></pre> <p>By Specifying Schema Hints:</p> <pre><code>.option(\"cloudFiles.schemaHints\", \"date DATE, user_info.dob DATE, purchase_options MAP&lt;STRING,STRING&gt;, time TIMESTAMP\")\n</code></pre> <p>We get</p> <pre><code>|-- date: string -&gt; date\n|-- quantity: int\n|-- user_info: struct\n|\u00a0\u00a0\u00a0\u00a0|-- id: string\n|\u00a0\u00a0\u00a0\u00a0|-- name: string\n|\u00a0\u00a0\u00a0\u00a0|-- dob: string -&gt; date\n|-- purchase_options: struct -&gt; map&lt;string,string&gt;\n|-- time: timestamp\n</code></pre> <p>Schema hints are used only if you do not provide a schema to Auto Loader. You can use schema hints whether cloudFiles.inferColumnTypes is enabled or disabled.</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/","title":"Intro Databricks Lakeflow Declarative Pipelines","text":""},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#lakeflow-declarative-pipelines-introduction","title":"Lakeflow Declarative Pipelines Introduction","text":"<p>Here\u2019s a clear introduction to Lakeflow declarative pipelines in Databricks:</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#1-what-is-lakeflow","title":"1. What is Lakeflow?","text":"<p>Lakeflow is a declarative pipeline framework by Databricks designed to simplify building and managing data pipelines over Delta Lake. Unlike traditional ETL pipelines where you imperatively write every transformation step, Lakeflow allows you to declare the desired end state of your data and lets Databricks handle the execution, orchestration, and dependency management.</p> <p>Think of it like \u201ctelling Databricks what you want, not how to do it\u201d.</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#2-key-concepts","title":"2. Key Concepts","text":"Concept Description Pipeline A logical collection of transformations (like ETL jobs) defined declaratively. Lakeflow Table A Delta table managed by Lakeflow, which tracks lineage, schema, and dependencies. Declarative Config JSON/YAML-like specification describing sources, transformations, and targets. State Management Lakeflow keeps track of which data has been processed and ensures idempotent updates. Incremental Processing Automatically detects new/changed data and applies transformations incrementally."},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#3-how-it-differs-from-normal-pipelines","title":"3. How It Differs from Normal Pipelines","text":"Feature Traditional ETL Lakeflow Declarative Pipeline Definition Imperative: step-by-step code Declarative: specify inputs, outputs, transformations Orchestration Manual or Airflow/Scheduler Built-in, dependency-aware orchestration Data Lineage Requires extra tooling Automatic tracking of lineage between tables Error Handling Manual retries Automatic state management &amp; retries Incremental Loads Developer writes logic Lakeflow detects changes &amp; processes incrementally"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#4-basic-pipeline-flow","title":"4. Basic Pipeline Flow","text":"<ol> <li>Define sources: Raw Delta tables, cloud storage, or external systems.</li> <li>Declare transformations: For example, aggregations, joins, or enrichments.</li> <li>Specify targets: Delta tables managed by Lakeflow.</li> <li>Run pipeline: Databricks ensures only necessary transformations run, handles dependencies, and maintains consistency.</li> </ol>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#5-advantages","title":"5. Advantages","text":"<ul> <li>Less boilerplate code \u2192 focus on business logic, not orchestration.</li> <li>Automatic incremental updates \u2192 avoids reprocessing all data.</li> <li>Built-in lineage and auditing \u2192 helps with compliance and debugging.</li> <li>Easier pipeline management \u2192 declarative config files version-controlled like code.</li> </ul>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#6-example-conceptual","title":"6. Example (Conceptual)","text":"<pre><code># Sample declarative pipeline config\npipeline_name: sales_pipeline\ntables:\n  - name: raw_sales\n    source: s3://data/raw_sales\n  - name: sales_summary\n    depends_on: raw_sales\n    transformations:\n      - type: aggregate\n        group_by: [\"region\", \"category\"]\n        metrics:\n          - name: total_sales\n            operation: sum(amount)\n</code></pre> <p>Here, you declare the desired summary table, and Lakeflow automatically handles reading <code>raw_sales</code>, performing aggregation, and writing <code>sales_summary</code> incrementally.</p> <p>Here\u2019s a detailed breakdown of Streaming Tables, Materialized Views, and Normal Views in Databricks DLT (Delta Live Tables) and their typical usage:</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#1-streaming-tables-aka-live-tables","title":"1. Streaming Tables (aka Live Tables)","text":"<p>Definition: A streaming table in DLT is a Delta table that is continuously updated as new data arrives. It\u2019s typically used for real-time pipelines.</p> <p>Key Characteristics:</p> Feature Description Data Ingestion Continuous from streaming sources (Kafka, Event Hubs, cloud storage). Storage Delta table on Databricks (supports ACID). Processing Mode <code>Append</code> or <code>Upsert</code> (via <code>MERGE</code>). Latency Near real-time; updates as soon as micro-batches are processed. Schema Evolution Supported, can evolve automatically or manually. <p>Use Case:</p> <ul> <li>Real-time dashboards (e.g., sales, stock prices, payments).</li> <li>Event-driven pipelines (e.g., fraud detection, monitoring trades).</li> </ul> <p>Example in DLT (Python):</p> <pre><code>import dlt\nfrom pyspark.sql.functions import *\n\n@dlt.table\ndef streaming_sales():\n    return (\n        spark.readStream.format(\"cloudFiles\")\n             .option(\"cloudFiles.format\", \"json\")\n             .load(\"/mnt/sales/json\")\n             .withColumn(\"processed_time\", current_timestamp())\n    )\n</code></pre>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#2-materialized-views","title":"2. Materialized Views","text":"<p>Definition: A materialized view is a Delta table that stores the precomputed results of a query. In DLT, it is a table whose content is automatically refreshed based on its dependencies.</p> <p>Key Characteristics:</p> Feature Description Data Storage Delta table with physical storage (unlike normal views). Refresh Incremental refresh based on upstream table changes. Performance Faster query since data is precomputed. Up-to-date Always consistent with underlying tables (incrementally). <p>Use Case:</p> <ul> <li>Aggregations (e.g., daily sales per region).</li> <li>Joins and transformations that are expensive to compute on demand.</li> <li>Serving layer for dashboards or ML pipelines.</li> </ul> <p>Example in DLT (Python):</p> <pre><code>import dlt\nfrom pyspark.sql.functions import sum\n\n@dlt.table\ndef sales_summary():\n    sales = dlt.read(\"LIVE.streaming_sales\")\n    return sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales\"))\n</code></pre> <p>Here <code>sales_summary</code> is materialized: it stores the aggregated totals physically and refreshes as new streaming data arrives.</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#3-normal-views","title":"3. Normal Views","text":"<p>Definition: A normal view in DLT is like a virtual table: it does not store data physically. Instead, it executes the underlying query each time you read it.</p> <p>Key Characteristics:</p> Feature Description Data Storage None; only query definition stored. Performance Slower for large or complex queries (computed on-demand). Up-to-date Always reflects the latest upstream data. Incremental Processing Not directly supported (since it\u2019s virtual). <p>Use Case:</p> <ul> <li>Lightweight transformations that do not need storage.</li> <li>Ad-hoc analytics.</li> <li>Debugging or temporary transformations in pipelines.</li> </ul> <p>Example in DLT (Python):</p> <pre><code>import dlt\n\n@dlt.view\ndef high_value_sales():\n    sales = dlt.read(\"LIVE.streaming_sales\")\n    return sales.filter(sales.amount &gt; 10000)\n</code></pre> <p><code>high_value_sales</code> is a normal view: it does not store any data and computes results on demand.</p>"},{"location":"databricks/13-Intro_Databricks_Lakeflow_Declarative_Pipelines/#4-quick-comparison","title":"4. Quick Comparison","text":"Feature Streaming Table Materialized View Normal View Storage Delta (persistent) Delta (precomputed) None Latency Near real-time Incrementally refreshed On-demand Performance High for reads High (precomputed) Low for complex queries Use Case Real-time ingestion Aggregations, dashboards Lightweight transformations, ad-hoc analysis <p>\u2705 Summary / Guidance:</p> <ul> <li>Use streaming tables for continuous ingestion from sources.</li> <li>Use materialized views when you want precomputed results for fast reads and dashboards.</li> <li>Use normal views for on-the-fly filtering or temporary logical tables.</li> </ul>"},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/","title":"Dlt Batch Vs Streaming Workloads","text":""},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#delta-live-tables-batch-vs-streaming-workloads","title":"Delta Live Tables : Batch vs Streaming Workloads","text":"<p>Here\u2019s a detailed breakdown of batch vs streaming workloads for Delta Live Tables (DLT) streaming table sources and when to use each:</p>"},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#1-conceptual-difference","title":"1. Conceptual Difference","text":"Aspect Batch Workload Streaming Workload Data Processing Processes a finite dataset at once. Continuously processes new data as it arrives. Latency High (runs periodically). Low (near real-time). Trigger Manual or scheduled (e.g., daily, hourly). Continuous / micro-batches. State Management Not required; table is fully recomputed or append-only. Required; DLT tracks which data has already been processed. Use Cases Historical analytics, daily aggregates, backfills. Real-time dashboards, alerts, event processing, fraud detection."},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#2-how-dlt-handles-sources","title":"2. How DLT Handles Sources","text":"<p>DLT streaming table sources can come from:</p> <ol> <li> <p>Streaming sources</p> </li> <li> <p>Kafka, Event Hubs, cloud storage (<code>cloudFiles</code>), Delta tables.</p> </li> <li>Automatically incremental, supports schema evolution.</li> <li>Example (Python):</li> </ol> <pre><code>@dlt.table\ndef streaming_sales():\n    return (\n        spark.readStream.format(\"cloudFiles\")\n             .option(\"cloudFiles.format\", \"json\")\n             .load(\"/mnt/sales/json\")\n    )\n</code></pre> <ol> <li> <p>Batch sources</p> </li> <li> <p>Existing Delta tables, Parquet, CSV, or JDBC.</p> </li> <li>DLT can treat these as batch reads if no streaming input is detected.</li> <li>Example (Python):</li> </ol> <pre><code>@dlt.table\ndef daily_sales():\n    return spark.read.format(\"delta\").load(\"/mnt/sales/delta\")\n</code></pre> <p>DLT automatically detects whether the source is streaming or batch, but you can explicitly control behavior with <code>spark.readStream</code> vs <code>spark.read</code>.</p>"},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#3-streaming-table-behavior-in-dlt","title":"3. Streaming Table Behavior in DLT","text":"Feature Behavior for Batch Sources Behavior for Streaming Sources Incremental Processing Only new runs; full dataset read each time unless optimized. DLT tracks offsets/checkpoints to process only new data. State Management Not needed. DLT maintains state to support upserts, aggregations, joins. Dependencies Upstream changes processed on next pipeline run. Changes propagate continuously downstream (materialized views get updated incrementally). Latency Minutes to hours (depends on schedule). Seconds to minutes (micro-batch). Use Case in DLT Backfill historical data, batch pipelines. Real-time dashboards, streaming aggregations, event-driven analytics."},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#4-choosing-between-batch-and-streaming","title":"4. Choosing Between Batch and Streaming","text":"Scenario Preferred Workload Daily sales report from last month Batch Real-time fraud detection on transactions Streaming Hourly ETL from a static CSV dump Batch Continuous clickstream analytics for dashboards Streaming Incremental aggregation from an append-only Delta table Can be either, depending on freshness requirements"},{"location":"databricks/14-DLT_Batch_Vs_Streaming_Workloads/#5-practical-notes-for-dlt","title":"5. Practical Notes for DLT","text":"<ol> <li> <p>Streaming tables can consume batch data</p> </li> <li> <p>You can define a streaming table that reads from a Delta table (<code>spark.readStream.format(\"delta\")</code>), so it behaves like streaming even if the upstream table is batch.</p> </li> <li> <p>Materialized views downstream</p> </li> <li> <p>Always incremental; DLT ensures updates propagate automatically.</p> </li> <li> <p>Checkpointing</p> </li> <li> <p>DLT automatically handles checkpoints for streaming sources. You don\u2019t need to manage offsets manually.</p> </li> </ol> <p>\u2705 Summary:</p> <ul> <li>Batch workloads \u2192 finite, scheduled processing.</li> <li>Streaming workloads \u2192 continuous, low-latency processing with state management.</li> <li>DLT streaming tables adapt to both but are most powerful when the source is streaming.</li> </ul> <p>Do you want me to make that diagram?</p>"},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/","title":"Dlt Data Storage Checkpoints","text":""},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/#data-storage-internals-checkpoints-and-renaming-streaming-tables-and-views","title":"Data Storage Internals, Checkpoints and Renaming Streaming Tables and Views","text":"<p>Here\u2019s a detailed explanation of how Delta Live Tables (DLT) handles storage for streaming tables, pipeline dependency, and renaming behavior:</p>"},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/#1-where-is-data-for-streaming-tables-stored","title":"1. Where is data for streaming tables stored?","text":"<ul> <li>Streaming tables in DLT are stored as Delta tables on the Databricks File System (DBFS) or your cloud storage configured for the pipeline, e.g., S3, ADLS Gen2, or GCS.</li> <li>Every streaming table has a physical Delta table location, even though you define it declaratively in DLT.</li> <li>The storage location is typically managed by the DLT pipeline, but you can explicitly configure it in advanced pipeline settings.</li> </ul> <p>Key points:</p> <ul> <li>Incremental state (processed offsets, checkpoints) is stored in _system-managed checkpoints within the pipeline\u2019s storage path.</li> <li>Upserts and merges are persisted in the Delta table itself.</li> <li>Data retention and compaction follow normal Delta table rules.</li> </ul>"},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/#2-is-storage-dependent-on-the-pipeline","title":"2. Is storage dependent on the pipeline?","text":"<p>Yes, partially:</p> <ul> <li>Pipeline-specific storage:   Each DLT pipeline manages its own metadata and checkpoints for the streaming tables it owns.</li> <li>Shared tables:   If multiple pipelines reference the same Delta table (e.g., using <code>LIVE.&lt;table_name&gt;</code>), the physical Delta table is shared, but each pipeline maintains its own lineage and state metadata.</li> </ul> <p>Implication:</p> <ul> <li>Deleting a pipeline does not delete the underlying Delta table automatically, unless you explicitly choose managed tables.</li> <li>Changing pipelines (like moving a table to a different pipeline) requires careful handling to avoid breaking downstream dependencies.</li> </ul>"},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/#3-what-happens-when-we-rename-streaming-tables","title":"3. What happens when we rename streaming tables?","text":"<ul> <li>DLT does not support a \u201crename\u201d operation in place for streaming tables.</li> <li> <p>If you rename a table in DLT:</p> </li> <li> <p>The new table name points to a new managed object in the pipeline.</p> </li> <li>The underlying Delta data is copied or remapped depending on configuration.</li> <li>Any downstream references (<code>LIVE.&lt;old_name&gt;</code>) break unless you update them to the new name.</li> <li> <p>Best practice:</p> </li> <li> <p>Avoid renaming streaming tables in active pipelines.</p> </li> <li>If renaming is needed, create a new table with the desired name and point downstream materialized views or pipelines to it.</li> </ul>"},{"location":"databricks/15-DLT_Data_Storage_Checkpoints/#4-practical-notes-recommendations","title":"4. Practical Notes / Recommendations","text":"Aspect Recommendation Storage Let DLT manage default Delta table locations unless you need a custom path. Pipeline dependency Be aware that streaming tables are tied to pipeline metadata (checkpoints, lineage). Renaming Prefer creating a new table and updating downstream references; avoid in-place renames for live pipelines. Backup If renaming or moving, snapshot or backup Delta tables to avoid data loss. <p>\u2705 Summary:</p> <ol> <li>Streaming tables always persist data as Delta tables in the pipeline\u2019s storage.</li> <li>Storage and checkpoints are pipeline-dependent, but the data itself can be shared.</li> <li>Renaming a streaming table breaks dependencies; best approach is to create a new table instead of renaming.</li> </ol>"},{"location":"databricks/16-Databricks_Secret_Scopes/","title":"Databricks Secret Scopes","text":""},{"location":"databricks/16-Databricks_Secret_Scopes/#databricks-secret-scopes-using-azure-key-vault","title":"Databricks Secret Scopes Using Azure Key Vault","text":""},{"location":"databricks/16-Databricks_Secret_Scopes/#1-prerequisites","title":"\ud83d\udd39 1. Prerequisites","text":"<ul> <li>An Azure Databricks workspace (Premium or above recommended).</li> <li>An Azure Key Vault created.</li> <li>Appropriate RBAC permissions (Owner/Contributor + Key Vault Administrator).</li> <li>Networking planned: VNet injection or secure workspace.</li> </ul>"},{"location":"databricks/16-Databricks_Secret_Scopes/#2-create-an-azure-key-vault","title":"\ud83d\udd39 2. Create an Azure Key Vault","text":"<ol> <li>Go to Azure Portal \u2192 Create a resource \u2192 Key Vault.</li> <li> <p>Set:</p> </li> <li> <p>Resource Group: choose existing/new.</p> </li> <li>Region: same as your Databricks workspace (recommended for latency &amp; compliance).</li> <li>Pricing tier: Standard.</li> <li> <p>In Access configuration:</p> </li> <li> <p>Choose RBAC (recommended) OR Vault access policy.</p> </li> <li>For Databricks, RBAC is simpler to manage at scale.</li> <li>Finish creation.</li> </ol>"},{"location":"databricks/16-Databricks_Secret_Scopes/#3-configure-networking-on-key-vault","title":"\ud83d\udd39 3. Configure Networking on Key Vault","text":"<ul> <li> <p>In Key Vault \u2192 Networking:</p> </li> <li> <p>Set Public access:</p> <ul> <li>Choose Disabled if you want private access only.</li> <li>Or choose Selected networks and allow only your Databricks subnets.</li> <li> <p>If you\u2019re using Private Endpoints:</p> </li> <li> <p>Click + Private Endpoint.</p> </li> <li>Link it to your Databricks VNet subnet.</li> <li>Approve the private endpoint connection in Key Vault.</li> </ul> </li> </ul>"},{"location":"databricks/16-Databricks_Secret_Scopes/#4-store-secrets-in-key-vault","title":"\ud83d\udd39 4. Store Secrets in Key Vault","text":"<ol> <li>In Key Vault \u2192 Objects \u2192 Secrets \u2192 + Generate/Import.</li> <li> <p>Add secrets, e.g.:</p> </li> <li> <p><code>db-password</code></p> </li> <li><code>api-key</code></li> </ol>"},{"location":"databricks/16-Databricks_Secret_Scopes/#5-create-a-databricks-secret-scope-backed-by-key-vault","title":"\ud83d\udd39 5. Create a Databricks Secret Scope Backed by Key Vault","text":"<p>In Databricks workspace:</p> <ol> <li> <p>Open Azure Databricks \u2192 Manage Account \u2192 User Settings \u2192 Access Tokens.</p> </li> <li> <p>Generate a personal access token if you\u2019ll use the CLI.</p> </li> <li>Run Databricks CLI (or REST API) to create the scope:</li> </ol> <pre><code>databricks secrets create-scope \\\n  --scope my-keyvault-scope \\\n  --scope-backend-type AZURE_KEYVAULT \\\n  --resource-id \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg-name&gt;/providers/Microsoft.KeyVault/vaults/&lt;kv-name&gt;\" \\\n  --dns-name \"https://&lt;kv-name&gt;.vault.azure.net/\"\n</code></pre> <p>\u2705 Now, secrets in Key Vault are accessible from Databricks under the scope <code>my-keyvault-scope</code>.</p>"},{"location":"databricks/16-Databricks_Secret_Scopes/#6-access-secrets-in-databricks-notebook","title":"\ud83d\udd39 6. Access Secrets in Databricks Notebook","text":"<pre><code># Example: get db-password stored in Key Vault\ndb_password = dbutils.secrets.get(scope=\"my-keyvault-scope\", key=\"db-password\")\n\nprint(\"Fetched secret length:\", len(db_password))  # don\u2019t print actual secret!\n</code></pre>"},{"location":"databricks/16-Databricks_Secret_Scopes/#7-networking-configurations-important","title":"\ud83d\udd39 7. Networking Configurations (Important)","text":"<p>You have two secure options for Key Vault \u2194 Databricks:</p>"},{"location":"databricks/16-Databricks_Secret_Scopes/#option-a-vnet-injection-most-common-for-secure-workspaces","title":"Option A: VNet Injection (most common for secure workspaces)","text":"<ul> <li>Deploy Databricks workspace in your own VNet.</li> <li> <p>Add service endpoints for Key Vault:</p> </li> <li> <p>Go to Databricks VNet \u2192 Subnets \u2192 Service endpoints \u2192 Add KeyVault.</p> </li> <li>In Key Vault Networking, allow that subnet.</li> </ul>"},{"location":"databricks/16-Databricks_Secret_Scopes/#option-b-private-endpoint","title":"Option B: Private Endpoint","text":"<ul> <li>Create a private endpoint for Key Vault in the Databricks VNet.</li> <li>Approve the connection in Key Vault.</li> <li>Disable public network access for maximum security.</li> </ul>"},{"location":"databricks/16-Databricks_Secret_Scopes/#8-permissions","title":"\ud83d\udd39 8. Permissions","text":"<ul> <li> <p>In Key Vault \u2192 Access Control (IAM):</p> </li> <li> <p>Assign your Databricks workspace managed identity:</p> <ul> <li>Key Vault Secrets User role.</li> <li>Or Key Vault Reader role (depending on setup).</li> </ul> </li> </ul>"},{"location":"databricks/16-Databricks_Secret_Scopes/#summary","title":"\u2705 Summary","text":"<ol> <li>Create Key Vault \u2192 Add secrets.</li> <li>Configure networking (service endpoint or private endpoint).</li> <li>Assign RBAC so Databricks can access Key Vault.</li> <li>Create secret scope in Databricks (backed by Key Vault).</li> <li>Access secrets inside notebooks with <code>dbutils.secrets.get</code>.</li> </ol> <p>Would you like me to also draw you a network diagram (Databricks \u2192 VNet \u2192 Key Vault with private endpoint) so you can visualize the traffic flow?</p>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/","title":"Databricks Controlplane Dataplane","text":""},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#databricks-control-plane-and-data-plane","title":"Databricks Control Plane and Data Plane","text":""},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#what-is-databricks-lakehouse-architecture","title":"\ud83d\ude80 What is Databricks Lakehouse Architecture?","text":"<p>Traditionally, companies had two separate systems:</p> <ul> <li>Data Lake \ud83e\udea3 (cheap storage, e.g., Azure Data Lake, S3, Blob): stores raw structured, semi-structured, unstructured data \u2192 flexible but lacks strong data management (ACID, governance, BI).</li> <li>Data Warehouse \ud83c\udfe2 (expensive but fast): optimized for SQL queries, BI, and analytics \u2192 great schema enforcement and governance but limited flexibility and costly.</li> </ul> <p>\ud83d\udd39 The Lakehouse combines both in one system:</p> <ul> <li>The low-cost, flexible storage of a data lake</li> <li>The governance, ACID transactions, performance of a warehouse</li> </ul>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#core-components-of-databricks-lakehouse","title":"\ud83c\udfd7\ufe0f Core Components of Databricks Lakehouse","text":"<ol> <li> <p>Storage Layer (Data Lake foundation)</p> </li> <li> <p>Data stored in open formats like Parquet, ORC, Avro, Delta.</p> </li> <li> <p>Uses cloud object storage (e.g., Azure Data Lake Storage Gen2, AWS S3, GCS).</p> </li> <li> <p>Delta Lake (the secret sauce \ud83e\uddc2)</p> </li> <li> <p>Adds ACID transactions on top of data lake storage.</p> </li> <li>Provides schema enforcement, schema evolution, time travel, data versioning.</li> <li> <p>Solves problems like \u201ceventual consistency\u201d and corrupted files in raw data lakes.</p> </li> <li> <p>Unified Governance (Unity Catalog)</p> </li> <li> <p>Centralized metadata &amp; permissions for files, tables, ML models, dashboards.</p> </li> <li> <p>Manages security, lineage, and data discovery across the Lakehouse.</p> </li> <li> <p>Compute Layer (Databricks Runtime / Spark + Photon)</p> </li> <li> <p>Uses Apache Spark + Photon execution engine for batch, streaming, ML, BI.</p> </li> <li> <p>Same engine for ETL, streaming, AI, SQL queries \u2192 no silos.</p> </li> <li> <p>Data Management Features</p> </li> <li> <p>Streaming + Batch = One Pipeline (via Delta Live Tables).</p> </li> <li>Materialized Views, Incremental Processing, Change Data Capture (CDC).</li> <li>MLflow integration for machine learning lifecycle management.</li> </ol>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#architecture-diagram-conceptual-flow","title":"\ud83d\udcca Architecture Diagram (Conceptual Flow)","text":"<pre><code>                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Business Apps / BI  \u2502\n                \u2502 (Power BI, Tableau)   \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Databricks SQL      \u2502\n                \u2502   &amp; Photon Engine     \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502          Delta Lake (ACID, Schema, CDC)       \u2502\n   \u2502   (Open Storage Format on Parquet + Log)      \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Cloud Object Store  \u2502\n                \u2502 (ADLS, S3, GCS)       \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#benefits-of-lakehouse","title":"\u26a1 Benefits of Lakehouse","text":"<ul> <li>\u2705 One platform \u2192 no need for separate warehouse + lake.</li> <li>\u2705 Cost efficient \u2192 cheap storage, scalable compute.</li> <li>\u2705 Flexibility \u2192 structured + semi-structured + unstructured.</li> <li>\u2705 ACID reliability \u2192 transactions, schema enforcement.</li> <li>\u2705 End-to-end \u2192 supports ETL, real-time streaming, ML/AI, BI in the same system.</li> </ul>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#in-databricks-azure-context","title":"\ud83c\udf10 In Databricks Azure Context","text":"<ul> <li>Storage \u2192 Azure Data Lake Storage (ADLS Gen2)</li> <li>Security/Governance \u2192 Azure Key Vault + Unity Catalog</li> <li>Compute \u2192 Databricks Clusters with Photon</li> <li>Serving \u2192 Power BI (Direct Lake Mode)</li> </ul>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#data-plane-vs-control-plane","title":"Data Plane vs Control Plane","text":""},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#1-simple-analogy","title":"\ud83d\udea6 1. Simple Analogy","text":"<p>Think of Databricks like Uber:</p> <ul> <li>Control Plane = Uber App \ud83d\udcf1 \u2192 handles where you go, who drives, billing, monitoring.</li> <li>Data Plane = The Car \ud83d\ude97 \u2192 where the actual ride happens (your data processing).</li> </ul> <p>So, Databricks separates management functions (control) from execution functions (data).</p>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#2-databricks-architecture","title":"\ud83c\udfd7\ufe0f 2. Databricks Architecture","text":""},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#control-plane","title":"\ud83d\udd39 Control Plane","text":"<ul> <li>Managed by Databricks itself (runs in Databricks\u2019 own AWS/Azure/GCP accounts).</li> <li> <p>Contains:</p> </li> <li> <p>Web UI / REST API \u2192 you log in here, create clusters, manage jobs.</p> </li> <li>Cluster Manager \u2192 decides how to spin up VMs/compute.</li> <li>Job Scheduler \u2192 triggers pipelines, notebooks, workflows.</li> <li>Metadata Storage \u2192 notebooks, workspace configs, Unity Catalog metadata.</li> <li>Monitoring / Logging \u2192 cluster health, job logs, error reporting.</li> </ul> <p>\u26a0\ufe0f Important: Your raw data does not go here. This plane is about orchestration, configs, and metadata.</p>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#data-plane","title":"\ud83d\udd39 Data Plane","text":"<ul> <li>Runs inside your cloud account (your subscription/project).</li> <li> <p>Contains:</p> </li> <li> <p>Clusters/Compute (Spark Executors, Driver, Photon) \u2192 where the data is processed.</p> </li> <li>Your Data \u2192 stored in ADLS, S3, or GCS.</li> <li>Networking \u2192 VNETs, Private Endpoints, Peering.</li> <li>Libraries / Runtime \u2192 Spark, Delta Lake, MLflow, etc.</li> </ul> <p>\u26a0\ufe0f Key Point: The actual data never leaves your cloud account. Processing happens within your boundary.</p>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#3-security-perspective","title":"\ud83d\udd10 3. Security Perspective","text":"<ul> <li> <p>Control Plane:</p> </li> <li> <p>Managed by Databricks.</p> </li> <li>Contains metadata, credentials, configs, but not raw data.</li> <li> <p>Can be hardened with SCIM, SSO, RBAC, IP Access Lists.</p> </li> <li> <p>Data Plane:</p> </li> <li> <p>Fully inside your cloud subscription.</p> </li> <li>Your sensitive data (PII, transactions, crypto, etc.) never touches Databricks\u2019 account.</li> <li> <p>You control networking:</p> <ul> <li>Private Link / VNET Injection \u2192 ensures traffic never goes over the public internet.</li> <li>Key Vault / KMS for secrets.</li> <li>Storage firewalls.</li> </ul> </li> </ul>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#4-architecture-diagram","title":"\ud83d\uddbc\ufe0f 4. Architecture Diagram","text":"<pre><code>               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502           CONTROL PLANE              \u2502\n               \u2502  (Databricks-managed account)        \u2502\n               \u2502                                      \u2502\n               \u2502  - Web UI / API                      \u2502\n               \u2502  - Cluster Manager                   \u2502\n               \u2502  - Job Scheduler                     \u2502\n               \u2502  - Unity Catalog Metadata            \u2502\n               \u2502  - Logs / Monitoring                 \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u2502 Secure REST/API Calls\n                               \u2502\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502             DATA PLANE                \u2502\n               \u2502   (Your cloud subscription/project)   \u2502\n               \u2502                                       \u2502\n               \u2502  - Spark Driver &amp; Executors           \u2502\n               \u2502  - Photon Engine                      \u2502\n               \u2502  - Data in ADLS/S3/GCS                \u2502\n               \u2502  - Networking (VNET, Firewall, PEs)   \u2502\n               \u2502  - Secrets from Key Vault/KMS         \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#5-why-this-separation","title":"\u26a1 5. Why This Separation?","text":"<p>\u2705 Security \u2192 Your data never leaves your account. \u2705 Scalability \u2192 Databricks manages orchestration, you manage compute. \u2705 Multi-cloud \u2192 Same control plane works across AWS, Azure, GCP. \u2705 Compliance \u2192 Helps with HIPAA, GDPR, financial regulations.</p>"},{"location":"databricks/17-Databricks_ControlPlane_DataPlane/#6-special-feature-databricks-serverless-sql","title":"\ud83d\udd11 6. Special Feature: Databricks Serverless SQL","text":"<ul> <li>Here, the data plane compute is managed by Databricks too (not your account).</li> <li>Good for quick BI queries (like Power BI), but some enterprises avoid it for sensitive data.</li> </ul>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/","title":"Databricks Dlt Code Walkthrough","text":""},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#delta-live-tables-code-walkthrough","title":"Delta Live Tables Code Walkthrough","text":"<ol> <li>Create Streaming Table for Orders</li> </ol> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"bronze\"},\n  comment = \"Orders Bronze Table\"\n)\ndef orders_bronze():\n  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n  return df\n</code></pre> <ol> <li>Create Materialized View for Customers</li> </ol> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"bronze\"},\n  comment = \"Customers Materialized View\"\n)\ndef customers_bronze():\n  df = spark.read.table(\"dev.bronze.customers_raw\")\n  return df\n</code></pre> <ol> <li>Create a view that joins above streaming table and materialized view</li> </ol> <pre><code>@dlt.view(\n  comment = 'Joined View'\n)\ndef joined_vw():\n  df_c = spark.read.table(\"LIVE.customers_bronze\")\n  df_o = spark.read.table(\"LIVE.orders_bronze\")\n\n  df_join = df_o.join(df_c,how = \"left_outer\",on = df_c.c_custkey==df_o.o_custkey)\n\n  return df_join  \n</code></pre> <ol> <li>Add a new column to the view</li> </ol> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"silver\"},\n  comment = \"joined table\",\n  name = 'joined_silver'\n)\ndef joined_silver():\n  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"_insertdate\",current_timestamp())\n  return df\n</code></pre> <ol> <li>Create gold level aggregation</li> </ol> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"gold\"},\n  comment = \"orders aggregated table\",\n)\ndef joined_silver():\n  df = spark.read.table(\"LIVE.joined_silver\")\n\n  df_final = df.groupBy('c_mktsegment').agg(count('o_orderkey').alias('sum_orders').withColumn('_insertdate',current_timestamp()))\n  return df_final\n</code></pre>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#deleting-dlt-pipeline","title":"Deleting DLT Pipeline","text":"<p>The tables / datasets in DLT are managed and linked to DLT pipelines. So if we delete a pipleine all fo them get dropped.</p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#incremental-load-in-dlt","title":"Incremental Load in DLT","text":"<p>When we inserted 10k records into orders_bronze, only those got ingested not the entire table.</p> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#adding-new-column","title":"Adding New Column","text":"<pre><code>@dlt.table(\n  table_properties = {\"quality\":\"gold\"},\n  comment = \"orders aggregated table\",\n)\ndef joined_silver():\n  df = spark.read.table(\"LIVE.joined_silver\")\n\n  df_final = df.groupBy('c_mktsegment').agg(count('o_orderkey').alias('sum_orders').agg(sum('o_totalprice').alias('sum_price').withColumn('_insertdate',current_timestamp()))\n  return df_final\n</code></pre> <p>We dont have to manipulate ddl, the dlt pipeline will auto detect addition of new column.</p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#renaming-tables","title":"Renaming Tables","text":"<p>We just change the name of the function in the table declaration and the table name will be renamed. The catalog will also reflect this.</p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#dlt-internals","title":"DLT Internals","text":"<p>Every streaming table, MV is supported by underlying tables in <code>_databricks_internal</code> schema.</p> <p></p> <p>and they have a table_id associated with it.</p> <p>If we go to these tables in storage account, we can see checkpoints that keep track of incremental data changes.</p> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#data-lineage","title":"Data Lineage","text":""},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#dlt-append-flow-and-autoloader","title":"DLT Append Flow and Autoloader","text":"<pre><code>@dlt.table(\n  table_properties = {\"quality\":\"bronze\"},\n  comment = \"orders autoloader\",\n  name = \"orders_autoloader_bronze\"\n)\ndef func():\n  df = (\n      spark.readStream\n      .format(\"cloudFiles\")\n      .option(\"cloudFilesFormat\",\"CSV\")\n      .option(\"cloudFiles.schemaLocation\",\"...\")\n      .option(\"pathGlobFilter\",\"*.csv\")\n      .option(\"cloudFiles.schemaEvolutionMode\",\"none\")\n      .load(\"/Volumes/etl/landing/files\"\n)\nreturn df\n</code></pre> <pre><code>dlt.createStreamingTable(\"order_union_bronze\")\n\n@dlt.append_flow(\n  target = \"orders_union_bronze\"\n)\ndef order_delta_append():\n  df = spark.readStream.table(\"LIVE.orders_bronze\")\n  return df\n\n@dlt.append_flow(\n  target = \"orders_union_bronze\"\n)\ndef order_autoloader_append():\n  df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n  return df\n</code></pre> <pre><code>@dlt.view(\n  comment = 'Joined View'\n)\ndef joined_vw():\n  df_c = spark.read.table(\"LIVE.customers_bronze\")\n  df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n\n  df_join = df_o.join(df_c,how = \"left_outer\",on = df_c.c_custkey==df_o.o_custkey)\n\n  return df_join  \n</code></pre>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#custom-configuration","title":"Custom Configuration","text":"<p>Use this param in code</p> <pre><code>_order_status = spark.conf.get(\"custom.orderStatus\",\"_NA\")\n</code></pre> <pre><code>for _status in _order_status.split(\",\"):\n    # create gold table\n    @dlt.table(\n        table_properties = {\"quality\":\"gold\"},\n        comment = \"order aggregated table\",\n        name = f\"orders_agg_{_status}_gold\"\n    )\n    def orders_aggregated_gold():\n        df = spark.read.table(\"LIVE.joined_silver\")\n        df_final = df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count('o_orderkey').alias(\"count_of_orders\"),sum(\"o_totalprice\").alias('sum_totalprice')).withColumn(\"_insert_date\", current_timestamp())\n\n        return df_final\n</code></pre> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#dlt-scd1-and-scd2","title":"DLT SCD1 and SCD2","text":"<p>Pre Requisites</p> <p></p> <p>Input Source Table</p> <pre><code>@dlt.view(\n  comment = \"Customer Bronze streaming view\"\n)\ndef customer_bronze():\n  df = spark.readStream.table(\"dev.bronze.customers_raw\")\n  return df\n</code></pre> <p>SCD Type1 Table</p> <pre><code>dlt.create_streaming_table('customer_sdc1_bronze')\n\ndlt.apply_changes(\n  target = \"customer_scd1_bronze\",\n  source = \"customer_bronze_vw\",\n  keys = ['c_custkey'],\n  stored_as_scd_type = 1,\n  apply_as_deletes = expr(\"__src_action = 'D'\"),\n  apply_as_truncates = expr(\"__src_action = 'T'\"),\n  sequence_by = \"__src_insert_dt\"\n)\n</code></pre> <p>SCD Type 2 Table</p> <pre><code>dlt.create_streaming_table('customer_sdc2_bronze')\n\ndlt.apply_changes(\n  target = \"customer_scd1_bronze\",\n  source = \"customer_bronze_vw\",\n  keys = ['c_custkey'],\n  stored_as_scd_type = 2,\n  except_column_list = ['__src_action','__src_insert_dt']\n  sequence_by = \"__src_insert_dt\"\n)\n</code></pre> <p>Changes in view to make SCD2 applicable</p> <pre><code>@dlt.view(\n  comment = 'Joined View'\n)\ndef joined_vw():\n  df_c = spark.read.table(\"LIVE.customers_scd2_bronze\").where(\"__END_AT is null\")\n  df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n\n  df_join = df_o.join(df_c,how = \"left_outer\",on = df_c.c_custkey==df_o.o_custkey)\n\n  return df_join  \n</code></pre> <p></p> <p>After inserting record with update the <code>__END_AT</code> for the new update is null signifying its the latest update</p> <p></p> <p>In SCD Type1 just the update is captured.</p> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#insert-old-timestamp-record","title":"Insert Old Timestamp record","text":""},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#scd-type1-vs-scd-type2-delete-records","title":"SCD Type1 vs SCD Type2 Delete Records","text":""},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#rules-for-data-quality-warn-drop-and-fail","title":"Rules for Data Quality : Warn, Drop and Fail","text":"<p>Defining the Rules</p> <pre><code>__order_rules = {\n  \"Valid Order Status\" : \"o_order_status in ('O','F','P')\",\n  \"Valid Order Price\" : \"o_orderprice &gt; 0\"\n}\n\n__customer_rules = {\n  \"valid market segment\" : \"c_mktsegment is not null\"\n}\n</code></pre> <p>Adding the rules</p> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"bronze\"},\n  comment = \"Orders Bronze Table\"\n)\n@dlt.expect_all(__order_rules) # warn\ndef orders_bronze():\n  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n  return df\n</code></pre> <pre><code>@dlt.table(\n  table_properties = {\"quality\":\"bronze\"},\n  comment = \"Customers Materialized View\"\n)\n@dlt.expect_all(__customer_rules) # warn\ndef customers_bronze():\n  df = spark.read.table(\"dev.bronze.customers_raw\")\n  return df\n</code></pre> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#edge-case","title":"Edge Case","text":"<p>Number of failed records here is 2, but in source table only one record was flawed, but since there are two consumers it shows 2 records failed.</p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#using-expectations-on-the-view","title":"Using Expectations on the view","text":"<p>Even though on top we can see market segment is null, since we are doing a left join and the joined view does not have details for the customer 99999,(because it failed expectation and record was dropped), so there were no failed records at all.</p> <p></p>"},{"location":"databricks/18-Databricks_DLT_Code_Walkthrough/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>Check this link</p>"},{"location":"databricks/19-Databricks_Serverless_Compute/","title":"Databricks Serverless Compute","text":""},{"location":"databricks/19-Databricks_Serverless_Compute/#databricks-serverless-compute","title":"Databricks Serverless Compute","text":""},{"location":"databricks/19-Databricks_Serverless_Compute/#architecture","title":"Architecture","text":"<p>Unlike Classic Architecture, the compute is not on the data plane, its on the compute plane managed by Databricks.</p> <p>Databricks spins up clusters in the same region as the workspace to reduce latency.</p> <p>The VM's used for one customer is not reused again.</p> <p>We need to enable serverless compute in account console to create serverless cluster.</p>"},{"location":"databricks/20-Databricks_Warehouses/","title":"Databricks Warehouses","text":""},{"location":"databricks/20-Databricks_Warehouses/#databricks-sql-warehouses","title":"Databricks SQL Warehouses","text":""},{"location":"databricks/20-Databricks_Warehouses/#types-of-sql-warehouses","title":"Types of SQL Warehouses","text":""},{"location":"databricks/20-Databricks_Warehouses/#sample-sql-query-and-monitoring","title":"Sample SQL Query and Monitoring","text":"<pre><code>SELECT c.c_mktsegment,count(o.o_orderkey) total_orders\nFROM\ndev.etl.orders_bronze o\nLEFT JOIN dev.etl.customer_scd2_bronze c\nON o.o_custkey = c.c_custkey\nWHERE c.__END_AT is null\ngroup by c.c_mktsegment\n</code></pre> <p>Query Monitoring</p> <p></p>"},{"location":"databricks/21-Databricks_Lakehouse_Federation/","title":"Databricks Lakehouse Federation","text":""},{"location":"databricks/21-Databricks_Lakehouse_Federation/#lakehouse-federation-in-databricks","title":"Lakehouse Federation in Databricks","text":"<p>This feature allows us to query external system data without replicating storage on databricks.</p>"},{"location":"databricks/21-Databricks_Lakehouse_Federation/#steps-to-setup-federation","title":"Steps to Setup Federation","text":"<p>We need the external location to setup connection.</p> <p></p> <p>Create a new Connection.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <ul> <li>Foreign catalogs are read only.</li> <li>Any catalog that doesnt come under Unity Catalog scheme of things is Foreign Catalog.</li> <li>We can manage permissions and also check lineage of tables of Foreign Catalogs.</li> </ul> <p>Creating Catalogs using SQL</p> <p></p>"},{"location":"databricks/22-Databricks_Metrics_Views/","title":"Metrics Views in Databricks","text":""},{"location":"databricks/22-Databricks_Metrics_Views/#example-use-case","title":"Example Use Case","text":""},{"location":"databricks/22-Databricks_Metrics_Views/#how-to-create-metrics-views-in-databricks","title":"How to Create Metrics Views in Databricks?","text":"<p>Measures are quantitative values like sales, revenue etc.</p> <p>Dimensions are used to support measures like year, month etc. Eg: sales per year, revenue per month</p> <p>Any joins in metric views are left outer joins.</p> <p>Metric views are defined in yaml format</p> <p>Eg:</p> <pre><code>version: 0.1\n\nsource: dev.bronze.orders_raw\n\ndimensions:\n  - name: Order Key\n    expr: o_orderkey\n\n  - name: Customer Key\n    expr: o_custkey\n\n  - name: Order Status\n    expr: o_orderstatus\n\n  - name: Order Status Readable\n    expr: &gt;\n      case \n        when o_orderstatus = 'O' then 'Open'\n        when o_orderstatus = 'F' then 'Fulfilling'\n        when o_orderstatus = 'P' then 'Processing'\n      end\n\n  - name: Order Date\n    expr: o_orderdate\n\n  - name: Order Year\n    expr: DATE_TRUNC('year',o_orderdate)\n\n  - name: Order Priority\n    expr: o_orderpriority\n\n  - name: Clerk\n    expr: o_clerk\n\n  - name: Ship Priority\n    expr: o_shippriority\n\nmeasures:\n  - name: Total Price\n    expr: SUM(o_totalprice)\n\n  - name: Average Total Price\n    expr: AVG(o_totalprice)\n\n  - name: Count of Orders\n    expr: COUNT(DISTINCT(o_orderkey))\n</code></pre> <p></p> <p></p> <p>Querying the metric view</p> <p></p>"},{"location":"databricks/22-Databricks_Metrics_Views/#window-functions","title":"Window Functions","text":"<pre><code>measures:\n  - name: Total Price\n    expr: SUM(o_totalprice)\n\n  - name: Average Total Price\n    expr: AVG(o_totalprice)\n\n  - name: Count of Orders\n    expr: COUNT(DISTINCT(o_orderkey))\n\n  - name: Total Orders Current Year\n    expr: COUNT(DISTINCT o_orderkey)\n    window:\n      - order: Order Year\n        range: current\n        semiadditive: last\n\n  - name: Total Orders Last Year\n    expr: COUNT(DISTINCT o_orderkey)\n    window:\n      - order: Order Year\n        range: trailing 1 year\n        semiadditive: last\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#final-metric-views-with-joins","title":"Final Metric Views with Joins","text":"<pre><code>version: 0.1\n\nsource: dev.bronze.orders_raw\n\njoins:\n  - name: cust_dim\n    source: dev.bronze.customer_raw\n    on: cust_dim.c_custkey = source.o_custkey\n\ndimensions:\n  - name: Order Key\n    expr: o_orderkey\n\n  - name: Customer Key\n    expr: o_custkey\n\n  - name: Order Status\n    expr: o_orderstatus\n\n  - name: Order Status Readable\n    expr: &gt;\n      case \n        when o_orderstatus = 'O' then 'Open'\n        when o_orderstatus = 'F' then 'Fulfilling'\n        when o_orderstatus = 'P' then 'Processing'\n      end\n\n  - name: Customer Market Segment\n    expr: cust_dim.c_mktsegment\n\n  - name: Order Date\n    expr: o_orderdate\n\n  - name: Order Year\n    expr: DATE_TRUNC('year',o_orderdate)\n\n  - name: Order Priority\n    expr: o_orderpriority\n\n  - name: Clerk\n    expr: o_clerk\n\n  - name: Ship Priority\n    expr: o_shippriority\n\nmeasures:\n  - name: Total Price\n    expr: SUM(o_totalprice)\n\n  - name: Average Total Price\n    expr: AVG(o_totalprice)\n\n  - name: Count of Orders\n    expr: COUNT(DISTINCT(o_orderkey))\n\n  - name: Total Orders Current Year\n    expr: COUNT(DISTINCT o_orderkey)\n    window:\n      - order: Order Year\n        range: current\n        semiadditive: last\n\n  - name: Total Orders Last Year\n    expr: COUNT(DISTINCT o_orderkey)\n    window:\n      - order: Order Year\n        range: trailing 1 year\n        semiadditive: last\n\n  - name: Year on Year Growth\n    expr: MEASURE(`Total Orders Current Year`)-MEASURE(`Total Orders Last Year`)\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#clear-example-of-metric-view-vs-standard-view","title":"Clear Example of Metric View vs Standard View","text":""},{"location":"databricks/22-Databricks_Metrics_Views/#1-standard-view-approach","title":"1. Standard View Approach","text":"<p>If you only had standard views, you\u2019d need separate SQL queries (or separate views) for each grouping level.</p>"},{"location":"databricks/22-Databricks_Metrics_Views/#a-group-by-state","title":"(a) Group by State","text":"<pre><code>SELECT\n    state,\n    SUM(revenue) / COUNT(DISTINCT customer_id) AS revenue_per_customer\nFROM orders\nGROUP BY state;\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#b-group-by-region","title":"(b) Group by Region","text":"<pre><code>SELECT\n    region,\n    SUM(revenue) / COUNT(DISTINCT customer_id) AS revenue_per_customer\nFROM orders\nGROUP BY region;\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#c-group-by-country","title":"(c) Group by Country","text":"<pre><code>SELECT\n    country,\n    SUM(revenue) / COUNT(DISTINCT customer_id) AS revenue_per_customer\nFROM orders\nGROUP BY country;\n</code></pre> <p>\ud83d\udc49 Notice: you either write multiple queries/views, or you compute all combinations in one big query with <code>GROUP BY CUBE(country, region, state)</code> (which can be heavy).</p>"},{"location":"databricks/22-Databricks_Metrics_Views/#2-metric-view-approach","title":"2. Metric View Approach","text":"<p>With a metric view, you define the metric once:</p> <pre><code>CREATE OR REPLACE METRIC VIEW revenue_metrics\nAS SELECT\n    country,\n    region,\n    state,\n    SUM(revenue) AS total_revenue,\n    COUNT(DISTINCT customer_id) AS distinct_customers\nFROM orders\nGROUP BY country, region, state;\n</code></pre> <ul> <li>Metric defined:</li> </ul> <pre><code>revenue_per_customer = total_revenue / distinct_customers\n</code></pre> <p>Now, when analysts query this metric view, they don\u2019t need to rewrite SQL for each grouping. The query engine rewrites it under the hood.</p>"},{"location":"databricks/22-Databricks_Metrics_Views/#example-queries-on-metric-view","title":"Example Queries on Metric View","text":""},{"location":"databricks/22-Databricks_Metrics_Views/#a-group-by-state_1","title":"(a) Group by State","text":"<pre><code>SELECT\n    state,\n    total_revenue / distinct_customers AS revenue_per_customer\nFROM revenue_metrics;\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#b-group-by-region_1","title":"(b) Group by Region","text":"<pre><code>SELECT\n    region,\n    SUM(total_revenue) / SUM(distinct_customers) AS revenue_per_customer\nFROM revenue_metrics\nGROUP BY region;\n</code></pre>"},{"location":"databricks/22-Databricks_Metrics_Views/#c-group-by-country_1","title":"(c) Group by Country","text":"<pre><code>SELECT\n    country,\n    SUM(total_revenue) / SUM(distinct_customers) AS revenue_per_customer\nFROM revenue_metrics\nGROUP BY country;\n</code></pre> <p>\ud83d\udc49 Difference: You don\u2019t redefine the metric \u2014 you just group by a different dimension (state, region, country). Databricks rewrites the query correctly to maintain metric consistency (so that KPIs mean the same thing everywhere).</p> <p>\u2705 In short:</p> <ul> <li>Standard views = you must predefine each grouping (state/region/country).</li> <li>Metric views = define the metric once (<code>revenue_per_customer</code>) and reuse across any dimension.</li> </ul>"},{"location":"databricks/23-Databricks_Streaming_Materialized_Views_SQL/","title":"Streaming and Materialized Views in Databricks SQL","text":"<p>There is no auto option for incremental load while developing streaming tables and views in SQL, hence additional option has to be provided.</p> <pre><code>CREATE OR REFRESH STREAMING TABLE dev.bronze.orders_st\nAS\nSELECT * FROM \nSTREAM read_files(\n  \"/Volumes/dev/bronze/landing/input/\",\n  format =&gt; 'csv',\n  includeExistingFiles =&gt; false\n)\n</code></pre> <pre><code>CREATE OR REPLACE MATERIALIZED VIEW dev.bronze.ordes_mv \n-- SCHEDULE EVERY 4 HOURS\nAS\nSELECT Country,sum(UnitPrice) as agg_total_price FROM dev.bronze.orders_st \ngroup by Country\n</code></pre> <p>Replacing the materialized view does not refresh entire data, just incrementally. The group aggregate is also calculated incrementally.</p> <pre><code>CREATE OR REPLACE MATERIALIZED VIEW dev.bronze.ordes_mv \n-- SCHEDULE EVERY 4 HOURS\nAS\nSELECT Country,sum(UnitPrice) as agg_total_price FROM dev.bronze.orders_st \ngroup by Country\n</code></pre> <p>Every run of the query on Serverless Warehouse spins up DLT job in background.</p> <p></p>"},{"location":"databricks/24-Databricks_CLI_Setup/","title":"\ud83d\udee0 Steps to Setup Databricks CLI on Linux","text":""},{"location":"databricks/24-Databricks_CLI_Setup/#1-install-python-pip","title":"1. Install Python &amp; pip","text":"<p>Databricks CLI is a Python package. Check if you have Python 3 installed:</p> <pre><code>python3 --version\n</code></pre> <p>If not, install it:</p> <pre><code>sudo apt update\nsudo apt install python3 python3-pip -y\n</code></pre>"},{"location":"databricks/24-Databricks_CLI_Setup/#2-install-databricks-cli","title":"2. Install Databricks CLI","text":"<p>Run:</p> <pre><code>pip3 install databricks-cli --upgrade\n</code></pre> <p>Check installation:</p> <pre><code>databricks --version\n</code></pre>"},{"location":"databricks/24-Databricks_CLI_Setup/#3-generate-a-personal-access-token-pat-in-databricks","title":"3. Generate a Personal Access Token (PAT) in Databricks","text":"<ol> <li>Go to your Databricks workspace in the browser.</li> <li>Click on your username (top right) \u2192 User Settings.</li> <li>Under Access Tokens, click Generate New Token.</li> <li>Copy the token (you won\u2019t see it again).</li> </ol>"},{"location":"databricks/24-Databricks_CLI_Setup/#4-configure-databricks-cli","title":"4. Configure Databricks CLI","text":"<p>Run:</p> <pre><code>databricks configure --token\n</code></pre> <p>It will ask:</p> <ul> <li> <p>Databricks Host (URL) \u2192 Example:</p> </li> <li> <p>AWS: <code>https://&lt;workspace-url&gt;.cloud.databricks.com</code></p> </li> <li>Azure: <code>https://&lt;workspace-name&gt;.azuredatabricks.net</code></li> <li>Token \u2192 Paste the token you generated.</li> </ul>"},{"location":"databricks/24-Databricks_CLI_Setup/#5-test-the-cli","title":"5. Test the CLI","text":"<p>Run a test command to list clusters:</p> <pre><code>databricks clusters list\n</code></pre> <p>If successful, you\u2019ll see details of your clusters.</p>"},{"location":"databricks/24-Databricks_CLI_Setup/#6-optional-store-multiple-profiles","title":"6. (Optional) Store Multiple Profiles","text":"<p>You can save multiple workspace logins using profiles in <code>~/.databricks/config</code>. Example:</p> <pre><code>[DEFAULT]\nhost = https://myworkspace.azuredatabricks.net\ntoken = dapi123abc\n\n[staging]\nhost = https://staging-workspace.azuredatabricks.net\ntoken = dapi456def\n</code></pre> <p>Then use:</p> <pre><code>databricks --profile staging clusters list\n</code></pre>"},{"location":"databricks/24-Databricks_CLI_Setup/#7-use-cli-for-common-tasks","title":"7. Use CLI for Common Tasks","text":"<p>Examples:</p> <pre><code># List jobs\ndatabricks jobs list\n\n# Upload a Python file to DBFS\ndatabricks fs cp myscript.py dbfs:/FileStore/scripts/myscript.py\n\n# Run a job\ndatabricks jobs run-now --job-id &lt;job_id&gt;\n</code></pre>"},{"location":"docs-deep-dive/databricks/00-What_Is_Lakehouse/","title":"What is Lakehouse?","text":""},{"location":"docs-deep-dive/databricks/00-What_Is_Lakehouse/#what-is-databricks-lakehouse","title":"What is Databricks Lakehouse?","text":"<p>A data lakehouse is a data management system that combines the benefits of data lakes and data warehouses. This article describes the lakehouse architectural pattern and what you can do with it on Databricks.</p> <p></p> <p>Data lakehouses often use a data design pattern that incrementally improves, enriches, and refines data as it moves through layers of staging and transformation. Each layer of the lakehouse can include one or more layers. This pattern is frequently referred to as a medallion architecture.</p> <p>Databricks is built on Apache Spark. Apache Spark enables a massively scalable engine that runs on compute resources decoupled from storage.</p> <p>The Databricks lakehouse uses two additional key technologies:</p> <p>Delta Lake: an optimized storage layer that supports ACID transactions and schema enforcement. Unity Catalog: a unified, fine-grained governance solution for data and AI.</p> <p>A schema-on-write approach, combined with Delta schema evolution capabilities, means that you can make changes to this layer without necessarily having to rewrite the downstream logic that serves data to your end users.</p>"},{"location":"docs-deep-dive/databricks/00-What_Is_Lakehouse/#capabilities-of-lakehouse","title":"Capabilities of Lakehouse","text":"<p>Real-time data processing: Process streaming data in real-time for immediate analysis and action.</p> <p>Data integration: Unify your data in a single system to enable collaboration and establish a single source of truth for your organization.</p> <p>Schema evolution: Modify data schema over time to adapt to changing business needs without disrupting existing data pipelines.</p> <p>Data transformations: Using Apache Spark and Delta Lake brings speed, scalability, and reliability to your data.</p> <p>Data analysis and reporting: Run complex analytical queries with an engine optimized for data warehousing workloads.</p> <p>Machine learning and AI: Apply advanced analytics techniques to all of your data. Use ML to enrich your data and support other workloads.</p> <p>Data versioning and lineage: Maintain version history for datasets and track lineage to ensure data provenance and traceability.</p> <p>Data governance: Use a single, unified system to control access to your data and perform audits.</p> <p>Data sharing: Facilitate collaboration by allowing the sharing of curated data sets, reports, and insights across teams.</p> <p>Operational analytics: Monitor data quality metrics, model quality metrics, and drift by applying machine learning to lakehouse monitoring data.</p>"},{"location":"docs-deep-dive/databricks/01-Lakehouse_vs_Delta_Lake_vs_Warehouse/","title":"Lakehouse vs Delta Lake vs Warehouse","text":""},{"location":"docs-deep-dive/databricks/01-Lakehouse_vs_Delta_Lake_vs_Warehouse/#lakehouse-vs-data-warehouse-vs-delta-lake","title":"Lakehouse vs Data Warehouse vs Delta Lake","text":"<p>Data warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as a set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. </p> <p>Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks lakehouse and Databricks SQL. For more information, see Data warehousing on Databricks.</p> <p>Powered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.</p> <p>The data lakehouse combines the benefits of data lakes and data warehouses and provides:</p> <ul> <li> <p>Open, direct access to data stored in standard data formats.</p> </li> <li> <p>Indexing protocols optimized for machine learning and data science.</p> </li> <li> <p>Low query latency and high reliability for BI and advanced analytics.</p> </li> <li> <p>By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the Data Lakehouse allows you to work from the same data and in the same platform across different use cases.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/","title":"All things Delta!","text":""},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/#what-is-delta-lake","title":"What is Delta Lake?","text":"<p>Delta Lake is an open-source storage layer that brings reliability to data lakes by adding a transactional storage layer on top of data stored in cloud storage (on AWS S3, Azure Storage, and GCS). It allows for ACID transactions, data versioning, and rollback capabilities. It allows you to handle both batch and streaming data in a unified way.</p> <p>Delta tables are built on top of this storage layer and provide a table abstraction, making it easy to work with large-scale structured data using SQL and the DataFrame API.</p>"},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/#what-are-delta-tables","title":"What are Delta Tables?","text":"<p>Delta table is the default data table format in Databricks and is a feature of the Delta Lake open source data framework. Delta tables are typically used for data lakes, where data is ingested via streaming or in large batches.</p>"},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/#what-are-lakeflow-declarative-pipelines","title":"What are Lakeflow Declarative Pipelines?","text":"<p>Lakeflow Declarative Pipelines manage the flow of data between many Delta tables, thus simplifying the work of data engineers on ETL development and management. The pipeline is the main unit of execution for Lakeflow Declarative Pipelines. Lakeflow Declarative Pipelines offers declarative pipeline development, improved data reliability, and cloud-scale production operations. </p> <p>Users can perform both batch and streaming operations on the same table and the data is immediately available for querying. You define the transformations to perform on your data, and Lakeflow Declarative Pipelines manages task orchestration, cluster management, monitoring, data quality, and error handling. Lakeflow Declarative Pipelines enhanced autoscaling can handle streaming workloads which are spiky and unpredictable.</p>"},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/#delta-tables-vs-dlt","title":"Delta Tables vs DLT","text":"<p>Delta table is a way to store data in tables, whereas Lakeflow Declarative Pipelines allows you to describe how data flows between these tables declaratively. Lakeflow Declarative Pipelines is a declarative framework that manages many delta tables, by creating them and keeping them up to date. In short, Delta tables is a data table architecture while Lakeflow Declarative Pipelines is a data pipeline framework.</p>"},{"location":"docs-deep-dive/databricks/02-All_Delta_Things_Databricks/#delta-lake-transaction-log","title":"Delta Lake Transaction Log","text":"<p>A single source of truth tracking all changes that users make to the table and the mechanism through which Delta Lake guarantees atomicity. </p> <p>The transaction log is key to understanding Delta Lake, because it is the common thread that runs through many of its most important features:</p> <ul> <li>ACID transactions</li> <li>Scalable metadata handling</li> <li>Time travel</li> </ul> <p>Check link for more : Github Delta Transaction Protocol</p>"},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/","title":"High Level Databricks Architecture","text":""},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#high-level-architecture","title":"High Level Architecture","text":"<p>A Databricks account is the top-level construct that you use to manage Databricks across your organization. At the account level, you manage:</p> <ul> <li> <p>Identity and access: Users, groups, service principals, SCIM provisioning, and SSO configuration. Workspace management: Create, update, and delete workspaces across multiple regions.</p> </li> <li> <p>Unity Catalog metastore management: Create and attach metastore to workspaces.</p> </li> <li> <p>Usage management: Billing, compliance, and policies.</p> </li> </ul> <p>An account can contain multiple workspaces and Unity Catalog metastores.</p> <p>Workspaces are the collaboration environment where users run compute workloads such as ingestion, interactive exploration, scheduled jobs, and ML training.</p> <p>Unity Catalog metastores are the central governance system for data assets such as tables and ML models. You organize data in a metastore under a three-level namespace:</p> <p>.. <p>Metastores are attached to workspaces. You can link a single metastore to multiple Databricks workspaces in the same region, giving each workspace the same data view. Data access controls can be managed across all linked workspaces.</p> <p></p>"},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#workspace-architecture","title":"Workspace Architecture","text":"<p>Databricks operates out of a control plane and a compute plane.</p> <p>The control plane includes the backend services that Databricks manages in your Databricks account. The web application is in the control plane.</p> <p>The compute plane is where your data is processed. There are two types of compute planes depending on the compute that you are using.</p> <ul> <li> <p>For serverless compute, the serverless compute resources run in a serverless compute plane in your Databricks account.</p> </li> <li> <p>For classic Databricks compute, the compute resources are in your AWS account in what is called the classic compute plane. This refers to the network in your AWS account and its resources.</p> </li> </ul> <p>Each Databricks workspace has an associated storage bucket known as the workspace storage bucket. The workspace storage bucket is in your cloud account.</p> <p></p>"},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#serverless-compute","title":"Serverless Compute","text":"<p>In the serverless compute plane, Databricks compute resources run in a compute layer within your Databricks account. Databricks creates a serverless compute plane in the same cloud region as your workspace's classic compute plane. You select this region when creating a workspace.</p> <p>To protect customer data within the serverless compute plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different Databricks customer workspaces and additional network controls between clusters of the same customer.</p> <p>In the classic compute plane, Databricks compute resources run in your cloud account. New compute resources are created within each workspace's virtual network in the customer's cloud account.</p> <p>A classic compute plane has natural isolation because it runs in each customer's own cloud account. </p>"},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#workspace-storage","title":"Workspace Storage","text":""},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#serverless-workspaces","title":"Serverless Workspaces","text":"<p>Serverless workspaces use default storage, which is a fully managed storage location for your workspace's system data and Unity Catalog catalogs. Serverless workspaces also support the ability to connect to your cloud storage locations. </p>"},{"location":"docs-deep-dive/databricks/03-High_Level_Architecture/#traditional-workspaces","title":"Traditional Workspaces","text":"<p>Workspace system data: Workspace system data is generated as you use various Databricks features such as creating notebooks. This bucket includes notebook revisions, job run details, command results, and Spark logs Unity Catalog workspace catalog: If your workspace was enabled for Unity Catalog automatically, the workspace storage bucket contains the default workspace catalog. All users in your workspace can create assets in the default schema in this catalog.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/","title":"Databricks ACID Guarantees","text":""},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#acid-guarantees-in-databricks","title":"ACID Guarantees in Databricks","text":"<p>Databricks uses Delta Lake by default for all reads and writes and builds upon the ACID guarantees provided by the open source Delta Lake protocol. ACID stands for atomicity, consistency, isolation, and durability.</p> <ul> <li> <p>Atomicity means that all transactions either succeed or fail completely.</p> </li> <li> <p>Consistency guarantees relate to how a given state of the data is observed by simultaneous operations.</p> </li> <li> <p>Isolation refers to how simultaneous operations potentially conflict with one another.</p> </li> <li> <p>Durability means that committed changes are permanent.</p> </li> </ul> <p>While many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on Databricks might differ from other systems you've worked with.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#how-are-transactions-scoped-on-databricks","title":"How are Transactions scoped on Databricks","text":"<p>Databricks manages transactions at the table level, meaning each transaction only affects one table at a time.</p> <p>It uses optimistic concurrency control, which means:</p> <ul> <li>There are no locks on reads or writes.</li> <li>Multiple users can read or write at the same time without blocking each other.</li> <li>Because locks are not used, deadlocks cannot occur.</li> </ul>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#isolation-levels","title":"Isolation Levels","text":"<ul> <li> <p>Snapshot isolation applies to reads.   This means each read operation sees a consistent snapshot of the table as it was when the read began \u2014 even if other users are modifying the table at the same time.</p> </li> <li> <p>Write-serializable isolation applies to writes.   This is stronger than snapshot isolation because Databricks ensures that concurrent write operations produce a consistent final state, as if they happened one after another in sequence.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#multi-table-reads","title":"Multi-table Reads","text":"<p>When a query reads from multiple tables, Databricks:</p> <ul> <li>Reads the current version of each table at the moment it is accessed.</li> <li>Does not block other transactions that are writing to those tables.</li> </ul>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#multi-table-transactions","title":"Multi-table Transactions","text":"<p>Databricks does not support <code>BEGIN</code> and <code>END</code> statements to group multiple operations into a single transaction across tables. If an application needs to modify multiple tables, it must commit changes to each table separately \u2014 one after another.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#combining-multiple-operations","title":"Combining Multiple Operations","text":"<p>Within a single table, you can combine inserts, updates, and deletes into one atomic write operation by using the <code>MERGE INTO</code> statement. This ensures that all those changes happen together as a single transaction.</p> <p>Sure. Here is a clear explanation without emojis.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#scenario","title":"Scenario","text":"<p>Suppose you have two Delta tables:</p> <ul> <li><code>orders</code></li> <li><code>customers</code></li> </ul> <p>You want to:</p> <ol> <li>Insert a new order into <code>orders</code></li> <li>Update the customer\u2019s last order date in <code>customers</code></li> </ol> <p>You want both changes to behave like one transaction.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#key-point","title":"Key Point","text":"<p>Databricks does not support true multi-table transactions. Each operation on a table is committed independently. If one operation fails, Databricks will not automatically roll back the others.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#example-sequential-multi-table-operations","title":"Example: Sequential Multi-table Operations","text":"<pre><code>-- Step 1: Insert a new order\nINSERT INTO orders (order_id, customer_id, amount, order_date)\nVALUES (101, 1, 250.00, current_timestamp());\n\n-- Step 2: Update the customer\u2019s last order date\nUPDATE customers\nSET last_order_date = current_timestamp()\nWHERE customer_id = 1;\n</code></pre> <p>Each of these statements is its own transaction. If the first succeeds but the second fails, the first will still be committed.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#handling-multi-table-consistency","title":"Handling Multi-table Consistency","text":""},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#option-1-application-level-control","title":"Option 1: Application-level Control","text":"<p>You can manage multi-table logic at the application or notebook level. For example, in Python:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ntry:\n    # Table 1: Insert new order\n    spark.sql(\"\"\"\n        INSERT INTO orders (order_id, customer_id, amount, order_date)\n        VALUES (101, 1, 250.00, current_timestamp())\n    \"\"\")\n\n    # Table 2: Update customer info\n    spark.sql(\"\"\"\n        UPDATE customers\n        SET last_order_date = current_timestamp()\n        WHERE customer_id = 1\n    \"\"\")\n\nexcept Exception as e:\n    print(\"Transaction failed:\", e)\n    # Optionally run manual rollback logic, such as deleting the new order\n</code></pre> <p>This way, you can control what happens if one statement fails. For instance, if the update fails, you could delete the order that was just inserted.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#option-2-using-a-staging-table","title":"Option 2: Using a Staging Table","text":"<p>You can use a staging table to stage changes before applying them to multiple tables.</p> <pre><code>-- Stage combined changes\nCREATE OR REPLACE TABLE staging_orders AS\nSELECT 101 AS order_id, 1 AS customer_id, 250.00 AS amount, current_timestamp() AS order_date;\n\n-- Merge into the orders table\nMERGE INTO orders AS o\nUSING staging_orders AS s\nON o.order_id = s.order_id\nWHEN NOT MATCHED THEN INSERT *;\n\n-- Merge into the customers table\nMERGE INTO customers AS c\nUSING staging_orders AS s\nON c.customer_id = s.customer_id\nWHEN MATCHED THEN UPDATE SET last_order_date = s.order_date;\n</code></pre> <p>This approach helps you track progress and retry failed steps, but it still does not provide atomicity across both tables.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#summary","title":"Summary","text":"Concept Databricks Behavior Atomic transactions across multiple tables Not supported Atomic operations on a single table Supported Sequential multi-table updates Supported, but managed by the application Rollback for multi-table transactions Must be handled manually MERGE INTO for combined operations Supported per table"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#how-does-databricks-implement-atomicity","title":"How does Databricks Implement Atomicity?","text":"<p>The transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.</p> <p>Data files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The VACUUM operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#how-does-databricks-implement-durability","title":"How does Databricks Implement Durability?","text":"<p>Databricks uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on Databricks inherit the durability guarantees of the cloud object storage on which they're stored.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#how-does-databricks-ensure-consistency","title":"How does Databricks ensure consistency?","text":"<p>Delta Lake uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:</p> <ul> <li>Read: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).<ul> <li>Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.</li> </ul> </li> <li>Write: Writes data files to the directory used to define the table.</li> <li>Validate and commit: Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.</li> </ul> <p>If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.</p> <p>If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.</p>"},{"location":"docs-deep-dive/databricks/04-Databricks_ACID_Guarantees/#how-does-databricks-implement-isolation","title":"How does Databricks Implement Isolation?","text":"<p>Databricks uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.</p> <p>Write serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.</p>"},{"location":"docs-deep-dive/databricks/05-Databricks_Medallion_Architecture/","title":"Medallion Design Pattern","text":""},{"location":"docs-deep-dive/databricks/05-Databricks_Medallion_Architecture/#databricks-medallion-architecture","title":"Databricks Medallion Architecture","text":"<p>The medallion architecture describes a series of data layers that denote the quality of data stored in the lakehouse. Databricks recommends taking a multi-layered approach to building a single source of truth for enterprise data products.</p> <p>This architecture guarantees atomicity, consistency, isolation, and durability as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics. The terms bronze (raw), silver (validated), and gold (enriched) describe the quality of the data in each of these layers.</p>"},{"location":"docs-deep-dive/databricks/05-Databricks_Medallion_Architecture/#multi-hop-design-pattern-of-medallion-architecture","title":"Multi Hop Design Pattern of Medallion Architecture","text":"<p>A medallion architecture is a data design pattern used to organize data logically. Its goal is to incrementally and progressively improve the structure and quality of data as it flows through each layer of the architecture (from Bronze \u21d2 Silver \u21d2 Gold layer tables). Medallion architectures are sometimes also referred to as multi-hop architectures.</p> <p>By progressing data through these layers, organizations can incrementally improve data quality and reliability, making it more suitable for business intelligence and machine learning applications.</p> <p></p> <p>Example Medallion Architecture</p> <p></p>"},{"location":"docs-deep-dive/databricks/05-Databricks_Medallion_Architecture/#silver-layer-good-practices","title":"Silver Layer Good Practices","text":"<ul> <li> <p>Should always include at least one validated, non-aggregated representation of each record. If aggregate representations drive many downstream workloads, those representations might be in the silver layer, but typically they are in the gold layer.</p> </li> <li> <p>Is where you perform data cleansing, deduplication, and normalization.</p> </li> <li> <p>Enhances data quality by correcting errors and inconsistencies.</p> </li> <li> <p>Structures data into a more consumable format for downstream processing.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/05-Databricks_Medallion_Architecture/#enforcing-data-quality-at-silver-layer","title":"Enforcing Data Quality at Silver Layer","text":"<ul> <li>Schema enforcement</li> <li>Handling of null and missing values</li> <li>Data deduplication</li> <li>Resolution of out-of-order and late-arriving data issues</li> <li>Data quality checks and enforcement</li> <li>Schema evolution</li> <li>Type casting</li> <li>Joins</li> </ul>"},{"location":"docs-deep-dive/databricks/06-Databricks_Single_Source_Of_Truth_Arch/","title":"Databricks Single Source of Truth Architecture","text":""},{"location":"docs-deep-dive/databricks/06-Databricks_Single_Source_Of_Truth_Arch/#single-source-of-truth-architecture-in-databricks","title":"Single Source of Truth Architecture in Databricks","text":"<p>Delta Lake transactions use log files stored alongside data files to provide ACID guarantees at a table level. Because the data and log files backing Delta Lake tables live together in cloud object storage, reading and writing data can occur simultaneously without risk of many queries resulting in performance degradation or deadlock for business-critical workloads. </p> <p>This means that users and applications throughout the enterprise environment can connect to the same single copy of the data to drive diverse workloads, with all viewers guaranteed to receive the most current version of the data at the time their query executes.</p>"},{"location":"docs-deep-dive/databricks/06-Databricks_Single_Source_Of_Truth_Arch/#views-in-databricks","title":"Views in Databricks","text":"<p>In Databricks, views are saved queries that reference data stored in tables within the lakehouse. They do not store data themselves; instead, they define how to access and present data from existing tables.</p> <p>When you create a table, the query that produces it runs once at write time, and the results are stored physically. When you create a view, the defining query does not store results. Instead, the query is executed every time someone reads from the view.</p> <p>Because of this behavior:</p> <ul> <li>Views always show up-to-date data, reflecting the latest state of the underlying tables.</li> <li>Compute resources are used only when someone queries the view, not when it is created or updated.</li> </ul> <p>Databricks allows you to use Unity Catalog to manage and secure views just like tables. This means:</p> <ul> <li>You can control access to views through Unity Catalog permissions.</li> <li>You can share views across teams and departments.</li> <li>Teams can reuse shared views to ensure consistent logic and metrics for business reporting and analytics.</li> </ul> <p>In summary, views in Databricks are a way to define reusable, secure, and always-current queries without physically storing new data.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/","title":"Databricks Scope of Lakehouse Architecture","text":""},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#databricks-scope-of-lakehouse-architecture","title":"Databricks : Scope of Lakehouse Architecture","text":""},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#user-personas","title":"User Personas","text":"<p>Data engineers provide data scientists and business analysts with accurate and reproducible data for timely decision-making and real-time insights. They implement highly consistent and reliable ETL processes to increase user confidence and trust in data. They ensure that data is well integrated with the various pillars of the business and typically follow software engineering best practices.</p> <p>Data scientists blend analytical expertise and business understanding to transform data into strategic insights and predictive models. They are adept at translating business challenges into data-driven solutions, be that through retrospective analytical insights or forward-looking predictive modeling. Leveraging data modeling and machine learning techniques, they design, develop, and deploy models that unveil patterns, trends, and forecasts from data. They act as a bridge, converting complex data narratives into comprehensible stories, ensuring business stakeholders not only understand but can also act upon the data-driven recommendations, in turn driving a data-centric approach to problem-solving within an organization.</p> <p>ML engineers (machine learning engineers) lead the practical application of data science in products and solutions by building, deploying, and maintaining machine learning models. Their primary focus pivots towards the engineering aspect of model development and deployment. ML Engineers ensure the robustness, reliability, and scalability of machine learning systems in live environments, addressing challenges related to data quality, infrastructure, and performance. By integrating AI and ML models into operational business processes and user-facing products, they facilitate the utilization of data science in solving business challenges, ensuring models don't just stay in research but drive tangible business value.</p> <p>Business analysts and business users: Business analysts provide stakeholders and business teams with actionable data. They often interpret data and create reports or other documentation for management using standard BI tools. They are typically the first point of contact for non-technical business users and operations colleagues for quick analysis questions. </p> <p>Dashboards and business apps delivered on the Databricks platform can be used directly by business users. Apps Developer create secure data and AI applications on the data platform and share those apps with business users. Business partners are important stakeholders in an increasingly networked business world. They are defined as a company or individuals with whom a business has a formal relationship to achieve a common goal, and can include vendors, suppliers, distributors, and other third-party partners. Data sharing is an important aspect of business partnerships, as it enables the transfer and exchange of data to enhance collaboration and data-driven decision-making.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#domains-of-platform-framework","title":"Domains of Platform Framework","text":"<p>Storage: In the cloud, data is mainly stored in scalable, efficient, and resilient object storage on cloud providers.</p> <p>Governance: Capabilities around data governance, such as access control, auditing, metadata management, lineage tracking, and monitoring for all data and AI assets.</p> <p>AI engine: The AI engine provides generative AI capabilities for the whole platform.</p> <p>Ingest and transform: The capabilities for ETL workloads.</p> <p>Advanced analytics, ML, and AI: All capabilities around machine learning, AI, Generative AI, and also streaming analytics.</p> <p>Data warehouse: The domain supporting DWH and BI use cases.</p> <p>Operational Database: Capabilities and services around operational databases like OLTP databases (online transaction processing), key-value stores, etc.</p> <p>Automation: Workflow management for data processing, machine learning, analytics pipelines, including CI/CD and MLOps support.</p> <p>ETL and Data science tools: The front-end tools that data engineers, data scientists and ML engineers primarily use for work.</p> <p>BI tools: The front-end tools that BI analysts primarily use for work.</p> <p>Data and AI apps Tools that build and host applications that use the data managed by the underlying platform and leverage its analytics and AI capabilities in a secure and governance-compliant manner.</p> <p>Collaboration: Capabilities for data sharing between two or more parties.</p> <p></p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#data-workloads-in-databricks","title":"Data Workloads in Databricks","text":""},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#ingest-and-transform","title":"Ingest and Transform","text":"<p>Databricks Lakeflow Connect offers built-in connectors for ingestion from enterprise applications and databases. The resulting ingestion pipeline is governed by Unity Catalog and is powered by serverless compute and Lakeflow Declarative Pipelines.</p> <p>Auto Loader incrementally and automatically processes files landing in cloud storage in scheduled or continuous jobs - without the need to manage state information. Once ingested, raw data needs to be transformed so it's ready for BI and ML/AI. Databricks provides powerful ETL capabilities for data engineers, data scientists, and analysts.</p> <p>Lakeflow Declarative Pipelines allows writing ETL jobs in a declarative way, simplifying the entire implementation process. Data quality can be improved by defining data expectations.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#data-warehouse","title":"Data Warehouse","text":"<p>The Databricks Data Intelligence Platform also has a complete data warehouse solution with Databricks SQL, centrally governed by Unity Catalog with fine-grained access control.</p> <p>AI functions are built-in SQL functions that allow you to apply AI on your data directly from SQL. Integrating AI into analysis jobs provides access to information previously inaccessible to analysts, and empowers them to make more informed decisions, manage risks, and sustain a competitive advantage through data-driven innovation and efficiency.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#operational-database","title":"Operational Database","text":"<p>Lakebase is an online transaction processing (OLTP) database based on Postgres and fully integrated with the Databricks Data Intelligence Platform. It allows you to create an OLTP database on Databricks, and integrate OLTP workloads with your Lakehouse. </p> <p>Lakebase allows to sync data between OLTP and online analytical processing (OLAP) workloads, and is well integrated with Feature management, SQL warehouses, and Databricks Apps.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#outline-of-databricks-feature-areas","title":"Outline of Databricks Feature Areas","text":""},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#cloud-storage","title":"Cloud Storage","text":"<p>All data for the lakehouse is stored in the cloud provider's object storage. Databricks supports three cloud providers: AWS, Azure, and GCP. Files in various structured and semi-structured formats (for example, Parquet, CSV, JSON, and Avro), as well as unstructured formats (such as images and documents), are ingested and transformed using either batch or streaming processes.</p> <p>Delta Lake is the recommended data format for the lakehouse (file transactions, reliability, consistency, updates, and so on). It's also possible to read Delta tables using Apache Iceberg clients.</p> <p>No proprietary data formats are used in the Databricks Data Intelligence Platform: Delta Lake and Iceberg are open source to avoid vendor lock-in.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#data-and-ai-governance","title":"Data and AI Governance","text":"<p>On top of the storage layer, Unity Catalog offers a wide range of data and AI governance capabilities, including metadata management in the metastore, access control, auditing, data discovery, and data lineage.</p> <p>Lakehouse monitoring provides out-of-the-box quality metrics for data and AI assets, and auto-generated dashboards to visualize these metrics.</p> <p>External SQL sources can be integrated into the lakehouse and Unity Catalog through lakehouse federation.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#orchestration","title":"Orchestration","text":"<p>Lakeflow Jobs enable you to run diverse workloads for the full data and AI lifecycle on any cloud. They allow you to orchestrate jobs as well as Lakeflow Declarative Pipelines for SQL, Spark, notebooks, DBT, ML models, and more.</p>"},{"location":"docs-deep-dive/databricks/07-Databricks_Scope_Of_Lakehouse_Arch/#collaboration","title":"Collaboration","text":"<p>Delta Sharing is an open protocol developed by Databricks for secure data sharing with other organizations regardless of the computing platforms they use.</p> <p>Databricks Marketplace is an open forum for exchanging data products. It takes advantage of Delta Sharing to give data providers the tools to  share data products securely and data consumers the power to explore and expand their access to the data and data services they need.</p> <p>Clean Rooms use Delta Sharing and serverless compute to provide a secure and privacy-protecting environment where multiple parties can work together on sensitive enterprise data without direct access to each other's data.</p>"},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/","title":"Databricks Architectural Guiding Principles","text":""},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/#databricks-guiding-priciples-for-lakehouse","title":"Databricks Guiding Priciples for Lakehouse","text":""},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/#curate-data-and-offer-trusted-products","title":"Curate Data and Offer Trusted Products","text":"<p>Curating data by establishing a layered (or multi-hop) architecture is a critical best practice for the lakehouse, as it allows data teams to structure the data according to quality levels and define roles and responsibilities per layer. A common layering approach is:</p> <ul> <li> <p>Ingest layer: Source data gets ingested into the lakehouse into the first layer and should be persisted there. When all downstream data is created from the ingest layer, rebuilding the subsequent layers from this layer is possible, if needed.</p> </li> <li> <p>Curated layer: The purpose of the second layer is to hold cleansed, refined, filtered and aggregated data. The goal of this layer is to provide a sound, reliable foundation for analyses and reports across all roles and functions.</p> </li> <li> <p>Final layer: The third layer is created around business or project needs; it provides a different view as data products to other business units or projects, preparing data around security needs (for example, anonymized data), or optimizing for performance (with pre-aggregated views). </p> </li> </ul> <p>The data products in this layer are seen as the truth for the business.</p> <p>Pipelines across all layers need to ensure that data quality constraints are met, meaning that data is accurate, complete, accessible, and consistent at all times, even during concurrent reads and writes. </p> <p>The validation of new data happens at the time of data entry into the curated layer, and the following ETL steps work to improve the quality of this data. </p> <p>Data quality must improve as data progresses through the layers and, as such, the trust in the data subsequently increases from a business point of view.</p>"},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/#democratizing-data-value-through-self-service","title":"Democratizing Data Value through Self Service","text":"<p>The best data lake cannot provide sufficient value, if users cannot access the platform or data for their BI and ML/AI tasks easily. Lower the barriers to accessing data and platforms for all business units. Consider lean data management processes and provide self-service access for the platform and the underlying data.</p> <p></p> <p>Businesses that have successfully moved to a data-driven culture will thrive. This means every business unit derives its decisions from analytical models or from analyzing its own or centrally provided data. For consumers, data has to be easily discoverable and securely accessible.</p> <p>A good concept for data producers is \u201cdata as a product\u201d: The data is offered and maintained by one business unit or business partner like a product and consumed by other parties with proper permission control. Instead of relying on a central team and potentially slow request processes, these data products must be created, offered, discovered, and consumed in a self-service experience.</p> <p>However, it's not just the data that matters. The democratization of data requires the right tools to enable everyone to produce or consume and understand the data. For this, you need the data lakehouse to be a modern data and AI platform that provides the infrastructure and tooling for building data products without duplicating the effort of setting up another tool stack.</p>"},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/#adopt-an-organization-wide-data-and-ai-governance-strategy","title":"Adopt an Organization Wide Data and AI Governance Strategy","text":"<p>Data governance is a broad topic. The lakehouse covers the following dimensions:</p> <ul> <li>Data quality</li> </ul> <p>The most important prerequisite for correct and meaningful reports, analysis results, and models is high-quality data. Quality assurance (QA) needs to exist around all pipeline steps. Examples of how to implement this include having data contracts, meeting SLAs, keeping schemas stable, and evolving them in a controlled way.</p> <ul> <li>Data catalog</li> </ul> <p>Another important aspect is data discovery: Users of all business areas, especially in a self-service model, must be able to discover relevant data easily. Therefore, a lakehouse needs a data catalog that covers all business-relevant data. The primary goals of a data catalog are as follows:</p> <ul> <li> <p>Ensure the same business concept is uniformly called and declared across the business. You might think of it as a semantic model in the curated and the final layer.</p> </li> <li> <p>Track the data lineage precisely so that users can explain how these data arrived at their current shape and form.</p> </li> <li> <p>Maintain high-quality metadata, which is as important as the data itself for proper use of the data.</p> </li> <li> <p>Access Control</p> </li> </ul> <p>As the value creation from the data in the lakehouse happens across all business areas, the lakehouse must be built with security as a first-class citizen. Companies might have a more open data access policy or strictly follow the principle of least privileges. </p> <p>Independent of that, data access controls must be in place in every layer. It is important to implement fine-grade permission schemes from the very beginning (column- and row-level access control, role-based or attribute-based access control). </p> <p>Companies can start with less strict rules. But as the lakehouse platform grows, all mechanisms and processes for a more sophisticated security regime should already be in place. Additionally, all access to the data in the lakehouse must be governed by audit logs from the get-go.</p>"},{"location":"docs-deep-dive/databricks/08-Databricks_Architecture_Guiding_Principles/#build-to-scale-and-optimize-for-costs-and-performance","title":"Build to Scale and Optimize for Costs and Performance","text":"<p>Standard ETL processes, business reports, and dashboards often have a predictable resource need from a memory and computation perspective. However, new projects, seasonal tasks, or modern approaches like model training (churn, forecast, maintenance) generate peaks of resource need. </p> <p>To enable a business to perform all these workloads, a scalable platform for memory and computation is necessary. New resources must be added easily on demand, and only the actual consumption should generate costs. As soon as the peak is over, resources can be freed up again and costs reduced accordingly. Often, this is referred to as horizontal scaling (fewer or more nodes) and vertical scaling (larger or smaller nodes).</p> <p>Scaling also enables businesses to improve the performance of queries by selecting nodes with more resources or clusters with more nodes. But instead of permanently providing large machines and clusters they can be provisioned on demand only for the time needed to optimize the overall performance to cost ratio. </p> <p>Another aspect of optimization is storage versus compute resources. Since there is no clear relation between the volume of the data and workloads using this data (for example, only using parts of the data or doing intensive calculations on small data), it is a good practice to settle on an infrastructure platform that decouples storage and compute resources.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/","title":"09 Databricks Objects Catalogs","text":""},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#catalogs-in-databricks","title":"Catalogs in Databricks","text":"<p>A catalog is the primary unit of data organization in the Databricks Unity Catalog data governance model. This article gives an overview of catalogs in Unity Catalog and how best to use them.</p> <p>Catalogs are the first layer in Unity Catalog's three-level namespace (catalog.schema.table-etc). They contain schemas, which in turn can contain tables, views, volumes, models, and functions. Catalogs are registered in a Unity Catalog metastore in your Databricks account</p> <p></p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#how-should-i-organize-my-data-in-catalogs","title":"How Should I organize my data in catalogs?","text":"<p>When you design your data governance model, you should give careful thought to the catalogs that you create. As the highest level in your organization's data governance model, each catalog should represent a logical unit of data isolation and a logical category of data access, allowing an efficient hierarchy of grants to flow down to schemas and the data objects that they contain. Catalogs therefore often mirror organizational units or software development lifecycle scopes. You might choose, for example, to have a catalog for production data and a catalog for development data, or a catalog for non-customer data and one for sensitive customer data.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#data-isolation-in-catalogs","title":"Data Isolation in Catalogs","text":"<p>Each catalog typically has its own managed storage location to store managed tables and volumes, providing physical data isolation at the catalog level. You can also choose to store data at the metastore level, providing a default storage location for catalogs that don't have a managed storage location of their own. You can add storage at the schema level for more granular data isolation.</p> <p>Because your Databricks account has one metastore per region, catalogs are inherently isolated by region.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#catalog-lvel-privileges","title":"Catalog Lvel Privileges","text":"<p>Because grants on any Unity Catalog object are inherited by children of that object, owning a catalog or having broad privileges on a catalog is very powerful. For example, catalog owners have all privileges on the catalog and the objects in the catalog, and they can grant access to any object in the catalog. Users with SELECT on a catalog can read any table in the catalog. Users with CREATE TABLE on a catalog can create a table in any schema in the catalog.</p> <p>To enforce the principle of least privilege, where users have the minimum access they need to perform their required tasks, typically you grant access only to the specific objects or level in the hierarchy that the user requires. But catalog-level privileges let the catalog owner manage what lower-level object owners can grant. Even if a user is granted access to a low-level data object like a table, for example, that user cannot access that table unless they also have the USE CATALOG privilege on the catalog that contains the table.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#catalog-types","title":"Catalog Types","text":"<p>Standard catalog: the typical catalog, used as the primary unit to organize your data objects in Unity Catalog. This is the catalog type that is discussed in this article.</p> <p>Foreign catalog: a Unity Catalog object that is used only in Lakehouse Federation scenarios. A foreign catalog mirrors a database in an external data system, enabling you to perform read-only queries on that data system in your Databricks workspace.</p> <p><code>hive_metastore</code> catalog: This is the repository of all data managed by the legacy Hive metastore in Databricks workspaces. When an existing non-Unity Catalog workspace is converted to Unity Catalog, all objects that are registered in the legacy Hive metastore are surfaced in Unity Catalog in the hive_metastore catalog. </p> <p><code>Workspace catalog</code>: In all new workspaces, this catalog is created for you by default. Typically, it shares its name with your workspace name. If this catalog exists, all users in your workspace (and only your workspace) have access to it by default, which makes it a convenient place for users to try out the process of creating and accessing data objects in Unity Catalog.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#workspace-catalog-binding","title":"Workspace Catalog Binding","text":"<p>If you use workspaces to isolate user data access, you might want to use workspace-catalog bindings. Workspace-catalog bindings enable you to limit catalog access by workspace boundaries. For example, you can ensure that workspace admins and users can only access production data in prod_catalog from a production workspace environment, prod_workspace. Catalogs are shared with all workspaces attached to the current metastore unless you specify a binding.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#creating-a-catalog-requirements","title":"Creating A Catalog - Requirements","text":"<p>To create a catalog, regardless of catalog type:</p> <ul> <li> <p>You must be a Databricks metastore admin or have the <code>CREATE CATALOG</code> privilege on the metastore.</p> </li> <li> <p>The compute resource that you use to run a notebook to create a catalog must be on Databricks Runtime 11.3 or above and must use a Unity Catalog-compliant access mode. See Access modes. SQL warehouses always support Unity Catalog.</p> </li> </ul> <p>To create a shared catalog:</p> <ul> <li> <p>The Delta Sharing share must already exist in your workspace.</p> </li> <li> <p>You must be a metastore admin, have the <code>USE PROVIDER</code> privilege on the metastore, or own the provider object that includes the share.</p> </li> </ul> <p>To create a standard catalog:</p> <ul> <li> <p>If you specify a managed storage location for the catalog, you must have the CREATE MANAGED STORAGE privilege on the target external location.</p> </li> <li> <p>If no metastore-level managed storage exists, then you must specify a managed storage location for the catalog.</p> </li> </ul> <p>To create a foreign catalog:</p> <p>You must be either the owner of the connection that you use to create the foreign catalog or have the CREATE FOREIGN CATALOG privilege on the connection.</p> <p>You must use compute on Databricks Runtime 13.1 or above. SQL warehouses must be Pro or Serverless.</p> <p>By default, the catalog is shared with all workspaces attached to the current metastore. If the catalog will contain data that should be restricted to specific workspaces, clear the All workspace have access option and use the Assign to workspaces button to add those workspaces. The current workspace must be included.</p> <p>After you assign a workspace, you can optionally change its default Read &amp; Write access level to Read Only: select the workspace from the list and click the Manage Access Level button.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#managing-the-default-catalog","title":"Managing the Default Catalog","text":"<p>A default catalog is configured for each workspace that is enabled for Unity Catalog. The default catalog lets you perform data operations without specifying a catalog. If you omit the top-level catalog name when you perform data operations, the default catalog is assumed.</p> <p>A workspace admin can view or switch the default catalog using the Admin Settings UI. You can also set the default catalog for a cluster using a Spark config.</p> <p>The workspace default catalog setting applies only when using compute that meets the compute requirements for Unity Catalog. Specifically, this means that you're using either a SQL warehouse, or a cluster configured with standard or dedicated access mode. Compute resources that aren't compatible with Unity Catalog use hive_metastore as the default catalog.</p> <p>Commands that do not specify the catalog (for example GRANT CREATE TABLE ON SCHEMA myschema TO mygroup) are evaluated for the catalog in the following order:</p> <ul> <li>Is the catalog set for the session using a <code>USE CATALOG</code> statement or a JDBC setting?</li> <li>Is the Spark configuration <code>spark.databricks.sql.initial.catalog.namespace</code> set on the cluster?</li> <li>Is there a workspace default catalog set for the cluster?</li> </ul> <p>The pipeline configuration for Lakeflow Declarative Pipelines sets a default catalog that overrides workspace default.</p>"},{"location":"docs-deep-dive/databricks/09-Databricks_Objects_Catalogs/#default-catalog-when-uc-is-enabled","title":"Default Catalog when UC is enabled","text":"<p>The default catalog that was initially configured for your workspace depends on how your workspace was enabled for Unity Catalog:</p> <ul> <li> <p>For some workspaces that were enabled for Unity Catalog automatically, the workspace catalog was set as the default catalog. </p> </li> <li> <p>For all other workspaces, the hive_metastore catalog was set as the default catalog.</p> </li> </ul> <p>When you are migrating from the Hive metastore to Unity Catalog, you can set the default catalog to hive_metastore to avoid impacting existing code that references the Hive metastore.</p>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/","title":"Volumes in Databricks","text":"<p>Volumes are Unity Catalog objects that enable governance over non-tabular datasets. Volumes represent a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files.</p> <p>While tables govern tabular data, volumes govern non-tabular data of any format, including structured, semi-structured, or unstructured.</p> <p>Databricks recommends using volumes to govern access to all non-tabular data. Volumes are available in two types:</p> <ul> <li> <p>Managed volumes: For simple Databricks-managed storage.</p> </li> <li> <p>External volumes: For adding governance to existing cloud object storage locations.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#uses-cases-for-volumes","title":"Uses cases for volumes","text":"<p>Use cases for volumes include:</p> <ul> <li> <p>Register landing areas for raw data produced by external systems to support its processing in the early stages of ETL pipelines and other data engineering activities.</p> </li> <li> <p>Register staging locations for ingestion. For example, using Auto Loader, COPY INTO, or CTAS (CREATE TABLE AS) statements.</p> </li> <li> <p>Provide file storage locations for data scientists, data analysts, and machine learning engineers to use as parts of their exploratory data analysis and other data science tasks.</p> </li> <li> <p>Give Databricks users access to arbitrary files produced and deposited in cloud storage by other systems. For example, large collections of unstructured data (such as image, audio, video, and PDF files) captured by surveillance systems or IoT devices, or library files (JARs and Python wheel files) exported from local dependency management systems or CI/CD pipelines.</p> </li> <li> <p>Store operational data, such as logging or checkpointing files.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#managed-vs-external-volumes","title":"Managed Vs External Volumes","text":""},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#why-use-managed-volumes","title":"Why use managed volumes?","text":"<ul> <li> <p>Managed volumes have the following benefits:</p> </li> <li> <p>Default choice for Databricks workloads.</p> </li> <li> <p>No need to manage cloud credentials or storage paths manually.</p> </li> <li> <p>Simplest option for creating governed storage locations quickly.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#why-use-use-external-volumes","title":"Why use use external volumes?","text":"<p>External volumes allow you to add Unity Catalog data governance to existing cloud object storage directories. Some use cases for external volumes include the following:</p> <ul> <li>Adding governance where data already resides, without requiring data copy.</li> <li>Governing files produced by other systems that must be ingested or accessed by Databricks.</li> <li>Governing data produced by Databricks that must be accessed directly from cloud object storage by other systems.</li> <li>Databricks recommends using external volumes to store non-tabular data files that are read or written by external systems in addition to Databricks. </li> <li>Unity Catalog doesn't govern reads and writes performed directly against cloud object storage from external systems, so you must configure additional policies and credentials in your cloud account so that data governance policies are respected outside Databricks.</li> </ul>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#path-for-accessing-volumes","title":"Path for Accessing Volumes","text":"<p><code>/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path&gt;/&lt;file-name&gt;</code></p> <p>You can't register files in volumes as tables in Unity Catalog. Volumes are intended for path-based data access only. Use tables when you want to work with tabular data in Unity Catalog.</p>"},{"location":"docs-deep-dive/databricks/10-Databricks_Objects_Volumes_Tables/#tables-in-databricks","title":"Tables in Databricks","text":""},{"location":"docs-deep-dive/databricks/11-Databricks_Views/","title":"Views in Databricks","text":"<p>A view is a read-only object that is the result of a query over one or more tables and views in a Unity Catalog metastore. You can create a view from tables and from other views in multiple schemas and catalogs.</p> <p>A view stores the text of a query typically against one or more data sources or tables in the metastore. In Databricks, a view is equivalent to a Spark DataFrame persisted as an object in a schema. Unlike DataFrames, you can query views from anywhere in Databricks, assuming that you have permission to do so. Creating a view does not process or write any data. Only the query text is registered to the metastore in the associated schema.</p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#metric-views","title":"Metric Views","text":"<p>Metric views in Unity Catalog define reusable business metrics that are centrally maintained and accessible to all users in your workspace. A metric view abstracts the logic behind commonly used KPIs\u2014such as revenue, customer count, or conversion rate\u2014so they can be consistently queried across dashboards, notebooks, and reports. Each metric view specifies a set of measures and dimensions based on a source table, view, or SQL query. Metric views are defined in YAML and queried using SQL.</p> <p>Using metric views helps reduce inconsistencies in metric definitions that might otherwise be duplicated across multiple tools and workflows. </p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#materialized-views","title":"Materialized Views","text":"<p>Materialized views incrementally calculate and update the results returned by the defining query. Materialized views on Databricks are a special kind of Delta table. Whereas all other views on Databricks calculate results by evaluating the logic that defined the view when it is queried, materialized views process results and store them in an underlying table when updates are processed using either a refresh schedule or running a pipeline update.</p> <p>You can register materialized views in Unity Catalog using Databricks SQL or define them as part of Lakeflow Declarative Pipelines.</p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#temporary-views","title":"Temporary Views","text":"<ul> <li> <p>A temporary view has limited scope and persistence and is not registered to a schema or catalog. The lifetime of a temporary view differs based on the environment you're using:</p> </li> <li> <p>In notebooks and jobs, temporary views are scoped to the notebook or script level. They cannot be referenced outside of the notebook in which they are declared, and no longer exist when the notebook detaches from the cluster.</p> </li> <li> <p>In Databricks SQL, temporary views are scoped to the query level. Multiple statements within the same query can use the temp view, but it cannot be referenced in other queries, even within the same dashboard.</p> </li> </ul>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#dynamic-views","title":"Dynamic Views","text":"<p>Give access to custom functions that help in column masking and row/column level access.</p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#dropping-a-view","title":"Dropping a View","text":"<pre><code>DROP VIEW IF EXISTS catalog_name.schema_name.view_name;\n</code></pre>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#creating-a-dynamic-view","title":"Creating a Dynamic View","text":"<p>In Unity Catalog, you can use dynamic views to configure fine-grained access control, including:</p> <ul> <li> <p>Security at the level of columns or rows.</p> </li> <li> <p>Data masking.</p> </li> <li> <p>Unity Catalog introduces the following functions, which allow you to dynamically limit which users can access a row, column, or record in a view:</p> </li> </ul> <p><code>current_user()</code>: Returns the current user's email address.</p> <p><code>is_account_group_member()</code>: Returns TRUE if the current user is a member of a specific account-level group. Recommended for use in dynamic views against Unity Catalog data.</p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#requirements-for-dynamic-views","title":"Requirements for Dynamic Views","text":"<p>To create or read dynamic views, requirements are the same as those for standard views, except for compute requirements. You must use one of the following compute resources:</p> <ul> <li> <p>A SQL warehouse.</p> </li> <li> <p>Compute with standard access mode (formerly shared access mode).</p> </li> <li> <p>Compute with dedicated access mode (formerly single user access mode) on Databricks Runtime 15.4 LTS or above.</p> </li> <li> <p>You cannot read dynamic views using dedicated compute on Databricks Runtime 15.3 or below.</p> </li> </ul> <p>To take advantage of the data filtering provided in Databricks Runtime 15.4 LTS and above, you must also verify that your workspace is enabled for serverless compute, because the data filtering functionality that supports dynamic views runs on serverless compute.</p>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#column-level-permissions","title":"Column Level Permissions","text":"<p>With a dynamic view, you can limit the columns a specific user or group can access. In the following example, only members of the auditors group can access email addresses from the sales_raw table. During query analysis, Apache Spark replaces the CASE statement with either the literal string REDACTED or the actual contents of the email address column. Other columns are returned as normal. This strategy has no negative impact on the query performance.</p> <pre><code>-- Alias the field 'email' to itself (as 'email') to prevent the\n-- permission logic from showing up directly in the column name results.\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  CASE WHEN\n    is_account_group_member('auditors') THEN email\n    ELSE 'REDACTED'\n  END AS email,\n  country,\n  product,\n  total\nFROM sales_raw\n</code></pre>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#row-level-permissions","title":"Row Level Permissions","text":"<pre><code>CREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  country,\n  product,\n  total\nFROM sales_raw\nWHERE\n  CASE\n    WHEN is_account_group_member('managers') THEN TRUE\n    ELSE total &lt;= 1000000\n  END;\n</code></pre>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#data-masking","title":"Data Masking","text":"<pre><code>-- The regexp_extract function takes an email address such as\n-- user.x.lastname@example.com and extracts 'example', allowing\n-- analysts to query the domain name.\n\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  region,\n  CASE\n    WHEN is_account_group_member('auditors') THEN email\n    ELSE regexp_extract(email, '^.*@(.*)$', 1)\n  END\n  FROM sales_raw\n</code></pre> <p>You have this expression:</p> <pre><code>regexp_extract(email, '^.*@(.*)$', 1)\n</code></pre> <p>and your input email is:</p> <pre><code>user.x.lastname@example.com\n</code></pre>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#step-1-function-purpose","title":"\ud83e\udde9 Step 1: Function purpose","text":"<p><code>regexp_extract(column, pattern, groupIndex)</code></p> <ul> <li>column \u2192 the string column to extract from (<code>email</code>)</li> <li>pattern \u2192 the regular expression pattern (<code>'^.*@(.*)$'</code>)</li> <li>groupIndex \u2192 which capturing group to return (in this case <code>1</code>)</li> </ul>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#step-2-understanding-the-regex-pattern","title":"\ud83e\udde0 Step 2: Understanding the regex pattern","text":""},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#pattern","title":"Pattern: <code>^.*@(.*)$</code>","text":"Regex Part Meaning What it matches in your email <code>^</code> Start of string Anchors the regex to the beginning <code>.*</code> Any characters (0 or more) Matches <code>user.x.lastname</code> <code>@</code> Literal <code>@</code> symbol Matches the <code>@</code> in your email <code>(.*)</code> Capture group 1 \u2192 any characters after <code>@</code> Matches <code>example.com</code> <code>$</code> End of string Ensures it goes till the end"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#step-3-what-regexp_extract-does","title":"\u2699\ufe0f Step 3: What <code>regexp_extract</code> does","text":"<p><code>regexp_extract</code> extracts the content of capturing group 1, i.e. whatever is inside <code>( ... )</code>.</p> <p>So, for your input:</p> <pre><code>user.x.lastname@example.com\n</code></pre> <ul> <li>Everything before <code>@</code> \u2192 <code>user.x.lastname</code></li> <li>Everything after <code>@</code> \u2192 <code>example.com</code> \u2190 captured group</li> </ul>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#final-output","title":"\u2705 Final Output","text":"<p>The result of:</p> <pre><code>regexp_extract(email, '^.*@(.*)$', 1)\n</code></pre> <p>is:</p> <pre><code>example.com\n</code></pre>"},{"location":"docs-deep-dive/databricks/11-Databricks_Views/#optional-what-if-you-used-group-0","title":"\ud83e\udde0 Optional \u2014 What if you used group 0?","text":"<p>If you wrote:</p> <pre><code>regexp_extract(email, '^.*@(.*)$', 0)\n</code></pre> <p>That would return the entire matched string, i.e. the full email:</p> <pre><code>user.x.lastname@example.com\n</code></pre>"},{"location":"docs-deep-dive/databricks/12-Databricks_Governed_Tags/","title":"Apply Governed Tags to UC Objects","text":"<p>Tags are attributes that include keys and optional values that you can use to organize and categorize securable objects in Unity Catalog. Using tags also simplifies the search and discovery of tables and views using the workspace search functionality.</p> <p>Securable object tagging is currently supported on catalogs, schemas, tables, table columns, volumes, views, registered models, and model versions.</p> <p>Governed tags are account-level tags with enforced rules for consistency and control. Using governed tags, you define the allowed keys and values and control which users and groups can assign them to objects. This ensures tags are applied consistently and conform to organizational standards, giving centralized control over classification, compliance, and operations.</p> <p>System tags are a special type of governed tag that are predefined by Databricks. System tags have a few distinct characteristics:</p> <ul> <li> <p>System tag definitions (keys and values) are predefined by Databricks.</p> </li> <li> <p>Users cannot modify or delete system tag keys or values.</p> </li> <li> <p>Users can control who is allowed to assign or unassign system tags through governed tag permission settings.</p> </li> </ul> <p>System tags are designed to support standardized tagging across organizations, particularly for use cases like data classification, ownership, or lifecycle tracking. By using predefined, governed tag definitions, system tags help enforce consistency without requiring users to manually define or manage tag structures.</p>"},{"location":"docs-deep-dive/databricks/12-Databricks_Governed_Tags/#constraint","title":"Constraint","text":"<ul> <li> <p>Tag keys are case sensitive. For example, Sales and sales are two distinct tags.</p> </li> <li> <p>You can assign a maximum of 50 tags to a single securable object.</p> </li> <li> <p>The maximum length of a tag key is 255 characters.</p> </li> <li> <p>The maximum length of a tag value is 1000 characters.</p> </li> <li> <p>The following characters are not allowed in tag keys: <code>. , - = / :</code></p> </li> <li> <p>Trailing and leading spaces are not allowed in tag keys or values.</p> </li> <li> <p>Tag search using the workspace search UI is supported only for tables and views.</p> </li> <li> <p>Tag search requires exact term matching.</p> </li> </ul> <pre><code>SET TAG ON CATALOG catalog `cost_center` = `hr`;\n\nUNSET TAG ON CATALOG catalog cost_center;\n</code></pre> <pre><code>-- Add the governed tag to ssn column\nALTER TABLE abac.customers.profiles\nALTER COLUMN SSN\nSET TAGS ('pii' = 'ssn');\n</code></pre>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/","title":"Connect to cloud object storage using Unity Catalog","text":""},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#how-does-uc-use-cloud-storage","title":"How Does UC use cloud storage?","text":"<p>Databricks recommends using Unity Catalog to manage access to all data that you have stored in cloud object storage. Unity Catalog provides a suite of tools to configure secure connections to cloud object storage. These connections provide access to complete the following actions:</p> <ul> <li>Ingest raw data into a lakehouse.</li> <li>Create and read managed tables and managed volumes of unstructured data in Unity Catalog-managed cloud storage.</li> <li>Register or create external tables containing tabular data and external volumes containing unstructured data in cloud storage that is managed using your cloud provider.</li> <li>Read and write unstructured data (Unity Catalog volumes).</li> </ul> <p>There are two primary ways in which databricks allows us to use cloud storage:</p> <ul> <li> <p>Default (or \u201cmanaged\u201d) storage locations for managed tables and managed volumes (unstructured, non-tabular data) that you create in Databricks. These managed storage locations can be defined at the metastore, catalog, or schema level. You create managed storage locations in your cloud provider, but their lifecycle is fully managed by Unity Catalog.</p> </li> <li> <p>Storage locations where external tables and volumes are stored. These are tables and volumes whose access from Databricks is managed by Unity Catalog, but whose data lifecycle and file layout are managed using your cloud provider and other data platforms. Typically you use external tables to register large amounts of your existing data in Databricks, or if you also require write access to the data using tools outside of Databricks.</p> </li> </ul> <p>There are three major options supported by UC:</p> <ul> <li>AWS/Azure S3 or Blob buckets</li> <li>Cloudflare R2 Buckets</li> <li>Legacy dbfs root</li> </ul>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#governing-access-to-cloud-storage","title":"Governing Access to Cloud Storage","text":"<p>To manage access to the underlying cloud storage that holds tables and volumes, Unity Catalog uses a securable object called an external location, which defines a path to a cloud storage location and the credentials required to access that location. </p> <p>Those credentials are, in turn, defined in a Unity Catalog securable object called a storage credential. By granting and revoking access to external location securables in Unity Catalog, you control access to the data in the cloud storage location. By granting and revoking access to storage credential securables in Unity Catalog, you control the ability to create external location objects.</p>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#overview-of-storage-credentials","title":"Overview of Storage Credentials","text":"<p>A storage credential represents an authentication and authorization mechanism for accessing data stored on your cloud tenant. For example, a storage credential is associated with an IAM role for S3 buckets, or with an R2 API token for Cloudflare R2 buckets.</p> <p>Privileges granted in Unity Catalog control which users and groups can use the credential to define external locations. Permission to create and use storage credentials should be granted only to users who need to create external location objects.</p>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#overview-of-external-locations","title":"Overview of External Locations","text":"<p>An external location combines a cloud storage path with a storage credential that authorizes access to the specified path. Multiple external locations can use the same storage credential. External locations can reference storage paths in any of the supported cloud storage options.</p> <p></p> <ul> <li>Each external location references a storage credential and a cloud storage location.</li> <li>Multiple external locations can reference the same storage credential. Storage credential 1 grants access to everything under the path bucket/tables/*, so both External location A and External location B reference it.</li> </ul> <p>External locations are used in Unity Catalog both for external data assets, like external tables and external volumes, and for managed data assets, like managed tables and managed volumes.</p>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#using-external-locations-when-creating-external-tables-and-volumes","title":"Using External Locations when creating external tables and volumes","text":"<p>External tables and external volumes registered in Unity Catalog are essentially pointers to data in cloud storage that you manage outside of Databricks. When you create an external table or external volume in Unity Catalog, you must reference a cloud storage path that is included in an external location object that you have been granted adequate privileges on.</p>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#using-external-location-when-creating-managed-tables-and-volumes","title":"Using External Location when creating managed tables and volumes","text":"<p>Managed tables and managed volumes are fully managed by Unity Catalog. They are stored by default in a managed storage location, which can be defined at the metastore, catalog, or schema level. When you assign a managed storage location to a metastore, catalog, or schema, you must reference an external location object, and you must have adequate privileges to use it.</p>"},{"location":"docs-deep-dive/databricks/13-Databricks_Connecting_To_Cloud_Object_Storage_Intro/#important","title":"\u26a0\ufe0f Important","text":"<p>If you update external table metadata using a non-Databricks client or using path-based access from within Databricks, that metadata does not automatically sync state with Unity Catalog. Databricks recommends against such metadata updates, but if you do perform one, you must run MSCK REPAIR TABLE  SYNC METADATA to bring the schema in Unity Catalog up to date."},{"location":"docs-deep-dive/databricks/14-Databricks_Managed_Storage_Location_Hierarchy/","title":"Specify a managed storage location in Unity Catalog","text":"<p>A managed storage location specifies a location in cloud object storage for storing data for managed tables and managed volumes.</p> <p>You can associate a managed storage location with a metastore, catalog, or schema. Managed storage locations at lower levels in the hierarchy override storage locations defined at higher levels when managed tables or managed volumes are created.</p> <p>New workspaces that are enabled for Unity Catalog automatically are created without a metastore-level managed storage location. </p>"},{"location":"docs-deep-dive/databricks/14-Databricks_Managed_Storage_Location_Hierarchy/#what-is-a-managed-storage-location","title":"What is a managed storage location?","text":"<p>Managed storage locations have the following properties:</p> <ul> <li>Managed tables and managed volumes store data and metadata files in managed storage locations.</li> <li>Managed storage locations cannot overlap with external tables or external volumes.</li> </ul> <p></p>"},{"location":"docs-deep-dive/databricks/14-Databricks_Managed_Storage_Location_Hierarchy/#rules-and-hierarchy","title":"Rules and Hierarchy","text":"<ul> <li>If the containing schema has a managed location, the data is stored in the schema managed location.</li> <li>If the containing schema does not have a managed location but the catalog has a managed location, the data is stored in the catalog managed location.</li> <li>If neither the containing schema nor the containing catalog have a managed location, data is stored in the metastore managed location.</li> </ul>"},{"location":"docs-deep-dive/databricks/14-Databricks_Managed_Storage_Location_Hierarchy/#setting-managed-storage","title":"Setting Managed Storage","text":"<pre><code>CREATE CATALOG &lt;catalog-name&gt;\nMANAGED LOCATION 's3://&lt;external-location-bucket-path&gt;/&lt;directory&gt;';\n</code></pre> <pre><code>CREATE SCHEMA &lt;catalog&gt;.&lt;schema-name&gt;\nMANAGED LOCATION 's3://&lt;external-location-bucket-path&gt;/&lt;directory&gt;';\n</code></pre>"},{"location":"scenarios/","title":"Scenarios","text":"<p>This is the overview page for Scenarios.</p>"},{"location":"scenarios/adf/architectures/","title":"Architectures","text":""},{"location":"scenarios/adf/architectures/#architectures","title":"Architectures","text":""},{"location":"scenarios/adf/architectures/#conditional-data-file-transformations","title":"Conditional Data File Transformations","text":""},{"location":"scenarios/adf/architectures/#copy-missing-destination-files-from-source","title":"Copy Missing Destination Files from Source","text":""},{"location":"scenarios/adf/architectures/#list-files-and-count-of-files","title":"List files and count of files","text":""},{"location":"scenarios/adf/architectures/#incremental-data-loading-with-json-watermarking","title":"Incremental Data Loading with JSON Watermarking","text":""},{"location":"scenarios/adf/architectures/#backdate-reprocess-pipeline","title":"Backdate Reprocess Pipeline","text":"<p>Same as above but with a backdate optional parameter</p> <p></p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/","title":"AWS Reference Architecture and Integration with Databricks","text":""},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#reference-databricks-architecture-on-aws","title":"Reference Databricks Architecture on AWS","text":"<p>The AWS reference architecture shows the following AWS-specific services for ingesting, storage, serving, and analysis:</p> <ul> <li>Amazon Redshift as a source for Lakehouse Federation</li> <li>Amazon AppFlow and AWS Glue for batch ingest</li> <li>AWS IoT Core, Amazon Kinesis, and AWS DMS for streaming ingest</li> <li>Amazon S3 as the object storage for data and AI assets</li> <li>Amazon RDS and Amazon DynamoDB as operational databases</li> <li>Amazon QuickSight as BI tool</li> <li>Amazon Bedrock is used by Model Serving to call external LLMs from leading AI startups and Amazon</li> </ul>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#organization-of-reference-architecture","title":"Organization of Reference Architecture","text":"<p>The reference architecture is structured along the swim lanes Source, Ingest, Transform, Query/Process, Serve, Analysis, and Storage:</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#source","title":"Source","text":"<p>There are three ways to integrate external data into the Data Intelligence Platform:</p> <ul> <li> <p>ETL: The platform enables integration with systems that provide semi-structured and unstructured data (such as sensors, IoT devices, media, files, and logs), as well as structured data from relational databases or business applications.</p> </li> <li> <p>Lakehouse Federation: SQL sources, such as relational databases, can be integrated into the lakehouse and Unity Catalog without ETL. In this case, the source system data is governed by Unity Catalog, and queries are pushed down to the source system.</p> </li> <li> <p>Catalog Federation: External Hive Metastore catalogs or AWS Glue can also be integrated into Unity Catalog through catalog federation, allowing Unity Catalog to control the tables stored in Hive Metastore or AWS Glue.</p> </li> </ul>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#ingest","title":"Ingest","text":"<p>Ingest data into the lakehouse via batch or streaming:</p> <ul> <li> <p>Databricks Lakeflow Connect offers built-in connectors for ingestion from enterprise applications and databases. The resulting ingestion pipeline is governed by Unity Catalog and is powered by serverless compute and Lakeflow Declarative Pipelines.</p> </li> <li> <p>Files delivered to cloud storage can be loaded directly using the Databricks Auto Loader.</p> </li> <li> <p>For batch ingestion of data from enterprise applications into Delta Lake, the Databricks lakehouse relies on partner ingest tools with specific adapters for these systems of record.</p> </li> <li> <p>Streaming events can be ingested directly from event streaming systems such as Kafka using Databricks Structured Streaming. Streaming sources can be sensors, IoT, or change data capture processes.</p> </li> </ul>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#storage","title":"Storage","text":"<p>Data is typically stored in the cloud storage system where the ETL pipelines use the medallion architecture to store data in a curated way as Delta files/tables or Apache Iceberg tables.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#transform-and-query-process","title":"Transform and Query / process","text":"<ul> <li> <p>The Databricks lakehouse uses its engines Apache Spark and Photon for all transformations and queries.</p> </li> <li> <p>Lakeflow Declarative Pipelines is a declarative framework for simplifying and optimizing reliable, maintainable, and testable data processing pipelines.</p> </li> </ul> <p>Powered by Apache Spark and Photon, the Databricks Data Intelligence Platform supports both types of workloads: SQL queries via SQL warehouses, and SQL, Python and Scala workloads via workspace clusters.</p> <p>For data science (ML Modeling and Gen AI), the Databricks AI and Machine Learning platform provides specialized ML runtimes for AutoML and for coding ML jobs. All data science and MLOps workflows are best supported by MLflow.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#serving","title":"Serving","text":"<p>For data warehousing (DWH) and BI use cases, the Databricks lakehouse provides Databricks SQL, the data warehouse powered by SQL warehouses, and serverless SQL warehouses.</p> <p>For machine learning, Mosaic AI Model Serving is a scalable, real-time, enterprise-grade model serving capability hosted in the Databricks control plane. Mosaic AI Gateway is Databricks solution for governing and monitoring access to supported generative AI models and their associated model serving endpoints.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#operational-databases","title":"Operational databases:","text":"<p>Lakebase is an online transaction processing (OLTP) database based on Postgres and fully integrated with the Databricks Data Intelligence Platform. It allows you to create OLTP databases on Databricks, and integrate OLTP workloads with your Lakehouse. External systems, such as operational databases, can be used to store and deliver final data products to user applications.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#collaboration","title":"Collaboration:","text":"<p>Business partners get secure access to the data they need through Delta Sharing.</p> <p>Based on Delta Sharing, the Databricks Marketplace is an open forum for exchanging data products.</p> <p>Clean Rooms are secure and privacy-protecting environments where multiple users can work together on sensitive enterprise data without direct access to each other's data.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#analysis","title":"Analysis","text":"<p>The final business applications are in this swim lane. Examples include custom clients such as AI applications connected to Mosaic AI Model Serving for real-time inference or applications that access data pushed from the lakehouse to an operational database.</p> <p>For BI use cases, analysts typically use BI tools to access the data warehouse. SQL developers can additionally use the Databricks SQL Editor (not shown in the diagram) for queries and dashboarding.</p> <p>The Data Intelligence Platform also offers dashboards to build data visualizations and share insights.</p>"},{"location":"scenarios/databricks/00-AWS_Reference_Arch_Databricks/#integrate","title":"Integrate","text":"<p>The Databricks platform integrates with standard identity providers for user management and single sign on (SSO). External AI services like OpenAI, LangChain or HuggingFace can be used directly from within the Databricks Intelligence Platform.</p> <p>External orchestrators can either use the comprehensive REST API or dedicated connectors to external orchestration tools like Apache Airflow.</p> <p>Unity Catalog is used for all data &amp; AI governance in the Databricks Intelligence Platform and can integrate other databases into its governance through Lakehouse Federation.</p> <p>Additionally, Unity Catalog can be integrated into other enterprise catalogs. Contact the enterprise catalog vendor for details.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/","title":"AWS + Databricks Reference Architecture","text":""},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#reference-architectures-using-databricks-and-aws","title":"Reference Architectures Using Databricks and AWS","text":""},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#built-in-ingestion-from-saas-apps-and-databases-with-lakeflow-connect","title":"Built-in ingestion from SaaS apps and databases with Lakeflow Connect","text":"<p>Databricks Lakeflow Connect offers built-in connectors for ingestion from enterprise applications and databases. The resulting ingestion pipeline is governed by Unity Catalog and is powered by serverless compute and Lakeflow Declarative Pipelines.</p> <p>Lakeflow Connect leverages efficient incremental reads and writes to make data ingestion faster, scalable, and more cost-efficient, while your data remains fresh for downstream consumption.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#batch-ingestion-and-etl","title":"Batch ingestion and ETL","text":"<p>Ingestion tools use source-specific adapters to read data from the source and then either store it in the cloud storage from where Auto Loader can read it, or call Databricks directly (for example, with partner ingestion tools integrated into the Databricks lakehouse). </p> <p>To load the data, the Databricks ETL and processing engine runs the queries via Lakeflow Declarative Pipelines. Orchestrate single or multitask jobs using Lakeflow Jobs and govern them using Unity Catalog (access control, audit, lineage, and so on). </p> <p>To provide access to specific golden tables for low-latency operational systems, export the tables to an operational database such as an RDBMS or key-value store at the end of the ETL pipeline.</p> <p></p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#stream-processing-and-cdc","title":"Stream Processing and CDC","text":"<p>The Databricks ETL engine Spark Structured Streaming to read from event queues such as Apache Kafka or AWS Kinesis. The downstream steps follow the approach of the Batch use case above.</p> <p>Real-time change data capture (CDC) typically uses an event queue to store the extracted events. From there, the use case follows the streaming use case.</p> <p>If CDC is done in batch where the extracted records are stored in cloud storage first, then Databricks Autoloader can read them and the use case follows Batch ETL.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#machine-learning-and-ai-traditional","title":"Machine Learning and AI : Traditional","text":"<p>For machine learning, the Databricks Data Intelligence Platform provides Mosaic AI, which comes with state-of-the-art machine and deep learning libraries. It provides capabilities such as Feature Store and Model Registry (both integrated into Unity Catalog), low-code features with AutoML, and MLflow integration into the data science lifecycle.</p> <p>All data science-related assets (tables, features, and models) are governed by Unity Catalog and data scientists can use Lakeflow Jobs to orchestrate their jobs.</p> <p>For deploying models in a scalable and enterprise-grade way, use the MLOps capabilities to publish the models in model serving.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#ai-agent-applications-genai","title":"AI Agent Applications (GenAI)","text":"<p>For deploying models in a scalable and enterprise-grade way, use the MLOps capabilities to publish the models in model serving.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#bi-and-sql-analytics","title":"BI and SQL Analytics","text":"<p>For BI use cases, business analysts can use dashboards, the Databricks SQL editor or BI tools such as Tableau or Amazon QuickSight. In all cases, the engine is Databricks SQL (serverless or non-serverless), and Unity Catalog provides data discovery, exploration, lineage, and access control.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#catalog-federation","title":"Catalog Federation","text":"<p>Catalog federation allows external Hive Metastores (such as MySQL, Postgres, or Redshift) or Amazon Glue to be integrated with Databricks.</p> <p>All workloads (AI, DWH, and BI) can benefit from this without the need to ETL the data into object storage first. The external source catalog is added to Unity Catalog where fine-grained access control is applied via the Databricks platform.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#share-data-with-3rd-party","title":"Share Data with 3rd Party","text":"<p>Enterprise-grade data sharing with 3rd parties is provided by Delta Sharing. It enables direct access to data in the object store secured by Unity Catalog. This capability is also used in the Databricks Marketplace, an open forum for exchanging data products.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#consume-shared-data-from-databricks","title":"Consume Shared Data from Databricks","text":"<p>The Delta Sharing Databricks-to-Databricks protocol allows to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for Unity Catalog.</p>"},{"location":"scenarios/databricks/01-Reference_Architectures_Pt1/#pillars-of-well-architected-lakehouse","title":"Pillars of Well Architected Lakehouse","text":"<p>Data and AI governance</p> <p>The oversight to ensure that data and AI bring value and support your business strategy.</p> <p>Interoperability and usability</p> <p>The ability of the lakehouse to interact with users and other systems.</p> <p>Operational excellence</p> <p>All operations processes that keep the lakehouse running in production.</p> <p>Security, privacy, and compliance</p> <p>Protect the Databricks application, customer workloads, and customer data from threats.</p> <p>Reliability</p> <p>The ability of a system to recover from failures and continue to function.</p> <p>Performance efficiency</p> <p>The ability of a system to adapt to changes in load.</p> <p>Cost optimization</p> <p>Managing costs to maximize the value delivered.</p>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/","title":"Why are closed segment files kept open by Kafka","text":""},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#why-are-closed-segment-files-kept-open-in-kafka","title":"Why are Closed Segment Files kept 'open' in Kafka?","text":""},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#1-background-kafka-partitions-and-log-segments","title":"1. Background \u2014 Kafka partitions and log segments","text":"<p>Every Kafka partition is stored on disk as a log, and that log is split into multiple segment files.</p> <p>For example: If you have a topic called <code>orders</code> with 3 partitions, and each partition has multiple segment files, you might see something like this on disk:</p> <pre><code>/data/kafka/orders-0/\n    00000000000000000000.log\n    00000000000000000000.index\n    00000000000000000000.timeindex\n    00000000000000001000.log\n    00000000000000001000.index\n    00000000000000001000.timeindex\n\n/data/kafka/orders-1/\n    ...\n</code></pre> <p>Each <code>.log</code> file is one segment \u2014 a fixed-size chunk of data from that partition\u2019s message stream. Kafka rolls over to a new segment file after a certain size or time limit (e.g., 1 GB or 1 hour).</p>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#2-how-brokers-handle-these-segment-files","title":"2. How brokers handle these segment files","text":"<p>For performance reasons, the Kafka broker keeps every segment file \"open\" \u2014 meaning it maintains an open file handle to it.</p> <p>An open file handle is the operating system\u2019s way of saying \u201cthis file is currently in use by a program.\u201d</p> <ul> <li>When a program (like Kafka) opens a file, the OS creates a file descriptor.</li> <li>The broker uses this descriptor to read or write data quickly without reopening the file every time.</li> </ul> <p>Kafka keeps these files open because:</p> <ul> <li>Producers and consumers may need to read or write to any segment at any moment.</li> <li>Opening and closing files repeatedly would be slow (system call overhead).</li> <li>Keeping them open allows faster reads and writes, since file descriptors are already established.</li> </ul>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#3-what-this-means-in-practice","title":"3. What this means in practice","text":"<p>Each partition can have multiple segment files, and Kafka keeps an open handle for every one of them \u2014 even if the segment is inactive (not currently being written to).</p> <p>Example:</p> <p>Suppose:</p> <ul> <li>You have 1,000 partitions on a broker.</li> <li>Each partition has 10 segment files (some active, some old).   \u2192 That\u2019s 10,000 open file handles just for the data logs.</li> </ul> <p>And Kafka opens three files per segment:</p> <ul> <li><code>.log</code></li> <li><code>.index</code></li> <li><code>.timeindex</code></li> </ul> <p>So total file handles could be: <code>1,000 partitions \u00d7 10 segments \u00d7 3 files = 30,000 open file handles</code>.</p> <p>That\u2019s quite a lot \u2014 and for larger clusters, it can reach hundreds of thousands.</p>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#4-why-the-os-needs-to-be-tuned","title":"4. Why the OS needs to be tuned","text":"<p>Operating systems (Linux, macOS, etc.) have a limit on how many files a process can keep open at once. This is called the open file descriptor limit (ulimit).</p> <p>On Linux, you can check it with:</p> <pre><code>ulimit -n\n</code></pre> <p>Typical defaults might be 1024 or 4096 \u2014 far too low for a Kafka broker.</p> <p>If Kafka tries to open more files than the OS allows, you\u2019ll get errors like:</p> <pre><code>Too many open files\n</code></pre> <p>and the broker may crash or fail to serve requests.</p>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#5-how-to-fix-tune-it","title":"5. How to fix (tune) it","text":"<p>Kafka administrators must increase the OS limit on open file descriptors for the Kafka process.</p> <p>This is usually done in:</p> <ul> <li>Systemd service configuration (<code>LimitNOFILE</code>), or</li> <li>Shell config (<code>ulimit -n 1000000</code>), depending on your deployment.</li> </ul> <p>A common best practice:</p> <pre><code>ulimit -n 1000000\n</code></pre> <p>Kafka\u2019s documentation recommends setting it to at least 100,000 or more, depending on:</p> <ul> <li>Number of partitions per broker</li> <li>Segment count per partition</li> <li>Replication factor</li> </ul>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#6-in-short-the-passage-means","title":"6. In short \u2014 the passage means:","text":"<p>\u201cEach Kafka broker keeps every log segment file open (even old ones), which can lead to a huge number of open files. Because of this, you must tune your operating system to allow many open file handles.\u201d</p>"},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#7-quick-summary-table","title":"7. Quick summary table","text":"Concept Explanation Log segment A chunk of data for a partition stored as a file (e.g., 1 GB each). File handle / file descriptor OS-level resource for accessing an open file. Kafka behavior Keeps all segment files open for faster I/O (no reopening overhead). Impact Can result in thousands of open files per broker. Solution Increase OS limits (<code>ulimit -n</code>) to allow large numbers of open files."},{"location":"scenarios/kafka/00-Why_Closed_Segments_Files_Open/#example-scenario","title":"Example scenario","text":"<ul> <li>Topic with 100 partitions, replication factor 3 \u2192 300 replicas across brokers.</li> <li>Each replica has 20 segment files.</li> <li>Each segment has 3 files (log, index, timeindex).</li> <li>Broker might handle ~100,000 open file descriptors.</li> </ul> <p>So you must tune Linux to handle that load.</p> <p>\u2705 In short: Kafka brokers keep all segment files open for speed, which can lead to thousands (or even hundreds of thousands) of open file handles. Therefore, you need to increase the OS\u2019s open file limit so Kafka can function reliably.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/","title":"How does Producer Guarantee Exactly Once Semantics in Kafka?","text":"<p>Excellent question \u2014 this gets to the core of Kafka\u2019s delivery guarantees and offset management model.</p> <p>Let\u2019s go step-by-step: first, we\u2019ll clarify who controls offsets, and then explain how the producer achieves exactly-once semantics (EOS) using producer IDs, epochs, and transactional commits.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#1-who-maintains-offsets-broker-or-producer","title":"\ud83e\udde9 1. Who maintains offsets \u2014 broker or producer?","text":""},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#offsets-are-maintained-by-the-broker","title":"\u2705 Offsets are maintained by the broker.","text":"<p>Let\u2019s unpack that clearly.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#producers-view","title":"Producer\u2019s view","text":"<p>When a producer sends messages to a Kafka topic-partition, it does not know the offsets in advance. Offsets are assigned by the partition leader broker when it writes the batch to its log.</p> <p>Process:</p> <ol> <li>Producer creates a batch of messages.</li> <li>It sets the base offset = 0 temporarily (a placeholder).</li> <li>Sends the batch to the leader broker for that partition.</li> <li>The leader writes the batch to the partition\u2019s log file, assigning real offsets sequentially.</li> <li> <p>The broker then returns a ProduceResponse to the producer that includes:</p> </li> <li> <p>The base offset of the first record.</p> </li> <li>Any error codes.</li> </ol> <p>So, the producer never generates offsets \u2014 the broker does.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#consumers-view","title":"Consumer\u2019s view","text":"<p>For consumers, offsets represent their position in the log \u2014 how far they\u2019ve read.</p> <ul> <li>Consumers read offsets assigned by brokers.</li> <li>They commit offsets (to Kafka or external store) to mark progress.</li> </ul> <p>So, offsets are a broker-managed sequence that both producers and consumers depend on.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#2-how-kafka-assigns-offsets-summary","title":"\u2699\ufe0f 2. How Kafka assigns offsets (summary)","text":"Stage Who assigns Description Produce Broker (leader) Assigns offsets sequentially per partition as batches are appended. Consume Broker provides Consumer fetch requests specify the starting offset, broker returns records and their offsets. Commit Consumer Saves last processed offset to Kafka (<code>__consumer_offsets</code> topic). <p>Offsets are strictly increasing and immutable within a partition \u2014 ensuring total order for that partition.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#3-how-producers-ensure-exactly-once-semantics-eos","title":"\ud83e\udde0 3. How producers ensure exactly-once semantics (EOS)","text":"<p>Kafka\u2019s exactly-once semantics were introduced in Kafka 0.11 through idempotent producers and transactions.</p> <p>Let\u2019s break this into two layers:</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#layer-1-idempotent-producer-no-duplicates","title":"Layer 1 \u2014 Idempotent producer (no duplicates)","text":"<p>Normally, when a producer retries (due to a temporary network failure), the same message could be written multiple times.</p> <p>Example:</p> <ol> <li>Producer sends message M1.</li> <li>Broker receives it, writes it, but the response back to producer is lost.</li> <li>Producer retries M1.</li> <li>Without EOS, the broker writes M1 again \u2014 duplicates.</li> </ol> <p>To fix this, Kafka introduced the idempotent producer, which ensures:</p> <p>\"No message will be written twice to the same topic-partition, even if retries happen.\"</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#how-idempotent-producers-work","title":"\ud83e\udde9 How idempotent producers work","text":"<p>Each producer is assigned three key identifiers:</p> Field Description Producer ID (PID) Unique 64-bit ID assigned by the broker when producer starts. Producer Epoch Incremented when producer restarts (used to detect old sessions). Sequence Number Incremented for each record sent to a partition. <p>Each partition the producer writes to has its own sequence counter.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#example","title":"\ud83d\udd01 Example","text":"Message PID Partition Sequence M1 500 0 0 M2 500 0 1 M3 500 0 2 <p>If a retry occurs, and M2 is sent again with the same PID and sequence, the broker checks:</p> <p>\u201cHave I already seen (PID=500, partition=0, seq=1)?\u201d</p> <p>If yes \u2192 duplicate ignored. If no \u2192 accept and append.</p> <p>Thus, retries no longer create duplicates.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#key-guarantees-from-idempotent-producer","title":"\u2705 Key guarantees from idempotent producer","text":"<ul> <li>Each (PID, partition, sequence) tuple is unique and ordered.</li> <li>The broker uses this metadata to detect and ignore duplicate writes.</li> <li>This works automatically when:</li> </ul> <pre><code>enable.idempotence=true\n</code></pre>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#layer-2-transactions-atomic-multi-partition-writes","title":"Layer 2 \u2014 Transactions (atomic multi-partition writes)","text":"<p>Idempotent producers guarantee no duplicates per partition, but what if a producer writes to multiple partitions or topics as part of one logical operation?</p> <p>Example:</p> <pre><code>Producer writes:\n  - Message to topic A, partition 0\n  - Message to topic B, partition 2\n</code></pre> <p>We want either both messages committed, or neither (atomicity).</p> <p>This is achieved with Kafka transactions.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#how-transactions-work","title":"\ud83e\udde9 How transactions work","text":"<p>Each transaction groups multiple produce requests into a single atomic unit.</p> <p>Steps:</p> <ol> <li>Producer begins a transaction:</li> </ol> <p><pre><code>producer.initTransactions();\nproducer.beginTransaction();\n</code></pre> 2. Producer sends records to multiple partitions. 3. When done, producer calls:</p> <pre><code>producer.commitTransaction();\n</code></pre> <p>or rolls back:</p> <p><pre><code>producer.abortTransaction();\n</code></pre> 4. The broker uses a transaction coordinator (one per producer) to manage state. 5. Kafka marks the affected records as:</p> <ul> <li>Committed (visible to consumers)</li> <li>Aborted (hidden from consumers)</li> </ul>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#transaction-metadata-on-broker","title":"\ud83e\uddf1 Transaction metadata on broker","text":"<p>Brokers store transaction state in an internal topic:</p> <pre><code>__transaction_state\n</code></pre> <p>It contains info about:</p> <ul> <li>Transaction IDs</li> <li>Producers\u2019 current epochs</li> <li>Whether transactions are ongoing, committed, or aborted</li> </ul> <p>Consumers that are transactional-aware (using isolation level <code>read_committed</code>) only see messages from committed transactions.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#combining-the-two-layers","title":"\u2696\ufe0f Combining the two layers","text":"Feature Provided by Ensures Idempotence Producer ID + sequence numbers No duplicates per partition Transactions Transaction coordinator Atomicity across multiple partitions Read committed isolation Consumers Consumers only see committed data <p>Together, these provide exactly-once semantics (EOS) end-to-end.</p>"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#exactly-once-delivery-flow-step-by-step","title":"\u2705 Exactly-once delivery flow (step-by-step)","text":"Step Action Who handles it Guarantee 1 Producer sends messages with PID + seq Producer \u2192 Broker Broker detects duplicates 2 Broker appends unique messages Broker No duplicate writes 3 Producer commits transaction Producer \u2192 Coordinator Atomic commit across partitions 4 Consumer reads messages Consumer Sees each committed record exactly once"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#4-putting-it-all-together","title":"\ud83d\udca1 4. Putting it all together","text":"Concept Controlled by Purpose Offsets Broker Identify message position in partition log Producer ID, Epoch, Sequence Producer + Broker Deduplicate writes (idempotence) Transaction Coordinator Broker (special role) Group multiple writes into a single atomic commit Consumer offsets Consumer (stored in <code>__consumer_offsets</code>) Track progress per consumer group Exactly-once semantics (EOS) Producer + Broker + Consumer Ensure one logical event = one delivery, no duplicates, no loss"},{"location":"scenarios/kafka/01-How_Does_Producer_Guarantee_Exactly_Once/#5-summary","title":"\ud83d\udd10 5. Summary","text":"Question Answer Who assigns offsets? The broker assigns offsets sequentially per partition. Producers do not set them. Who maintains offsets? Brokers maintain partition offsets; consumers maintain their read offsets separately. How does the producer ensure exactly-once? By using idempotence (PID + sequence numbers) and transactions (atomic commits). How are duplicates prevented? Broker checks <code>(PID, partition, sequence)</code> to detect and ignore duplicates. How are multi-partition writes atomic? Kafka\u2019s transaction coordinator manages commit or abort for all partitions in the transaction. <p>\u2705 In simple terms:</p> <ul> <li>Offsets are always assigned and managed by the broker.</li> <li> <p>Producers achieve exactly-once semantics using a combination of:</p> </li> <li> <p>Idempotent producers (no duplicates),</p> </li> <li>Transactions (atomic commits), and</li> <li>Read-committed consumers (see only committed data).</li> </ul> <p>Together, these make Kafka capable of true end-to-end exactly-once delivery \u2014 even across failures, retries, and multi-partition writes.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/","title":"Does Sequence Number Remain the same for a message if producer sends it again to same partition?","text":"<p>Excellent \u2014 this is a very precise and important question.</p> <p>The short answer is:</p> <p>\u2705 Yes. If a producer retries sending the same message batch to the same partition, it uses the same sequence numbers for those messages.</p> <p>And that\u2019s exactly how Kafka\u2019s idempotence mechanism detects and prevents duplicates.</p> <p>Let\u2019s unpack this in detail so you understand why, how, and what happens internally.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#1-quick-recap-sequence-numbers-in-kafka","title":"1. Quick recap: sequence numbers in Kafka","text":"<p>When idempotence is enabled (<code>enable.idempotence=true</code>, which is default since Kafka 3.0), every producer instance is assigned:</p> Field Description Producer ID (PID) A unique 64-bit ID assigned by the broker when the producer starts. Sequence number A monotonically increasing integer per <code>(producer, partition)</code> pair. Producer epoch A number incremented each time the producer restarts or recovers. <p>Each partition that a producer writes to has its own independent sequence counter.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#2-how-sequence-numbers-are-assigned","title":"2. How sequence numbers are assigned","text":"<p>Every time the producer sends a message (or batch of messages) to a partition, it assigns sequence numbers to those records:</p> Message Partition Sequence number M1 0 0 M2 0 1 M3 0 2 <p>If those three messages are sent in a single batch, the batch header will say:</p> <pre><code>firstSequence = 0\nlastSequence = 2\n</code></pre> <p>Then, when the next batch is created for that same partition, its first sequence will start from <code>3</code>.</p> <p>So the sequence number keeps increasing within each partition, as long as the producer session remains alive.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#3-when-the-producer-retries-a-batch-due-to-network-or-timeout","title":"3. When the producer retries a batch (due to network or timeout)","text":"<p>Now here\u2019s your question\u2019s scenario:</p> <p>Imagine the producer sent a batch, but didn\u2019t get an acknowledgment from the broker (e.g., network timeout, temporary partition leader failure).</p> <p>From the producer\u2019s perspective, it\u2019s not sure if the batch was received or not.</p> <p>So it retries sending the same batch again.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#what-happens-to-the-sequence-number","title":"What happens to the sequence number?","text":"<p>\ud83d\udc49 The sequence number stays the same.</p> <p>Kafka producers do not reassign new sequence numbers for retried batches. They reuse the exact same <code>(PID, Partition, Sequence)</code> combination.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#4-what-the-broker-does","title":"4. What the broker does","text":"<p>The broker keeps track of the last acknowledged sequence number for every <code>(PID, Partition)</code> pair.</p> <p>Let\u2019s say:</p> <ul> <li>Broker previously appended messages 0\u20132 (sequence 0, 1, 2) from PID=1234.</li> </ul> <p>Now the producer retries sending that same batch (sequence 0\u20132) again. The broker checks its internal table:</p> <pre><code>Last seen sequence for PID=1234, Partition=0 is 2.\nIncoming batch starts at 0.\n\u2192 These are duplicates. Ignore them.\n</code></pre> <p>Broker silently discards the duplicate batch. The producer eventually gets an acknowledgment (either from the original or retried request).</p> <p>\u2705 Result: The messages appear exactly once in the partition log.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#5-what-if-the-producer-sends-new-messages-after-retry","title":"5. What if the producer sends new messages after retry?","text":"<p>Once the producer gets acknowledgment for the batch (either the first or the retried one), it increments its per-partition sequence counter and continues:</p> Batch Partition Seq Range Notes 1 0 0\u20132 Sent, retried once, appended once 2 0 3\u20135 New batch 3 0 6\u20138 New batch <p>So the sequence numbers only increase after successful acknowledgment.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#6-what-if-the-producer-restarts","title":"6. What if the producer restarts?","text":"<p>When a producer restarts, it loses its local sequence counters, so it cannot continue the previous sequence.</p> <p>To handle this safely:</p> <ul> <li>The producer epoch is incremented by the broker when the producer reconnects.</li> <li>A higher epoch means \u201cnew session\u201d \u2192 any in-flight or stale messages from older epochs are ignored.</li> </ul> <p>This ensures that no old messages from a crashed producer get appended later.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#7-internal-consistency-rules-simplified","title":"7. Internal consistency rules (simplified)","text":"Condition Broker\u2019s action Sequence = expected next Accept and append Sequence &lt; expected (duplicate) Ignore (already seen) Sequence &gt; expected (gap) Producer out of sync \u2192 broker triggers OutOfOrderSequence error <p>This is how Kafka brokers maintain strict order and idempotence per partition.</p>"},{"location":"scenarios/kafka/02-Does_Seq_No_Remain_Same_After_Producer_Goes_Down/#8-summary-table","title":"8. Summary table","text":"Scenario Sequence reused? Broker action Outcome Producer retries same batch \u2705 Yes Detect duplicate, ignore No duplicates written Producer sends new batch \ud83d\udeab No (increments sequence) Append new messages Correct order preserved Producer restarts (new epoch) \ud83d\udd01 Sequence reset but epoch \u2191 Old messages from previous epoch ignored Prevents mixing sessions <p>\u2705 In short:</p> <ul> <li>Sequence numbers are assigned by the producer, per partition.</li> <li>When a producer retries the same message or batch, it reuses the same sequence numbers.</li> <li>The broker detects duplicates using <code>(PID, Partition, Sequence)</code> and discards any already-written messages.</li> <li>That\u2019s how Kafka ensures exactly-once delivery at the broker level \u2014 even when retries happen.</li> </ul>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/","title":"What happens when a relection happens and how is idempotency still preserved?","text":""},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#1-the-situation-a-leader-re-election-happens","title":"1. The situation: a leader re-election happens","text":"<p>Let\u2019s start with what causes this.</p> <p>A leader re-election occurs when the broker currently leading a partition:</p> <ul> <li>Crashes,</li> <li>Is shut down,</li> <li>Loses connection,</li> <li>Or is demoted by the controller.</li> </ul> <p>Then, one of the in-sync replicas (ISR) becomes the new leader.</p> <p>So now:</p> <ul> <li>Old leader = broker A (down)</li> <li>New leader = broker B</li> </ul>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#2-what-happens-to-producer-writes-in-flight","title":"2. What happens to producer writes in flight?","text":"<p>Suppose a producer was sending messages to partition P0, which had leader = broker A.</p> <p>Before broker A crashed:</p> <ul> <li>The producer sent batches with sequence numbers 100\u2013109.</li> <li>Some of them were acknowledged.</li> <li>Some were still in-flight (not acknowledged yet).</li> </ul> <p>Now broker A goes down, broker B becomes the leader.</p> <p>The question is:</p> <p>How does broker B know what the last sequence number was for this producer, so it can continue correctly and not write duplicates?</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#3-the-challenge","title":"3. The challenge","text":"<ul> <li>Producer\u2019s idempotence relies on <code>(ProducerID, Partition, Sequence)</code> tracking.</li> <li>The old leader (broker A) kept an in-memory table tracking this information:</li> </ul> <p><pre><code>PID 1234 \u2192 last sequence 109\n</code></pre> * But broker A just failed.   The new leader (broker B) doesn\u2019t have that in memory.</p> <p>So, how does broker B know where to continue?</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#4-the-solution-leader-epoch-and-log-recovery","title":"4. The solution \u2014 Leader Epoch and Log Recovery","text":"<p>When a new leader is elected, it does log recovery based on leader epochs.</p> <p>Let\u2019s unpack these two important ideas.</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#a-leader-epoch","title":"a. Leader Epoch","text":"<p>Each time a new leader is elected for a partition, Kafka increments the leader epoch (a small integer counter).</p> <p>This number identifies \u201cwho was leader when this data was written\u201d.</p> <p>Example:</p> Epoch Leader Broker Description 0 Broker A Original leader 1 Broker B After re-election 2 Broker C Next re-election <p>Every log entry on disk includes its leader epoch, so when replicas sync, they can tell which messages were written by which leader.</p> <p>This is part of the batch header:</p> <pre><code>LeaderEpoch: 7\n</code></pre>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#b-log-recovery-and-truncation","title":"b. Log Recovery and Truncation","text":"<p>When a new leader (say broker B) takes over, it:</p> <ol> <li>Loads the partition log from disk.</li> <li>Uses the last known committed offsets and leader epochs to truncate any uncommitted or divergent messages.</li> <li>Ensures the log is consistent with the rest of the in-sync replicas.</li> </ol> <p>So, when broker B becomes leader, it knows exactly which messages are truly committed.</p> <p>This guarantees that the new leader\u2019s log is identical to what all in-sync replicas have.</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#5-producer-recovery-after-leader-failover","title":"5. Producer recovery after leader failover","text":"<p>Now, back to the producer.</p> <p>After broker A fails:</p> <ol> <li>The producer keeps retrying (because <code>acks=all</code> or <code>retries &gt; 0</code>).</li> <li>It gets a NotLeaderForPartition error from a broker (meaning \u201cthe leader changed\u201d).</li> <li>The producer then fetches new metadata from Kafka and learns that broker B is now the leader.</li> <li>It resends its next message batches to broker B.</li> </ol>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#6-how-the-new-leader-validates-sequence-numbers","title":"6. How the new leader validates sequence numbers","text":"<p>Broker B (the new leader) looks at the incoming batch:</p> <ul> <li><code>(PID=1234, seq=100\u2013109, epoch=E1)</code></li> </ul> <p>Broker B maintains producer state on disk, stored inside the partition\u2019s log. When it became leader, it rebuilt this state from the log segments (the \u201cproducer state snapshot\u201d).</p> <p>So it knows the latest acknowledged sequence for each producer.</p> <p>Example:</p> <pre><code>PID=1234, lastSeq=109, epoch=E1\n</code></pre> <p>Now if the producer retries an old batch (sequence 100\u2013109), broker B detects:</p> <p>\u201cThese sequence numbers are &lt;= last seen \u2192 duplicates \u2192 ignore.\u201d</p> <p>If the producer sends a new batch (seq=110\u2013119), broker B appends them as expected.</p> <p>\u2705 Result:</p> <ul> <li>Duplicates avoided</li> <li>Order preserved</li> <li>Producer resumes seamlessly after leader re-election</li> </ul>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#7-what-if-the-producer-itself-restarts-after-failover","title":"7. What if the producer itself restarts after failover?","text":"<p>If the producer crashes and restarts, it loses its local sequence counters.</p> <p>That\u2019s where the producer epoch comes in.</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#producer-epoch-version-number-for-the-producer-session","title":"Producer Epoch = version number for the producer session.","text":"<p>Each time a producer with the same <code>transactional.id</code> restarts:</p> <ul> <li>The transaction coordinator assigns it a new producer epoch.</li> <li>The new epoch signals to brokers that this is a new session.</li> </ul> <p>Brokers then:</p> <ul> <li>Accept messages from the new epoch,</li> <li>Reject any late or duplicate messages from old epochs.</li> </ul> <p>So, if an old batch (epoch=1) arrives after a restart (new epoch=2), the broker discards it.</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#8-how-all-these-pieces-fit-together","title":"8. How all these pieces fit together","text":"Mechanism Controlled by Purpose Producer ID (PID) Broker assigns Identifies producer instance Sequence numbers Producer assigns Ordered numbering of messages per partition Producer epoch Broker increments Distinguishes new producer session after restart Leader epoch Broker increments per partition Distinguishes new partition leader during re-election Producer state snapshot Broker (on disk) Tracks last sequence for each PID Log truncation New leader broker Removes uncommitted data from previous leader Metadata refresh Producer Learns new leader after failover <p>Together, these ensure that idempotence and exactly-once semantics remain intact even across leader failures.</p>"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#9-step-by-step-eos-through-leader-failover","title":"9. Step-by-step: EOS through leader failover","text":"<p>Let\u2019s walk through a real example.</p> Step Action What Happens 1 Broker A is leader Producer sends (PID=1234, seq=100\u2013109) 2 Broker A crashes mid-write Some messages written, some pending 3 Broker B becomes leader Rebuilds state, truncates uncommitted data 4 Producer retries batch Still uses (PID=1234, seq=100\u2013109) 5 Broker B checks snapshot Sees duplicates \u2192 ignores 6 Producer sends new batch (seq=110\u2013119) Broker B accepts and appends \u2705 Result No duplicates, no lost messages, consistent order"},{"location":"scenarios/kafka/03-What_Happens_When_ReElection_Happens/#10-summary-kafkas-idempotence-leader-epochs","title":"10. Summary: Kafka\u2019s idempotence + leader epochs","text":"Concept Description Role in EOS Producer ID (PID) Unique ID per producer Identify source of messages Sequence Number Increment per partition Detect duplicates, maintain order Producer Epoch Increment per restart Ignore stale messages after restart Leader Epoch Increment per leader re-election Ensure log consistency after failover Producer State Snapshot Stored on broker Recover last sequence after failover Retry behavior Retains same sequence for same batch Prevent duplicates across retries or failovers <p>\u2705 In short:</p> <ul> <li>Offsets are still assigned by the broker.</li> <li>Sequence numbers are assigned by the producer.</li> <li>When the leader changes, the new leader rebuilds its producer state from the log and uses the leader epoch to stay consistent.</li> <li>If the producer retries the same batch, the sequence numbers remain the same, and the new leader detects duplicates and discards them.</li> <li>Producer epoch protects against old messages from previous producer sessions.</li> </ul> <p>Together, this combination of PID, sequence numbers, producer epoch, and leader epoch allows Kafka to maintain exactly-once semantics \u2014 even across broker failovers and leader re-elections.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/","title":"Give me a walkthrough of the Leader Epoch and How it helps in Log Truncation?","text":""},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#1-the-problem-these-concepts-solve","title":"\ud83e\uddf1 1. The problem these concepts solve","text":"<p>Imagine this scenario:</p> <ul> <li>You have a topic with replication factor = 3.</li> <li> <p>So for each partition, you have:</p> </li> <li> <p>1 leader replica (accepts writes)</p> </li> <li>2 follower replicas (replicate from the leader)</li> </ul> <p>Let\u2019s say the partition\u2019s replicas are on brokers:</p> <pre><code>Leader \u2192 Broker A\nFollowers \u2192 Broker B, Broker C\n</code></pre> <p>Now the producer sends messages to Broker A (the leader).</p> <p>The followers (B and C) fetch those messages from A to stay in sync.</p> <p>Everything is fine \u2014 until something goes wrong.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#2-the-failure-scenario","title":"\u26a0\ufe0f 2. The failure scenario","text":"<p>Let\u2019s say the producer sends three messages:</p> <pre><code>M1, M2, M3\n</code></pre> <p>and the replication happens like this:</p> Message Broker A (Leader) Broker B (Follower) Broker C (Follower) M1 \u2705 written \u2705 replicated \u2705 replicated M2 \u2705 written \u2705 replicated \u274c not yet replicated M3 \u2705 written \u274c not yet replicated \u274c not yet replicated <p>Then suddenly:</p> <p>Broker A (the leader) crashes.</p> <p>Kafka controller needs to pick a new leader.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#3-what-happens-next-leader-election","title":"3. What happens next \u2014 leader election","text":"<p>Kafka now elects one of the followers as the new leader.</p> <p>Say Broker B becomes the new leader.</p> <p>But look carefully at the data table above:</p> Message Broker A Broker B M1 \u2705 \u2705 M2 \u2705 \u2705 M3 \u2705 \u274c <p>\u2192 Broker A had M3, but Broker B does not.</p> <p>This means Broker B\u2019s log is shorter \u2014 it\u2019s missing M3.</p> <p>Now we have divergence:</p> <ul> <li>Broker A\u2019s log = <code>[M1, M2, M3]</code></li> <li>Broker B\u2019s log = <code>[M1, M2]</code></li> </ul> <p>So\u2026 \ud83d\udc49 What happens when Broker A comes back up?</p> <p>It still thinks it\u2019s got the \u201clatest data\u201d (including M3). But the cluster\u2019s new truth (after failover) says:</p> <p>The partition leader is now B, and the official log ends at M2.</p> <p>If we don\u2019t fix this, clients could read inconsistent data (some see M3, some don\u2019t). That\u2019s a consistency violation.</p> <p>Kafka solves this using two key mechanisms:</p> <ul> <li>Leader epochs</li> <li>Log truncation</li> </ul>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#4-what-is-a-leader-epoch","title":"\ud83d\udd22 4. What is a \u201cLeader Epoch\u201d?","text":"<p>A leader epoch is a number that Kafka increments every time a new leader is elected for a partition.</p> <p>You can think of it like a \u201cversion number\u201d for leadership of a partition.</p> <p>Example:</p> Leader Epoch Leader Broker Description 0 Broker A First leader 1 Broker B After A crashes 2 Broker A If A later becomes leader again <p>Each message that\u2019s written to the log is tagged with the current leader epoch.</p> <p>So if Broker A (epoch 0) wrote M3, the message might be represented as:</p> <pre><code>&lt;M3, epoch=0, offset=2&gt;\n</code></pre> <p>When Broker B becomes the leader, it moves to:</p> <pre><code>Leader epoch = 1\n</code></pre> <p>Now any new messages written by B will have <code>epoch=1</code>.</p> <p>So if Broker B writes a new message M4:</p> <pre><code>&lt;M4, epoch=1, offset=3&gt;\n</code></pre> <p>That\u2019s how Kafka can tell which leader wrote which messages.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#5-what-is-log-truncation","title":"\u2702\ufe0f 5. What is \u201cLog Truncation\u201d?","text":"<p>When Broker A (the old leader) comes back online, it needs to catch up with the current leader (Broker B).</p> <p>But remember \u2014 A still has M3 (epoch 0) that B never had.</p> <p>Now the cluster says \u201cBroker B\u2019s log is the source of truth\u201d (epoch 1).</p> <p>When Broker A connects to Broker B to sync again, it compares its log with Broker B\u2019s.</p> <p>They match up to offset 1 (M1, M2), but Broker A has extra data (M3) that B doesn\u2019t have \u2014 and B\u2019s epoch is newer.</p> <p>So Kafka makes Broker A truncate (delete) M3 from its log, because it was written by an old leader epoch that is no longer valid.</p> <p>\ud83d\udc49 This deletion is called log truncation.</p> <p>After truncation:</p> <ul> <li>Broker A\u2019s log = <code>[M1, M2]</code></li> <li>Broker B\u2019s log = <code>[M1, M2]</code>   \u2192 Both are consistent again.</li> </ul>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#6-step-by-step-example-with-epochs-and-truncation","title":"\ud83e\udded 6. Step-by-step example with epochs and truncation","text":"Step Action Leader Epoch Leader Log State 1 Start 0 Broker A A: [ ] 2 Producer sends M1, M2, M3 0 Broker A A: [M1, M2, M3] 3 Followers replicate partially 0 Broker A B: [M1, M2] 4 Broker A crashes \u2014 \u2014 \u2014 5 Broker B becomes leader 1 Broker B B: [M1, M2] 6 Producer sends M4 1 Broker B B: [M1, M2, M4] 7 Broker A recovers \u2014 Broker A A: [M1, M2, M3] 8 Broker A compares logs with leader (B) \u2014 \u2014 A has M3 from older epoch 9 Kafka detects conflict (epoch mismatch) \u2014 \u2014 M3 (epoch 0) invalid 10 Broker A truncates M3 \u2014 \u2014 A: [M1, M2, M4] after sync <p>Now the logs are consistent again.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#7-how-kafka-knows-where-to-truncate","title":"\u2699\ufe0f 7. How Kafka knows where to truncate","text":"<p>Each broker maintains a small internal map called the Leader Epoch Cache, which records:</p> <pre><code>(epoch \u2192 start_offset)\n</code></pre> <p>Example:</p> <pre><code>Epoch 0 \u2192 starts at offset 0\nEpoch 1 \u2192 starts at offset 3\n</code></pre> <p>When a follower detects it has messages beyond the current leader\u2019s epoch range, it uses this cache to know exactly where the new leader\u2019s log begins \u2014 and truncates everything after that.</p>"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#8-why-this-is-critical-for-consistency","title":"\ud83d\udd10 8. Why this is critical for consistency","text":"<p>This combination ensures Kafka\u2019s data consistency and durability guarantees:</p> Guarantee Mechanism No old leader can reintroduce stale messages Leader epochs prevent writing with outdated epochs All replicas converge to same log Log truncation ensures followers delete invalid data Clients never see \u201cghost messages\u201d (like M3 above) New leader only serves committed data Exactly-once semantics survive failover Idempotent producers rely on consistent logs"},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#9-summary-leader-epoch-vs-log-truncation","title":"\ud83e\udde9 9. Summary \u2014 Leader Epoch vs Log Truncation","text":"Concept Description Purpose Leader Epoch Incremented every time a new leader is elected for a partition. Each message stores its leader epoch. Identifies which leader wrote which messages; helps detect divergent logs. Log Truncation Process where a replica deletes messages written by an old leader that don\u2019t exist in the new leader\u2019s log. Ensures consistency between replicas after failover."},{"location":"scenarios/kafka/04-Give_Walkthrough_Of_Leader_Epoch_Log_Truncation/#10-in-simple-words","title":"\u2705 10. In simple words","text":"<ul> <li> <p>A leader epoch is a \u201cgeneration number\u201d for each partition leader.   Every new leader = new epoch.</p> </li> <li> <p>A log truncation is what Kafka does to remove messages that were written by a previous leader but didn\u2019t get fully replicated before the failover.</p> </li> </ul> <p>Together, they ensure that after a failover, all brokers agree on exactly which messages exist \u2014 no duplicates, no lost messages, no ghosts.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/","title":"Explain the difference between vm_dirty_ratio and vm_background_dirty_ratio","text":""},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#1-what-these-settings-are-about-the-big-picture","title":"\ud83e\udde0 1. What these settings are about \u2014 the big picture","text":"<p>When an application (like Kafka) writes data to disk, it doesn\u2019t write it directly to the physical disk each time \u2014 that would be too slow.</p> <p>Instead, Linux uses something called the page cache (a part of RAM) to hold recently written or read data temporarily.</p> <p>So:</p> <ul> <li>Kafka writes \u2192 it first goes to memory (the page cache)</li> <li>Later \u2192 the Linux kernel flushes (writes) that data from memory to disk in the background.</li> </ul> <p>These two settings:</p> <pre><code>vm.dirty_ratio\nvm.background_dirty_ratio\n</code></pre> <p>control how much of your system\u2019s memory can fill up with \"dirty\" (unflushed) pages before the kernel starts writing them to disk.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#2-what-are-dirty-pages","title":"\ud83e\udde9 2. What are \"dirty pages\"?","text":"<p>\u201cDirty pages\u201d = memory pages that contain data that\u2019s been modified but not yet written (\u201cflushed\u201d) to disk.</p> <p>Example:</p> <ul> <li>Kafka writes messages to its log segment file.</li> <li>The OS keeps those writes in memory first (as dirty pages).</li> <li>Eventually, those pages are written to disk asynchronously.</li> </ul>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#3-vmbackground_dirty_ratio-start-cleaning-soon","title":"\u2699\ufe0f 3. <code>vm.background_dirty_ratio</code> \u2014 \u201cstart cleaning soon\u201d","text":"<p>This setting tells Linux:</p> <p>\u201cWhen this percentage of total memory has dirty pages, start writing them to disk in the background.\u201d</p> <p>It\u2019s like an early warning threshold for the kernel\u2019s background flusher thread.</p> <ul> <li>The flushing happens asynchronously (in the background).</li> <li>The goal is to keep the number of dirty pages low so they don\u2019t pile up.</li> </ul> <p>Example:</p> <pre><code>vm.background_dirty_ratio = 10\n</code></pre> <p>\u2192 When 10% of your RAM is full of dirty pages, Linux starts slowly writing them out to disk.</p> <p>So, the system starts cleaning early \u2014 gently.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#4-vmdirty_ratio-stop-the-writers","title":"\u2699\ufe0f 4. <code>vm.dirty_ratio</code> \u2014 \u201cstop the writers!\u201d","text":"<p>This is the hard limit.</p> <p>It tells Linux:</p> <p>\u201cIf this percentage of total memory is full of dirty pages, stop all new writes until some are written to disk.\u201d</p> <p>At this point, applications like Kafka or MySQL will be forced to wait because the OS won\u2019t accept more dirty pages.</p> <p>Example:</p> <pre><code>vm.dirty_ratio = 20\n</code></pre> <p>\u2192 When 20% of memory is dirty, Linux will block new writes from user processes until the kernel finishes flushing enough pages.</p> <p>This is how Linux prevents the system from running out of memory.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#5-how-they-work-together","title":"\ud83e\ude9c 5. How they work together","text":"<p>You can think of these two settings like a bathtub and a drain:</p> Setting What it does Analogy <code>vm.background_dirty_ratio</code> When this much water fills the tub, start draining slowly in the background. \u201cStart draining at 10% full.\u201d <code>vm.dirty_ratio</code> If the water gets this high, stop pouring in more until it drains. \u201cStop pouring at 20% full.\u201d <p>So:</p> <ul> <li>Between 0% \u2192 10%: fine, OS just caches data in memory.</li> <li>Between 10% \u2192 20%: OS starts background flushing.</li> <li>Above 20%: OS blocks applications until flushing catches up.</li> </ul>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#6-typical-values-defaults-and-tuning","title":"\ud83e\uddf0 6. Typical values (defaults and tuning)","text":"Setting Default (Linux) Recommended for Kafka or database workloads <code>vm.dirty_ratio</code> 20 10\u201315 <code>vm.background_dirty_ratio</code> 10 5 <p>For Kafka or large I/O systems, you generally want to lower both so that:</p> <ul> <li>The OS starts flushing earlier (<code>background_dirty_ratio=5</code>)</li> <li>You never hit the hard stop (<code>dirty_ratio=10</code>\u2013<code>15</code>)</li> </ul> <p>This avoids long I/O pauses where Kafka threads get blocked waiting for the OS to flush dirty pages.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#7-real-world-impact-on-kafka","title":"\ud83d\udd0d 7. Real-world impact on Kafka","text":"<p>Kafka relies on the Linux page cache for writing logs (it doesn\u2019t fsync every message). That\u2019s why these settings matter directly for Kafka performance.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#if-ratios-are-too-high","title":"If ratios are too high:","text":"<ul> <li>Kafka writes very fast initially (everything goes to memory).</li> <li>Memory fills up with dirty pages.</li> <li>Suddenly, OS stops accepting new writes.</li> <li>Kafka\u2019s disk I/O stalls for seconds while flushing happens.   \u2192 You get high latency spikes and possible broker timeouts.</li> </ul>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#if-ratios-are-tuned-properly","title":"If ratios are tuned properly:","text":"<ul> <li>Linux flushes gradually and continuously.</li> <li>Kafka writes stay smooth and predictable.</li> <li>No sudden stalls.</li> </ul> <p>So, lowering these ratios makes Kafka more stable under heavy write loads.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#8-quick-example-with-numbers","title":"\ud83e\uddee 8. Quick example with numbers","text":"<p>Let\u2019s say your broker has 64 GB RAM.</p> Setting Value Dirty memory threshold <code>vm.background_dirty_ratio=10</code> Start flushing when 10% dirty 6.4 GB <code>vm.dirty_ratio=20</code> Block new writes at 20% dirty 12.8 GB <p>That means:</p> <ul> <li>Once 6.4 GB of writes are cached but not yet flushed \u2192 kernel starts background writes.</li> <li>If 12.8 GB pile up \u2192 all write calls will block until the kernel flushes some data.</li> </ul> <p>If you change to:</p> <pre><code>vm.background_dirty_ratio = 5\nvm.dirty_ratio = 10\n</code></pre> <p>\u2192 Flushing starts earlier (at ~3 GB) and never lets dirty pages exceed ~6 GB.</p> <p>That means smoother, steadier disk I/O and fewer Kafka pauses.</p>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#9-how-to-view-and-change-these-values","title":"\ud83e\udded 9. How to view and change these values","text":"<p>Check current values:</p> <pre><code>cat /proc/sys/vm/dirty_ratio\ncat /proc/sys/vm/background_dirty_ratio\n</code></pre> <p>Change temporarily:</p> <pre><code>sudo sysctl -w vm.dirty_ratio=10\nsudo sysctl -w vm.background_dirty_ratio=5\n</code></pre> <p>Make it permanent: Add to <code>/etc/sysctl.conf</code>:</p> <pre><code>vm.dirty_ratio=10\nvm.background_dirty_ratio=5\n</code></pre> <p>Then reload:</p> <pre><code>sudo sysctl -p\n</code></pre>"},{"location":"scenarios/kafka/05-Explain_Diff_dirtyRatio_dirtyBackgroundRatio/#10-tldr-like-youre-explaining-to-a-10-year-old","title":"\ud83e\udde0 10. TL;DR \u2014 like you\u2019re explaining to a 10-year-old","text":"<p>Imagine your computer\u2019s memory as a notebook where you jot down things to save to disk later.</p> <ul> <li><code>vm.background_dirty_ratio</code> = \u201cWhen 10 pages are filled, start copying them quietly to disk.\u201d</li> <li><code>vm.dirty_ratio</code> = \u201cIf 20 pages are filled and you\u2019re still writing, stop! Clean some up first.\u201d</li> </ul> <p>You want to start cleaning before the notebook gets full, otherwise, you\u2019ll have to stop writing until the old pages are cleared.</p> <p>\u2705 In short:</p> Setting Meaning Safe value for Kafka <code>vm.background_dirty_ratio</code> When to start background flushing dirty pages 5 <code>vm.dirty_ratio</code> When to block writes until flush completes 10\u201315 <p>Kafka benefits when these are low, because it avoids long OS-level pauses caused by massive flushes.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/","title":"Are Kafka Consumers Thread Safe","text":"<p>Kafka consumers are not thread-safe.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#1-what-thread-safety-means","title":"1. What \u201cthread safety\u201d means","text":"<p>Imagine a library.</p> <ul> <li>You have one librarian (the Kafka consumer).</li> <li>You have multiple helpers (threads).</li> <li>They all want to check out books (poll messages from Kafka).</li> </ul> <p>If all the helpers try to use the same librarian\u2019s checkout desk at the same time \u2014 things get confusing:</p> <ul> <li>Two helpers reach for the same book.</li> <li>The librarian loses track of who took what.</li> <li>Records get mixed up.</li> </ul> <p>That\u2019s what happens when multiple threads use a non-thread-safe object at the same time.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#2-kafkas-consumer-that-single-librarian","title":"2. Kafka\u2019s consumer = that single librarian","text":"<p>Kafka\u2019s consumer object (the one you create from <code>KafkaConsumer</code>) is not thread-safe.</p> <p>That means:</p> <ul> <li>Only one thread should call its methods (<code>poll()</code>, <code>commitSync()</code>, <code>close()</code>, etc.) at a time.</li> <li>If multiple threads touch it, you can get data corruption, missed messages, or weird crashes.</li> </ul>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#3-why-kafkaconsumer-is-not-thread-safe","title":"3. Why KafkaConsumer is not thread-safe","text":"<p>Kafka\u2019s consumer manages a lot of state internally \u2014 things like:</p> <ul> <li>The list of partitions it owns.</li> <li>The last offsets it read.</li> <li>The last committed offsets.</li> <li>Heartbeats (to the group coordinator).</li> <li>The network connection to the broker.</li> </ul> <p>All of that is stored inside the consumer object, not in Kafka itself.</p> <p>If two threads try to change that state at once:</p> <ul> <li>One might be polling new messages.</li> <li>Another might be committing offsets.</li> <li>Another might be closing the consumer.</li> </ul> <p>Result:</p> <ul> <li>Messages get processed twice or skipped.</li> <li>Kafka thinks the consumer \u201cdied\u201d (missed heartbeats).</li> <li>You might even get a <code>ConcurrentModificationException</code>.</li> </ul> <p>So Kafka\u2019s design keeps things simple:</p> <p>\u201cOne consumer instance = one thread.\u201d</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#4-what-happens-if-you-ignore-this-rule","title":"4. What happens if you ignore this rule","text":"<p>If multiple threads share a single <code>KafkaConsumer</code>, you can see things like:</p> <ul> <li><code>ConcurrentModificationException</code></li> <li><code>IllegalStateException: Consumer is not subscribed to any topics</code></li> <li><code>CommitFailedException</code></li> <li>Random missing messages</li> <li>Consumer group instability (constant rebalances)</li> </ul> <p>All of these are signs that more than one thread is calling the consumer\u2019s methods.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#5-the-correct-way-one-consumer-per-thread","title":"5. The correct way: one consumer per thread","text":"<p>Back to our library analogy.</p> <p>If you have 3 helpers who each want to check out books, the right way is:</p> <ul> <li>Give each helper their own librarian (their own KafkaConsumer).</li> <li>Each librarian works independently.</li> <li>Each one has their own checkout desk and logbook.</li> </ul> <p>That way, there\u2019s no confusion.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#in-kafka-terms","title":"In Kafka terms:","text":"<p>If you want multiple threads to read from Kafka, do this:</p> <pre><code>One thread \u2192 One KafkaConsumer instance \u2192 Some partitions\n</code></pre> <p>Kafka will automatically balance partitions across consumers in the same group.</p> <p>So if you have:</p> <ul> <li>6 partitions</li> <li>3 consumer threads (each with its own consumer)   \u2192 Each thread gets 2 partitions.</li> </ul>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#6-how-kafka-ensures-each-thread-gets-a-fair-share","title":"6. How Kafka ensures each thread gets a fair share","text":"<p>When you create multiple consumers in the same consumer group, Kafka\u2019s group coordinator does the balancing for you.</p> <p>Example:</p> <pre><code>Topic: orders\nPartitions: P0, P1, P2, P3, P4, P5\n</code></pre> Thread Consumer Assigned Partitions Thread-1 Consumer-1 P0, P1 Thread-2 Consumer-2 P2, P3 Thread-3 Consumer-3 P4, P5 <p>Each consumer reads only its partitions. If one thread stops, Kafka reassigns its partitions to others.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#7-but-what-if-you-want-multiple-threads-processing-messages","title":"7. But what if you want multiple threads processing messages?","text":"<p>Ah \u2014 this is the common tricky part.</p> <p>Let\u2019s say you want one thread to poll messages, but multiple worker threads to process the messages in parallel.</p> <p>That\u2019s fine \u2014 as long as only one thread is calling <code>poll()</code> and <code>commit()</code>.</p> <p>Here\u2019s how you do it safely:</p> <ol> <li>One thread runs the consumer (polls from Kafka).</li> <li>It hands the fetched messages (records) to a thread pool (e.g., ExecutorService).</li> <li>Worker threads process those messages.</li> <li>Once the workers finish, the consumer thread commits offsets.</li> </ol> <p>That\u2019s thread-safe because:</p> <ul> <li>Only one thread talks to Kafka.</li> <li>Worker threads handle business logic separately.</li> </ul>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#8-example-flow-kid-analogy","title":"8. Example flow (kid analogy)","text":"<p>Imagine:</p> <ul> <li>One librarian (KafkaConsumer)</li> <li>Many book readers (worker threads)</li> <li>The librarian checks out stacks of books to readers (polls records)</li> <li>Readers read the books (process messages)</li> <li>When everyone finishes, the librarian marks the books as \u201cdone\u201d (commit offsets)</li> </ul> <p>That\u2019s how you keep order.</p> <p>But if readers start trying to check out or return books directly \u2014 chaos!</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#9-summary-of-best-practices","title":"9. Summary of best practices","text":"Goal Safe approach Want to read messages faster? Use multiple consumer threads, each with its own <code>KafkaConsumer</code> (same group). Want to process messages in parallel? Use one consumer thread + worker thread pool for processing. Never do this Share one <code>KafkaConsumer</code> across threads. OK to do this Share one <code>KafkaProducer</code> across threads (it is thread-safe)."},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#10-deep-analogy-summary","title":"10. Deep analogy summary","text":"Concept Analogy Rule <code>KafkaConsumer</code> A librarian with a logbook Only one person (thread) should use it Partitions Juice boxes or book piles Each consumer thread gets its own Group rebalancing The teacher redistributing books Kafka automatically does it Poll loop Librarian giving out new books Only librarian handles borrowing/returning Worker threads Readers Can read (process), but don\u2019t talk to the librarian"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#11-key-takeaway","title":"11. Key takeaway","text":"<p>Kafka consumers are not thread-safe, because they keep a lot of internal state that would break if multiple threads accessed it at once.</p> <p>So you must choose one of two patterns:</p> <ol> <li> <p>Multi-consumer pattern:</p> </li> <li> <p>One thread per consumer.</p> </li> <li> <p>Each consumer has its own partitions.</p> </li> <li> <p>Single-consumer + worker pool pattern:</p> </li> <li> <p>One thread polls Kafka.</p> </li> <li>Worker threads process data concurrently.</li> <li>The consumer thread alone commits offsets.</li> </ol> <p>Both work well \u2014 just don\u2019t mix them up.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#in-one-line","title":"In one line:","text":"<p>Treat your KafkaConsumer like a delicate machine \u2014 only one person should operate it at a time. Others can help process the output, but no one else touches the controls.</p> <p>From a single consumer group, you can have multiple consumers, each running in its own thread, but you cannot share one consumer instance across multiple threads.</p> <p>Let\u2019s restate and unpack this so it sticks.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#1-consumer-group-the-whole-team","title":"\ud83e\uddf1 1. Consumer group = the whole team","text":"<p>Think of a consumer group as a team of workers (consumers) all reading from the same topic together. Each worker (consumer) gets assigned a unique subset of the topic\u2019s partitions.</p> <p>So if you have:</p> <ul> <li>A topic with 6 partitions</li> <li>And 3 consumers in the same group   \u2192 Each one will read from 2 partitions.</li> </ul>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#2-each-consumer-one-worker","title":"\ud83e\uddcd 2. Each consumer = one worker","text":"<p>Each consumer instance (the <code>KafkaConsumer</code> object) manages:</p> <ul> <li>Its own partitions</li> <li>Its own offset tracking</li> <li>Its own heartbeat with the group coordinator</li> </ul> <p>That means every consumer needs to have:</p> <ul> <li>Its own independent thread of control (poll loop)</li> <li>Its own internal state and connection to the broker</li> </ul> <p>So:</p> <p>1 consumer = 1 thread = 1 partition subset</p> <p>That\u2019s the safe pattern.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#3-what-you-cannot-do","title":"\ud83d\udeab 3. What you cannot do","text":"<p>You cannot have multiple threads call methods (like <code>poll()</code>, <code>commitSync()</code>, etc.) on the same <code>KafkaConsumer</code> instance.</p> <p>Because:</p> <ul> <li>KafkaConsumer is not thread-safe</li> <li>Its internal state will get corrupted</li> <li>Kafka might throw exceptions like:</li> </ul> <pre><code>IllegalStateException: Consumer is not subscribed to any topics\n</code></pre> <p>So this is illegal:</p> <pre><code>KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n\n// WRONG \u2014 two threads sharing one consumer\nnew Thread(() -&gt; pollLoop(consumer)).start();\nnew Thread(() -&gt; commitLoop(consumer)).start();\n</code></pre> <p>\u2192 both threads talk to the same consumer instance.</p> <p>Kafka does not allow that.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#4-what-you-can-do-safely","title":"\u2705 4. What you can do safely","text":"<p>You can have multiple consumers (each with their own instance) running in the same consumer group, each on a different thread.</p> <p>Example:</p> <pre><code>for (int i = 0; i &lt; numConsumers; i++) {\n    new Thread(new ConsumerRunnable(groupId, topics)).start();\n}\n</code></pre> <p>Where <code>ConsumerRunnable</code> creates its own KafkaConsumer:</p> <pre><code>public void run() {\n    KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n    consumer.subscribe(topics);\n    while (true) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n        // process records\n    }\n}\n</code></pre> <p>Here:</p> <ul> <li>Each thread has its own consumer.</li> <li>All consumers belong to the same group.</li> <li>Kafka automatically distributes partitions between them.</li> </ul> <p>This is perfectly safe and the recommended approach.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#5-another-safe-pattern-one-consumer-multiple-worker-threads","title":"\u2699\ufe0f 5. Another safe pattern: one consumer, multiple worker threads","text":"<p>This is also safe \u2014 and often used when you have more partitions than threads, or you want tight control.</p> <p>Pattern:</p> <ul> <li>One thread runs the KafkaConsumer (polls records)</li> <li>It puts records into a queue</li> <li>Worker threads pick up the messages and process them concurrently</li> </ul> <p>Example:</p> <pre><code>KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\nExecutorService workers = Executors.newFixedThreadPool(5);\n\nwhile (true) {\n    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n    for (ConsumerRecord&lt;String, String&gt; record : records) {\n        workers.submit(() -&gt; processRecord(record));\n    }\n}\n</code></pre> <p>Here:</p> <ul> <li>Only one thread touches the consumer.</li> <li>Worker threads handle processing.</li> <li>Safe and efficient.</li> </ul>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#6-summary-whats-safe-and-whats-not","title":"\ud83e\udde0 6. Summary \u2014 what\u2019s safe and what\u2019s not","text":"Scenario Safe? Explanation One consumer, one thread \u2705 The standard pattern. One consumer, multiple threads \u274c Not thread-safe. Causes errors or corruption. Multiple consumers, one thread \u274c Pointless \u2014 only one can call <code>poll()</code>. Others are idle. Multiple consumers, multiple threads \u2705 The correct multi-threaded model for one group. One consumer thread + worker threads for processing \u2705 Only one thread polls, others process."},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#7-why-multiple-consumers-in-the-same-thread-doesnt-make-sense","title":"\ud83d\udd0d 7. Why \u201cmultiple consumers in the same thread\u201d doesn\u2019t make sense","text":"<p>Technically, you can create multiple <code>KafkaConsumer</code> instances in one thread, but only one can be actively polling at a time.</p> <p>Kafka\u2019s design assumes each consumer has a dedicated polling loop, so having multiple consumers in one thread gives you no benefit \u2014 it just complicates your code.</p> <p>That\u2019s why we always say:</p> <p>One thread \u2192 one consumer.</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#final-rule-of-thumb","title":"\u2705 Final rule of thumb","text":"<p>If you remember just one sentence, make it this:</p> <p>Each KafkaConsumer must be used by exactly one thread \u2014 but a consumer group can contain many consumers (each in its own thread).</p>"},{"location":"scenarios/kafka/06-Are_Kafka_Consumers_Thread_Safe/#analogy","title":"Analogy","text":"<p>Imagine:</p> <ul> <li>A consumer group is a team of librarians working in one big library.</li> <li>Each consumer is a librarian.</li> <li>Each thread is the librarian\u2019s brain that controls what they do.</li> </ul> <p>If two librarians share one brain (same thread) \u2014 one of them does nothing. If one librarian has two brains (multiple threads) \u2014 chaos. Each librarian needs exactly one brain.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/","title":"Is retention defined at topic level or partition level?","text":""},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#short-answer","title":"Short Answer","text":"<p>No, <code>retention.ms</code> is not defined at the partition level. It is defined at the topic level, and applies uniformly to all partitions of that topic.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#detailed-explanation","title":"Detailed Explanation","text":""},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#1-configuration-levels-in-kafka","title":"1. Configuration levels in Kafka","text":"<p>Kafka configuration settings can exist at different scopes:</p> Scope Examples Description Broker-level <code>log.retention.hours</code>, <code>log.segment.bytes</code> Default values that apply to all topics on that broker (unless overridden). Topic-level <code>retention.ms</code>, <code>cleanup.policy</code>, <code>segment.bytes</code> Specific to one topic; overrides the broker defaults. Partition-level \u2014 Kafka does not allow per-partition settings for retention or compaction. <p>So <code>retention.ms</code> sits at the topic level, not per partition.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#2-how-it-applies-to-partitions","title":"2. How it applies to partitions","text":"<p>Every topic in Kafka is divided into partitions. Each partition has its own log directory on disk. The broker applies the topic\u2019s retention rules to each partition\u2019s log independently.</p> <p>So while there is no separate configuration per partition, the effect of <code>retention.ms</code> is enforced per partition.</p> <p>That means:</p> <ul> <li>Each partition\u2019s log cleaner or retention manager will check the modification timestamps of its segments,</li> <li>And delete (or compact) them according to the topic\u2019s <code>retention.ms</code> rule.</li> </ul> <p>But the rule itself comes from the topic configuration, not the partition.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#3-example","title":"3. Example","text":"<p>Suppose:</p> <pre><code># Topic-level config\nretention.ms = 604800000   # 7 days\n</code></pre> <p>and your topic has 3 partitions: <code>p0</code>, <code>p1</code>, <code>p2</code>.</p> <p>Then:</p> <ul> <li>Each partition (<code>p0</code>, <code>p1</code>, <code>p2</code>) will keep messages for up to 7 days,</li> <li>After that, old segments will be deleted (for delete-based cleanup) or eligible for compaction (if <code>cleanup.policy=compact</code>).</li> </ul> <p>If you wanted one partition to keep messages for 30 days and another for 7, Kafka doesn\u2019t support that \u2014 you\u2019d have to create two separate topics.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#4-where-its-stored","title":"4. Where it\u2019s stored","text":"<p>You can verify topic-level retention settings using:</p> <pre><code>kafka-configs.sh --bootstrap-server &lt;broker&gt; --describe --topic &lt;topic-name&gt;\n</code></pre> <p>You\u2019ll see something like:</p> <pre><code>Topic: my-topic  Configs: retention.ms=604800000, cleanup.policy=delete\n</code></pre> <p>There\u2019s no command or metadata that shows a per-partition <code>retention.ms</code> value, because it simply doesn\u2019t exist.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#5-related-parameters","title":"5. Related parameters","text":"Parameter Level Description <code>retention.ms</code> Topic How long messages are retained before deletion. <code>retention.bytes</code> Topic Max log size before old segments are removed. <code>log.retention.ms</code> Broker Default <code>retention.ms</code> for topics without explicit configuration. <code>log.retention.bytes</code> Broker Default max size for topics without explicit configuration. <p>So \u2014 broker-level values are defaults, topic-level values override them, and partitions follow the topic\u2019s settings.</p>"},{"location":"scenarios/kafka/07-Is_Retention_ms_Defined_Partition_Level/#6-analogy","title":"6. Analogy","text":"<p>Imagine a school (Kafka broker) with classes (topics) and students (partitions):</p> <ul> <li> <p>The school sets a default rule:   \u201cHomework will be kept for 7 days.\u201d</p> </li> <li> <p>A teacher (topic) can override this rule:   \u201cIn my class, homework is kept for 10 days.\u201d</p> </li> <li> <p>Each student\u2019s notebook (partition) follows that rule independently,   but they can\u2019t have different rules within the same class.   Every student in that class follows the same \u201ckeep homework for 10 days\u201d policy.</p> </li> </ul> <p>In short:</p> <ul> <li>\u274c You cannot define <code>retention.ms</code> at partition level.</li> <li>\u2705 It\u2019s a topic-level property, applied independently to each partition\u2019s data.</li> </ul>"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/","title":"What is the difference between Sticky Assignor and Cooperative Sticky Assignor?","text":""},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#stickyassignor-vs-cooperativestickyassignor","title":"StickyAssignor vs CooperativeStickyAssignor","text":""},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#1-conceptual-overview","title":"1. Conceptual Overview","text":"<p>Kafka uses assignors to decide which consumer in a group will read from which partitions. When group membership changes (a consumer joins or leaves), Kafka performs a rebalance, redistributing partitions among consumers.</p> <p>Two common assignors are:</p> Assignor Type Goal Rebalance Style <code>StickyAssignor</code> Eager Minimize partition movement (keep assignments stable) Eager (stop-the-world) <code>CooperativeStickyAssignor</code> Cooperative Same goal, but with smoother rebalances Incremental (cooperative)"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#2-how-they-work","title":"2. How They Work","text":""},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#a-stickyassignor","title":"a) StickyAssignor","text":"<ul> <li>Purpose: Keeps partition assignments as stable as possible across rebalances.</li> <li> <p>Rebalance type: Eager.</p> </li> <li> <p>All consumers in the group must stop processing.</p> </li> <li>Every partition is revoked from all consumers.</li> <li>After that, partitions are reassigned \u2014 possibly to the same consumers as before, but only after all were stopped.</li> <li>Drawback: Causes noticeable pauses in consumption because it\u2019s a stop-the-world rebalance.</li> </ul> <p>Example: Assume three consumers: C1, C2, and C3, each consuming two partitions. If C3 leaves, Kafka revokes all partitions from all consumers, pauses processing, and then redistributes all six partitions. Even if C1 and C2 get back the same partitions, there was a full pause.</p>"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#b-cooperativestickyassignor","title":"b) CooperativeStickyAssignor","text":"<ul> <li>Purpose: Same stickiness as <code>StickyAssignor</code>, but rebalances happen more smoothly.</li> <li> <p>Rebalance type: Incremental or cooperative.</p> </li> <li> <p>Only the partitions that must move are revoked.</p> </li> <li>Other partitions stay assigned, so consumers can keep processing most of their workload.</li> <li>Benefit: Dramatically reduces pause time and improves overall availability during rebalances.</li> </ul> <p>Example: With the same setup (C1, C2, C3), if C3 leaves, Kafka only reassigns the two partitions that C3 owned. C1 and C2 continue processing their existing partitions without interruption.</p>"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#3-analogy","title":"3. Analogy","text":"<p>Imagine three kids sharing toy cars.</p> <p>StickyAssignor: When one kid leaves, the teacher says: \u201cAll of you put your cars back in the box. Now we\u2019ll redistribute them.\u201d Everyone must stop playing, even if they get back the same cars.</p> <p>CooperativeStickyAssignor: When one kid leaves, the teacher says: \u201cOnly take the cars from the kid who left. Everyone else can keep theirs.\u201d The game continues almost uninterrupted.</p>"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#4-configuration","title":"4. Configuration","text":"<p>You choose the assignor using the <code>partition.assignment.strategy</code> property in your consumer configuration.</p> <pre><code># Eager rebalancing\npartition.assignment.strategy=org.apache.kafka.clients.consumer.StickyAssignor\n\n# Incremental rebalancing (recommended)\npartition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor\n</code></pre> <p>Modern Kafka clients (version 2.4 and later) support <code>CooperativeStickyAssignor</code>, and it is the preferred choice because it minimizes rebalance interruptions.</p>"},{"location":"scenarios/kafka/08-Difference_Btwn_Sticky_Cooperative_Sticky_Assignor/#5-summary-table","title":"5. Summary Table","text":"Feature StickyAssignor CooperativeStickyAssignor Goal Keep assignments stable Same goal Rebalance Type Eager (stop-the-world) Incremental (cooperative) Partition Revocation All partitions Only those that need reassignment Processing Pause High Minimal Introduced In Kafka 0.11 Kafka 2.4 Recommended For Legacy setups Most modern deployments"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/","title":"How does Kafka Ensure Partial Idempotence?","text":""},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#1-the-problem-it-solves-duplicate-messages-during-retries","title":"1. The problem it solves \u2014 duplicate messages during retries","text":"<p>Without idempotence, a Kafka producer can accidentally send the same message multiple times under certain failure conditions.</p> <p>Here\u2019s a common scenario:</p> <ol> <li>The producer sends a batch to the broker.</li> <li>The broker successfully writes it to the log.</li> <li>The acknowledgment from the broker to the producer is lost (e.g., due to a network glitch).</li> <li>The producer assumes the send failed and retries the message.</li> <li>The broker accepts the retry as a new message and writes it again.</li> </ol> <p>Result: The same message is stored twice, with two different offsets. Consumers downstream will see the record twice, which can cause problems in financial systems, metrics aggregation, and any system requiring strict consistency.</p> <p>That\u2019s where idempotence comes in.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#2-what-idempotence-means","title":"2. What idempotence means","text":"<p>The term idempotent means that performing the same operation multiple times yields the same result as performing it once.</p> <p>In Kafka:</p> <p>Enabling idempotence ensures that even if a producer retries the same message, the broker will only commit it once, never creating duplicates.</p> <p>So, no matter how many times the producer resends due to transient failures, the broker will de-duplicate it automatically.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#3-how-it-works-internally","title":"3. How it works internally","text":"<p>When <code>enable.idempotence=true</code>, Kafka activates the Idempotent Producer Protocol, introduced in Kafka 0.11.0.</p> <p>Here\u2019s what happens under the hood:</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#step-1-assigning-a-unique-producer-id-pid","title":"Step 1: Assigning a unique Producer ID (PID)","text":"<ul> <li>When a producer connects to the cluster, the Kafka broker assigns it a unique 64-bit Producer ID (PID).</li> <li>This PID identifies that producer session.</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#step-2-sequence-numbers-per-partition","title":"Step 2: Sequence numbers per partition","text":"<ul> <li>Each time the producer sends a batch to a partition, it includes a monotonically increasing sequence number for that partition.</li> <li> <p>Example:</p> </li> <li> <p>Partition 0 \u2192 sequence numbers 1, 2, 3, ...</p> </li> <li>Partition 1 \u2192 sequence numbers 1, 2, 3, ... (separate sequence per partition)</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#step-3-deduplication-on-the-broker","title":"Step 3: Deduplication on the broker","text":"<ul> <li> <p>When the broker receives a batch, it checks:</p> </li> <li> <p>The Producer ID</p> </li> <li>The sequence number</li> <li>If it has already seen that sequence number from the same producer, it recognizes the message as a duplicate and discards it.</li> </ul> <p>This means that if the producer retries the same batch (after a timeout, network error, or leader election), Kafka guarantees exactly one write to the topic log.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#4-key-configuration-interactions","title":"4. Key configuration interactions","text":"<p>When <code>enable.idempotence=true</code>, Kafka automatically adjusts certain other producer parameters to safe defaults:</p> Setting Automatically adjusted value Purpose <code>acks</code> <code>all</code> Waits for acknowledgment from all in-sync replicas. <code>retries</code> <code>Integer.MAX_VALUE</code> Retries indefinitely until success or timeout. <code>max.in.flight.requests.per.connection</code> \u2264 5 Limits concurrent in-flight requests to maintain ordering guarantees. <p>If you set conflicting configurations manually (for example, <code>acks=1</code> with idempotence), Kafka will override them or throw a configuration error.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#5-what-idempotence-guarantees-and-what-it-doesnt","title":"5. What idempotence guarantees (and what it doesn\u2019t)","text":"Property Guarantee with <code>enable.idempotence=true</code> No duplicates (producer \u2192 broker) \u2705 Guaranteed (no matter how many retries occur). Preserved ordering per partition \u2705 Guaranteed, as sequence numbers track per-partition order. Exactly-once delivery across retries \u2705 Achieved within a single producer session. Durability across producer restarts \u274c Not guaranteed (new PID assigned on restart). Transactions across multiple partitions/topics \u274c Not guaranteed (use transactions for that). <p>In other words:</p> <ul> <li>Idempotence = Exactly-once per producer session per partition.</li> <li>Transactions = Exactly-once across sessions and partitions.</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#6-limitations","title":"6. Limitations","text":"<p>While <code>enable.idempotence=true</code> is powerful, it has scope boundaries:</p> <ol> <li> <p>PID is lost when the producer restarts.    After a restart, a new PID is generated, and the sequence numbers reset.    The broker treats the new producer as distinct \u2014 so it can\u2019t deduplicate messages across sessions.</p> </li> <li> <p>Only applies per partition.    Kafka ensures ordering and deduplication independently for each partition, not across multiple partitions.</p> </li> <li> <p>Consumer-side duplication still possible if the consumer reprocesses messages without proper offset management (for example, after failure or replay).</p> </li> </ol> <p>For complete exactly-once semantics across multiple topics and sessions, you need transactional producers (<code>initTransactions()</code> / <code>commitTransaction()</code>).</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#7-example-timeline","title":"7. Example timeline","text":"Event Without idempotence With idempotence Producer sends record R1 Broker writes R1 Broker writes R1 Acknowledgment lost Producer retries R1 Producer retries R1 Broker receives retry Broker writes R1 again \u2192 duplicate Broker sees same sequence \u2192 skips duplicate Consumer reads Sees R1 twice Sees R1 once"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#8-recommended-configuration","title":"8. Recommended configuration","text":"<p>For strong reliability and ordering guarantees, use the following settings:</p> <pre><code>acks=all\nenable.idempotence=true\nretries=Integer.MAX_VALUE\nmax.in.flight.requests.per.connection=1\ndelivery.timeout.ms=120000\nlinger.ms=5\nbatch.size=32768\n</code></pre> <p>This ensures:</p> <ul> <li>No duplicates (idempotence).</li> <li>Ordered writes.</li> <li>Infinite retries for transient errors.</li> <li>Reasonable batching and throughput.</li> </ul> <p>If you also need exactly-once across multiple topics or partitions (for example, when using Kafka Streams), enable transactions:</p> <pre><code>enable.idempotence=true\ntransactional.id=unique-producer-id\n</code></pre>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#9-summary","title":"9. Summary","text":"Concept Description Goal Prevent duplicate messages caused by retries or network errors. Mechanism Unique producer ID (PID) and sequence numbers per partition. Effect Broker deduplicates duplicate sends automatically. Scope Guarantees exactly-once per producer session, per partition. Next level Combine with transactions for exactly-once semantics across multiple partitions and sessions."},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#in-short","title":"In short:","text":"<p>Setting <code>enable.idempotence=true</code> ensures that even if a producer retries due to timeouts, disconnects, or leader re-elections, Kafka will store each message exactly once per partition. It eliminates duplicate writes caused by retries and is the foundation for Kafka\u2019s exactly-once delivery guarantees.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#where-is-pid-stored","title":"Where is PID Stored?","text":""},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#1-where-the-producer-id-pid-comes-from","title":"1. Where the Producer ID (PID) comes from","text":"<p>When a Kafka producer with <code>enable.idempotence=true</code> starts, it doesn\u2019t generate its own PID. Instead, it requests a unique Producer ID from the Kafka cluster controller (a special broker responsible for coordination tasks).</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#step-by-step","title":"Step-by-step:","text":"<ol> <li>The producer connects to any broker.</li> <li>That broker forwards a request to the cluster controller.</li> <li>The controller allocates a 64-bit Producer ID (PID) and returns it to the producer.</li> <li>The producer caches this PID locally in memory.</li> </ol> <p>From then on, the producer includes this PID with every produce request it sends.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#where-its-stored","title":"Where it\u2019s stored:","text":"<ul> <li> <p>Producer side:</p> </li> <li> <p>Stored in producer memory (inside the producer client instance).</p> </li> <li>Not persisted to disk by default.</li> <li>Lost when the producer application restarts or crashes.</li> <li> <p>Broker side:</p> </li> <li> <p>Each broker maintains the PID\u2013sequence mapping in memory (part of its replication log state).</p> </li> <li>The mapping is also persisted in the partition log to survive broker restarts (see next sections).</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#2-what-the-broker-stores-the-pid-and-sequence-mapping","title":"2. What the broker stores \u2014 the PID and sequence mapping","text":"<p>Each broker tracks, for every partition it manages:</p> <ul> <li>The latest PID that has written to that partition.</li> <li>The last sequence number seen from that PID.</li> </ul> <p>This information is maintained so that the broker can detect duplicates if a producer retries a batch.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#internally","title":"Internally:","text":"<p>When a broker receives a record batch, it checks:</p> <ol> <li>The PID in the request header.</li> <li>The sequence number range of the batch (e.g., 120\u2013125).</li> <li>The last sequence number it has already written for that PID.</li> </ol> <p>If the new batch\u2019s sequence number overlaps with an existing range, or if it\u2019s exactly equal to one already committed, the broker recognizes it as a duplicate and discards it.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#where-brokers-keep-this-metadata","title":"Where brokers keep this metadata:","text":"<p>This depends on what type of state it is.</p> State Storage location Volatility Description PID \u2192 sequence mapping (active) Broker memory (per-partition state) Lost if broker restarts Used for fast duplicate detection during normal operation PID and last sequence metadata (committed) Kafka log segment (on disk) Persistent Stored as part of the message batch header and as control records Transaction-related PIDs __transaction_state internal topic Persistent Tracks PID ownership and transaction status for EOS producers"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#3-persistence-of-pid-and-sequence-metadata","title":"3. Persistence of PID and sequence metadata","text":"<p>Kafka persists this metadata indirectly through control records written into the partition log itself.</p> <p>Each message batch written by an idempotent producer includes in its header:</p> <ul> <li><code>producerId</code> (PID)</li> <li><code>producerEpoch</code></li> <li><code>baseSequence</code></li> <li><code>lastSequence</code></li> <li><code>isTransactional</code> flag</li> </ul> <p>These headers are stored inside the log segment on disk along with the actual messages.</p> <p>So if a broker restarts:</p> <ul> <li>It replays the log from disk during recovery.</li> <li>During this replay, it reconstructs the PID \u2192 last sequence number mapping in memory.</li> <li>After recovery, duplicate detection continues working as if nothing happened.</li> </ul> <p>That\u2019s why Kafka\u2019s idempotence remains consistent across broker restarts, but not across producer restarts (since the producer gets a new PID each time).</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#4-what-happens-on-producer-restart","title":"4. What happens on producer restart","text":"<p>When the producer application restarts:</p> <ul> <li>The old PID is gone (it was in memory only).</li> <li>Kafka assigns a new PID to the new instance.</li> <li>Because it\u2019s a new PID, the broker treats this as an entirely new producer identity.</li> <li>The old PID\u2019s sequence tracking remains on the broker for a time but is unrelated to the new producer.</li> </ul> <p>This is why idempotence guarantees are session-scoped \u2014 they only apply while the same producer instance (and PID) is alive.</p> <p>If you want to maintain idempotence across restarts, you need transactions with a <code>transactional.id</code>.</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#5-transactional-producers-and-pid-persistence","title":"5. Transactional producers and PID persistence","text":"<p>When you set:</p> <pre><code>transactional.id=my-producer-txn\nenable.idempotence=true\n</code></pre> <p>Kafka upgrades from an idempotent producer to a transactional producer.</p> <p>In this case:</p> <ul> <li>The PID is no longer ephemeral.</li> <li>It\u2019s stored persistently in Kafka\u2019s internal topic:</li> </ul> <p><pre><code>__transaction_state\n</code></pre> * This topic maps each <code>transactional.id</code> to a PID and epoch number. * If the producer restarts with the same <code>transactional.id</code>, it retrieves the same PID (with incremented epoch). * This allows Kafka to continue deduplication across producer restarts \u2014 a key part of exactly-once semantics (EOS).</p>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#6-summary-where-pid-and-sequence-numbers-live","title":"6. Summary: where PID and sequence numbers live","text":"Component Data Stored Where Stored Persistence Purpose Producer PID, per-partition sequence numbers Producer memory Lost on restart Identifies producer session; generates sequence numbers Broker (active memory) Last sequence number per PID per partition Broker memory Reconstructed after restart Used for duplicate detection during runtime Broker (disk) PID, baseSequence, lastSequence (in batch headers) Partition log segments Persistent Replay on restart to rebuild state Transactional producers PID \u2194 transactional.id mapping <code>__transaction_state</code> internal topic Persistent Enables idempotence across restarts and partitions"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#7-illustrated-data-flow-example","title":"7. Illustrated data flow example","text":"<ol> <li>Producer starts \u2192 requests PID from controller \u2192 receives <code>PID = 12345</code>.</li> <li> <p>Producer writes to partition 0:</p> </li> <li> <p>Batch header: <code>{ PID=12345, baseSeq=0, lastSeq=9 }</code></p> </li> <li>Broker appends batch to log segment file <code>/kafka-logs/topic-0/000000000001.log</code></li> <li>Broker updates its in-memory table:</li> </ol> <p><pre><code>topic-0 partition-0:\n  PID 12345 \u2192 lastSeq=9\n</code></pre> 5. Producer retries batch with <code>{PID=12345, baseSeq=0, lastSeq=9}</code> due to timeout.</p> <ul> <li>Broker checks and sees it\u2019s already written \u2014 discards duplicate.</li> <li>Broker restarts later \u2192 reads log segment \u2192 rebuilds PID mapping.</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#8-in-summary","title":"8. In summary","text":"<ul> <li>PID (Producer ID): Assigned by Kafka controller, stored in producer memory; identifies the producer session.</li> <li>Sequence numbers: Maintained by producer per partition; embedded in message batch headers.</li> <li>Broker state: Maintains last sequence per PID in memory for duplicate detection; reconstructs it from the partition log after restart.</li> <li>Transactions: Persist PID \u2194 transactional.id mapping in <code>__transaction_state</code>, enabling exactly-once delivery across restarts.</li> </ul>"},{"location":"scenarios/kafka/09-How_Does_Kafka_Ensure_Partial_Idempotence/#bottom-line","title":"Bottom line:","text":"<p><code>enable.idempotence=true</code> activates a coordinated system where:</p> <ul> <li>The producer tracks sequence numbers.</li> <li>The broker tracks what it has already accepted per producer.</li> <li>Both use persistent log metadata so that duplicate suppression continues even after broker restarts.</li> </ul> <p>If you also configure <code>transactional.id</code>, Kafka persists the PID mapping cluster-wide, extending these guarantees across producer sessions and enabling true exactly-once semantics.</p>"},{"location":"scenarios/spark/","title":"Spark","text":"<p>This is the overview page for Spark.</p>"},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/","title":"Smj Spill To Disk Q1","text":""},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#spark-scenario-smj-spills-to-disk","title":"Spark Scenario - SMJ Spills to Disk","text":""},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#suppose-we-have-skewed-data-and-the-data-for-the-key-does-not-fit-in-memory-and-spills-to-the-disk-can-we-still-join-using-smj-because-entire-data-wont-fit-in-executor-right","title":"Suppose we have skewed data, and the data for the key does not fit in memory and spills to the disk, can we still join using SMJ because entire data wont fit in executor right?","text":"<p>Context</p> <p>Which Join Strategy can handle skewed join relatively well?</p> <p>When data is skewed \u2014 that is, when one partition contains a disproportionately large amount of records \u2014 Shuffle Hash Join faces major challenges because it needs to build a hash table locally during the join, demanding a lot of extra memory and often running into out-of-memory errors. On the other hand, Sort Merge Join copes much better: it doesn\u2019t require additional memory for hash tables, and if a partition is massive, Spark can sort and merge the data while spilling to disk if necessary, ensuring the job completes successfully. Based on this intuition, we can say, Sort Merge Join is generally the more reliable and scalable choice for handling skewed data in Spark.</p> <p>Would the Join Fail?</p> <p>Let\u2019s break it down:</p>"},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#1-how-smj-works","title":"1. How SMJ works","text":"<ul> <li>Both sides of the join are partitioned and sorted on the join key.</li> <li>For each partition, Spark performs a merge-like scan (similar to merge sort).</li> <li>Unlike Broadcast Hash Join (BHJ) or Shuffle Hash Join, SMJ does not require the entire dataset or entire partition for a key to fit in memory.</li> </ul>"},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#2-what-happens-with-skew-large-keys","title":"2. What happens with skew &amp; large keys","text":"<ul> <li>If a single key has an extremely large number of rows, Spark will try to process them together in a streaming fashion during the merge phase.</li> <li>Spark spills sorted runs to disk if the in-memory buffer is not enough.</li> <li>At join time, it merges these spilled runs from disk and streams through them.</li> <li>This means SMJ can still succeed even if data for a single key does not fit entirely into executor memory.</li> </ul>"},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#3-but-whats-the-catch","title":"3. But what\u2019s the catch?","text":"<ul> <li>Performance degradation: spilling means heavy disk I/O, and processing skewed keys becomes very slow.</li> <li>OOM risk in extreme cases: if the skewed key is so large that even the streaming join structures (e.g., buffers, iterators) cannot handle it, Spark can still run into <code>OutOfMemoryError</code>.</li> <li>Shuffle skew: the partition holding that skewed key will be disproportionately large, leading to stragglers.</li> </ul>"},{"location":"scenarios/spark/01-SMJ_Spill_To_Disk_Q1/#4-compared-to-other-joins","title":"4. Compared to other joins","text":"<ul> <li>BHJ: would fail outright if the broadcast side cannot fit in memory.</li> <li>Shuffle Hash Join: needs the entire build side of the hash map for that partition in memory \u2192 skewed data is more likely to OOM.</li> <li>SMJ: more tolerant since it can spill and stream, but performance can still tank.</li> </ul> <p>\u2705 So the answer: Yes, you can still join using SMJ even if data for a key does not fit into executor memory, because Spark will spill to disk and stream. But it will be very slow and possibly unstable if the skew is extreme.</p> <p>\u201cSort-Merge Join in Spark is tolerant to skew because it doesn\u2019t require all data for a key to fit in memory. It sorts partitions using ExternalSorter, which spills sorted runs to disk if they don\u2019t fit. During the merge phase, Spark streams through both sides with iterators and uses spillable buffers for keys with large numbers of rows. This means the join can still succeed even if one key doesn\u2019t fit in executor memory, but it will be very slow due to repeated spill/merge cycles.\u201d</p> <p>\u201cSMJ will succeed even with skewed data because Spark spills and streams, but performance tanks. To handle this in practice, we enable AQE skew join optimization (spark.sql.adaptive.skewJoin.enabled=true), which splits large partitions at runtime. We can tune thresholds (skewedPartitionFactor, skewedPartitionThresholdInBytes) and increase shuffle partitions. If skew is extreme, I\u2019d consider salting keys or broadcasting the smaller side. I always check the Spark UI task distribution to confirm skew.\u201d</p>"},{"location":"scenarios/spark/02-SMJ_Spill_To_Disk_Q2/","title":"Smj Spill To Disk Q2","text":""},{"location":"scenarios/spark/02-SMJ_Spill_To_Disk_Q2/#explain-how-streaming-data-from-disk-to-exector-works-for-smj","title":"Explain How Streaming Data from Disk to Exector works for SMJ?","text":""},{"location":"scenarios/spark/02-SMJ_Spill_To_Disk_Q2/#sort-merge-join-execution-with-skew-spill","title":"\ud83d\udd39 Sort-Merge Join Execution (with skew + spill)","text":"<ol> <li> <p>Both sides sorted &amp; partitioned</p> </li> <li> <p>Table A (smaller side) \u2192 all rows sorted by key.</p> </li> <li> <p>Table B (skewed side) \u2192 rows also sorted by key, but because the skewed key is huge, Spark may spill a lot of its sorted chunks to disk.</p> </li> <li> <p>Executor merge phase</p> </li> <li> <p>Spark creates iterators:</p> <ul> <li>One for A (fits in memory).</li> <li>One for B (some in memory, some spilled to disk).</li> </ul> </li> <li> <p>When join key is encountered</p> </li> <li> <p>Spark buffers all rows for that key from A (usually small enough to keep in memory).</p> </li> <li> <p>Spark starts pulling rows for that key from B in batches.</p> <ul> <li>If the rows are in memory, read directly.</li> <li>If rows were spilled, load them back sequentially from disk (streaming).</li> </ul> </li> <li> <p>Join output</p> </li> <li> <p>For each batch of rows from B, Spark does the Cartesian product with A\u2019s buffered rows.</p> </li> <li>Emits results in a streaming fashion.</li> <li>If B\u2019s key group is gigantic, Spark keeps pulling more batches from disk until all pairs are produced.</li> </ol>"},{"location":"scenarios/spark/02-SMJ_Spill_To_Disk_Q2/#key-insight","title":"\ud83d\udd39 Key Insight","text":"<ul> <li>Spark never tries to load all of skewed table B into memory at once.</li> <li> <p>Instead:</p> </li> <li> <p>A is small \u2192 fully in memory.</p> </li> <li>B is big \u2192 read batch \u2192 join with A \u2192 emit results \u2192 read next batch \u2192 repeat.</li> <li>If B is insanely large, Spark may spill intermediate join buffers again, but the logic is still stream &amp; spill, not \u201cload all at once.\u201d</li> </ul>"},{"location":"scenarios/spark/02-SMJ_Spill_To_Disk_Q2/#analogy","title":"\ud83d\udd39 Analogy","text":"<p>Imagine:</p> <ul> <li>Table A = a tiny bowl of 5 apples \ud83c\udf4e.</li> <li>Table B = a giant truckload of apples \ud83d\ude9a.</li> <li>Spark doesn\u2019t dump the whole truck into memory.</li> <li>Instead, it unloads one crate at a time, joins with the 5 apples from A, writes results out, and then grabs the next crate.</li> </ul> <p>\u2705 So yes, exactly: executor keeps small table A in memory, and streams batches of rows for the skewed key from table B (from memory and disk), joining them incrementally.</p>"},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/","title":"Smj Output During Spill Q3","text":""},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#does-the-data-output-after-all-the-execution-on-the-executors-is-complete","title":"Does the data output after all the execution on the executors is complete?","text":""},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#what-happens-inside-the-executor","title":"\ud83d\udd39 What Happens Inside the Executor","text":"<ol> <li> <p>Join Execution Starts</p> </li> <li> <p>Executor begins processing its partition (say, skewed table B against smaller A).</p> </li> <li>Table A (small) is buffered in memory.</li> <li> <p>Table B rows are streamed batch by batch (from memory and disk if spilled).</p> </li> <li> <p>Streaming Join Loop</p> </li> <li> <p>For each batch of rows from B, executor does the Cartesian product with A.</p> </li> <li>Emits output rows immediately (doesn\u2019t wait to finish all batches).</li> <li> <p>If output rows themselves don\u2019t fit in memory, they too can be spilled to temporary files (shuffle/disk spill).</p> </li> <li> <p>Completion of Task</p> </li> <li> <p>Executor keeps producing and spilling/streaming until all rows for that partition are joined.</p> </li> <li> <p>When done, the results of that task are either:</p> <ul> <li>Stored in shuffle files (if another stage depends on it).</li> <li>Sent to the driver (if you requested <code>.collect()</code>).</li> </ul> </li> </ol>"},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#what-happens-at-the-driver-for-collect","title":"\ud83d\udd39 What Happens at the Driver (for <code>.collect()</code>)","text":"<ul> <li>The driver doesn\u2019t wait for all executors to finish globally before receiving anything.</li> <li>Instead, each executor/task sends its partition\u2019s results back as soon as they\u2019re ready.</li> <li>Spark driver accumulates those partitions until the entire dataset is received.</li> <li>Only when all partitions are received does <code>.collect()</code> return the final Python list.</li> </ul>"},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#so-to-your-question","title":"\ud83d\udd39 So to Your Question","text":"<p>\u201cDoes the executor show all data in <code>collect()</code> only when it finishes joining all batches of table B from disk?\u201d</p> <p>\u2705 Yes, but at the executor level:</p> <ul> <li>Each executor must finish its partition (processing all batches of table B, including spills) before it can hand that partition\u2019s results to the driver.</li> <li>The driver only gets complete partitions from executors, not row-by-row streaming.</li> <li>When all executors finish and send their partitions \u2192 driver merges them \u2192 <code>.collect()</code> returns.</li> </ul>"},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#analogy-apples-again","title":"\ud83d\udd39 Analogy (Apples Again \ud83c\udf4e)","text":"<ul> <li>Executor = worker making apple pairs.</li> <li>Worker has one small bowl (A) and one giant truck (B).</li> <li>Worker processes crates from the truck one at a time, makes pairs with the bowl, and stacks results.</li> <li>Worker doesn\u2019t hand over pairs to the boss (driver) crate by crate \u2014 he waits until all his crates are processed (partition done).</li> <li>Then he delivers his entire stack of results to the boss.</li> <li>Boss waits for all workers to deliver their stacks \u2192 only then shows you the final full list (<code>collect()</code>).</li> </ul>"},{"location":"scenarios/spark/03-SMJ_Output_During_Spill_Q3/#takeaway","title":"\u2705 Takeaway","text":"<ul> <li>Executors stream through skewed/spilled data batch by batch.</li> <li>But the driver only receives results partition by partition (after executor finishes).</li> <li><code>.collect()</code> blocks until all executors finish and return their partitions.</li> <li>That\u2019s why <code>.collect()</code> can OOM the driver \u2192 it tries to hold the entire dataset at once.</li> </ul>"},{"location":"scenarios/spark/04-Cross_vs_Broadcast_Join/","title":"Cross Vs Broadcast Join","text":""},{"location":"scenarios/spark/04-Cross_vs_Broadcast_Join/#what-is-the-difference-between-cross-join-and-broadcast-join","title":"What is the difference between Cross Join and Broadcast Join?","text":"<p>At first glance, a Cartesian Product (a.k.a. Shuffle-and-Replication Nested Loop) join may look similar to a Broadcast Join since data ends up being available across executors. But there is a crucial difference:</p> <p>In a Broadcast Join, the driver first collects the smaller dataset and then broadcasts a full copy of it to every executor. Each executor then joins its local partition of the larger dataset with this broadcasted copy, i.e. there is no shuffle step involved in this process! In a Cartesian Product (Cross Join), the data is not broadcasted by the driver. Instead, Spark performs a shuffle-and-replication, where all partitions of both datasets are exchanged and replicated across executors, ensuring that every partition of one dataset is matched with every partition of the other.</p> <p>\ud83d\udc49 In short: Broadcast Join = driver-based broadcast of a small dataset without any shuffle vs. Cross Join = shuffle-driven replication of all partitions across executors.</p> <p></p>"},{"location":"spark/","title":"Spark","text":"<p>This is the overview page for Spark.</p>"},{"location":"spark/01-Spark_Architecture_YARN/","title":"Spark Architecture Yarn","text":""},{"location":"spark/01-Spark_Architecture_YARN/#spark-architecture-on-yarn","title":"Spark Architecture on YARN","text":""},{"location":"spark/01-Spark_Architecture_YARN/#1-key-components","title":"1. Key Components","text":""},{"location":"spark/01-Spark_Architecture_YARN/#a-master-node-cluster-manager","title":"a) Master Node / Cluster Manager","text":"<ul> <li>YARN ResourceManager (when Spark runs on YARN)</li> <li>Responsible for resource allocation (CPU, memory across cluster).</li> <li>It doesn\u2019t run Spark code itself, but decides where and how many containers to start.</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#b-worker-nodes","title":"b) Worker Nodes","text":"<ul> <li>The machines that actually execute Spark tasks.</li> <li>Each worker hosts YARN NodeManager.</li> <li>Containers are launched here to run parts of Spark applications.</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#c-driver-node","title":"c) Driver Node","text":"<ul> <li>The process that runs your main Spark application code (<code>SparkContext</code>).</li> <li> <p>Responsible for:</p> </li> <li> <p>Creating the DAG of transformations and actions.</p> </li> <li>Submitting tasks to executors.</li> <li>Collecting results.</li> <li> <p>In YARN:</p> </li> <li> <p>Cluster mode \u2192 Driver runs inside an ApplicationMaster container on a worker node.</p> </li> <li>Client mode \u2192 Driver runs on your local machine (the edge node / laptop) while executors run in cluster.</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#d-executors","title":"d) Executors","text":"<ul> <li>JVM processes on worker nodes.</li> <li>Run tasks assigned by the Driver.</li> <li>Store data in memory/disk for caching/shuffles.</li> <li>Communicate with Driver throughout the job\u2019s life.</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#e-application-master-am","title":"e) Application Master (AM)","text":"<ul> <li>A YARN-specific concept.</li> <li>Every YARN application (including Spark) gets its own AM.</li> <li> <p>Responsibilities:</p> </li> <li> <p>Request containers from YARN ResourceManager.</p> </li> <li>Monitor health of containers.</li> <li>For Spark-on-YARN, the ApplicationMaster often hosts the Driver (in cluster mode).</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#2-what-happens-when-you-run-spark-submit","title":"2\ufe0f\u20e3 What Happens When You Run <code>spark-submit</code>","text":"<p>Let\u2019s assume you run:</p> <pre><code>spark-submit --master yarn --deploy-mode cluster my_app.py\n</code></pre>"},{"location":"spark/01-Spark_Architecture_YARN/#step-by-step-flow","title":"Step-by-Step Flow:","text":"<ol> <li> <p>spark-submit starts</p> </li> <li> <p>The client (edge node or local machine) contacts YARN ResourceManager.</p> </li> <li>Submits your application (including JARs, Python files, configs).</li> </ol> <ol> <li> <p>YARN allocates ApplicationMaster container</p> </li> <li> <p>ResourceManager picks a worker node and starts the ApplicationMaster (AM).</p> </li> <li>For Spark, this AM bootstraps the Driver inside itself (in cluster mode).</li> </ol> <ol> <li> <p>Driver starts inside AM</p> </li> <li> <p><code>SparkContext</code> is created.</p> </li> <li> <p>Driver:</p> <ul> <li>Builds logical execution plan (DAG).</li> <li>Asks AM to request executor containers.</li> </ul> </li> </ol> <ol> <li> <p>AM requests Executors</p> </li> <li> <p>AM communicates with ResourceManager \u2192 \u201cI need N executors with X cores &amp; Y memory each.\u201d</p> </li> <li>ResourceManager talks to NodeManagers on workers \u2192 allocates containers.</li> <li>Executors are launched on those workers.</li> </ol> <ol> <li> <p>Executors register with Driver</p> </li> <li> <p>Each executor JVM contacts Driver:      \u201cI\u2019m alive and ready.\u201d</p> </li> <li>Driver now knows how many executors it has and their resources.</li> </ol> <ol> <li> <p>Tasks scheduled</p> </li> <li> <p>Driver divides DAG into stages \u2192 tasks.</p> </li> <li>Tasks are shipped to executors.</li> <li>Executors run the tasks, fetch data (HDFS, S3, Delta, etc.), cache/shuffle results.</li> </ol> <ol> <li> <p>Execution &amp; Results</p> </li> <li> <p>Executors send status &amp; results back to Driver.</p> </li> <li>Driver coordinates retries on failure.</li> </ol> <ol> <li> <p>Job Completion</p> </li> <li> <p>Once all actions complete, Driver tells AM to stop executors.</p> </li> <li>AM unregisters with ResourceManager.</li> <li>Job is marked as finished.</li> </ol>"},{"location":"spark/01-Spark_Architecture_YARN/#3-cluster-mode-vs-client-mode-big-interview-question","title":"3\ufe0f\u20e3 Cluster Mode vs Client Mode (Big Interview Question!)","text":"<ul> <li> <p>Cluster Mode</p> </li> <li> <p>Driver runs inside the cluster (in AM container).</p> </li> <li> <p>Good for production (doesn\u2019t depend on client machine).</p> </li> <li> <p>Client Mode</p> </li> <li> <p>Driver runs on submitting machine (edge node).</p> </li> <li>Executors still in cluster.</li> <li>Good for development / debugging.</li> </ul>"},{"location":"spark/01-Spark_Architecture_YARN/#4-quick-visual","title":"4\ufe0f\u20e3 Quick Visual","text":"<pre><code>+-----------------+          +-----------------+\n| spark-submit    |          | YARN RM         |\n| (client)        |          | (ResourceManager)|\n+--------+--------+          +--------+--------+\n         |                            |\n         | Submit App                 |\n         v                            |\n+--------+--------+                   |\n| App Master (AM) | &lt;-----------------+\n| (hosts Driver)  |\n+--------+--------+\n         |\n         | Request Executors\n         v\n+--------+--------+     +--------+--------+\n| Executor (Node) | ... | Executor (Node) |\n+-----------------+     +-----------------+\n</code></pre>"},{"location":"spark/01-Spark_Architecture_YARN/#5-in-summary","title":"5\ufe0f\u20e3 In Summary","text":"<ul> <li>Driver = Brain of Spark App (DAG, task scheduling).</li> <li>Executors = Workers that do actual computation.</li> <li>ApplicationMaster = YARN-specific agent that negotiates resources and may host Driver.</li> <li>spark-submit = Entry point that asks YARN to spin everything up.</li> <li>Cluster Mode = Driver in cluster (recommended for prod).</li> <li>Client Mode = Driver on local machine.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/","title":"Spark Driver Oom","text":""},{"location":"spark/02-Spark_Driver_OOM/#spark-driver-out-of-memory","title":"Spark Driver Out Of Memory","text":""},{"location":"spark/02-Spark_Driver_OOM/#tldr-when-the-driver-goes-oom","title":"TL;DR \u2014 When the driver goes OOM","text":"<p>The driver JVM runs out of heap memory (or the driver process runs out of OS memory). Typical causes:</p> <ul> <li>You collect too much data (e.g. <code>df.collect()</code>, <code>toPandas()</code>).</li> <li>Building a broadcast (Spark collects small table on driver first).</li> <li>Very many tasks/partitions (driver holds large task metadata / DAG).</li> <li>Too many cached blocks / block metadata tracked on driver.</li> <li>Large accumulators/driver-side state (job results, listeners, query progress).</li> <li>Driver running in a resource-constrained environment (client mode / small driver container).</li> <li>Streaming state / progress objects growing unbounded (structured streaming).</li> <li>Python driver process OOM (PySpark <code>collect()</code> or <code>toPandas()</code> can blow Python memory).</li> <li>Huge closure serialization or large objects kept accidentally in driver variables.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#1-where-driver-memory-is-used-what-can-fill-the-driver-heap","title":"1) Where driver memory is used (what can fill the driver heap)","text":"<ul> <li>Result aggregation: results of <code>collect()</code>, <code>take()</code> that are brought to driver.</li> <li>Broadcast creation: driver materializes &amp; serializes broadcast data before sending to executors.</li> <li>Metadata: DAG, StageInfo, TaskInfo, JobInfo, SQL plan metadata held in driver.</li> <li>BlockManagerMaster metadata: mapping of blockId \u2192 locations for cached blocks (large when many blocks cached).</li> <li>Driver-side data structures: listeners, accumulators, job results, streaming query progress history.</li> <li>Serialized closures: driver holds references to closures until shipped.</li> <li>Driver UI &amp; metrics objects: Web UI stores some in-memory structures.</li> <li>Python objects (PySpark): Python driver process memory is separate and can OOM even if JVM is fine.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#2-concrete-examples-and-log-clues","title":"2) Concrete examples and log clues","text":""},{"location":"spark/02-Spark_Driver_OOM/#a-broadcast-join-causing-driver-oom","title":"A. Broadcast join causing driver OOM","text":"<p>Stack trace hint:</p> <pre><code>java.lang.OutOfMemoryError: Java heap space\nat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(...)\n</code></pre> <p>Meaning: Spark tried to collect the broadcast side to the driver (to serialize and slice it) and the dataset was too large.</p> <p>Fixes:</p> <ul> <li>Don\u2019t broadcast that table. Disable or lower broadcast threshold:</li> </ul> <p><pre><code>spark.sql.autoBroadcastJoinThreshold=10MB  # default ~10MB; reduce it\n</code></pre> * Use shuffle join (remove <code>broadcast()</code> hint), increase driver memory, or pre-aggregate/filter to make broadcast side small.</p>"},{"location":"spark/02-Spark_Driver_OOM/#b-collect-topandas-errors","title":"B. <code>collect()</code> / <code>toPandas()</code> errors","text":"<p>Symptom: <code>java.lang.OutOfMemoryError: Java heap space</code> OR Python <code>MemoryError</code> (if using <code>toPandas()</code>). Meaning: you pulled a lot of rows into driver memory (JVM or Python).</p> <p>Fixes:</p> <ul> <li>Avoid <code>collect()</code>. Use <code>write.parquet(...)</code>, <code>foreachPartition()</code>, or <code>toLocalIterator()</code> (streams partitions; but still must not accumulate full result).</li> <li>For pandas usage, use <code>df.limit(n).toPandas()</code> only for small n or use chunked writes.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#c-too-many-partitions-tasks-metadata-explosion","title":"C. Too many partitions / tasks \u2192 metadata explosion","text":"<p>Symptom: driver memory grows gradually; many small tasks; driver GC overhead. Cause: driver stores Task/Stage info per task. If partitions &gt;&gt; millions, driver metadata map grows big.</p> <p>Fixes:</p> <ul> <li>Reduce number of partitions before heavy actions: use <code>repartition()</code> (careful: shuffle) or consolidate upstream.</li> <li>Avoid tiny files and extremely high partition counts.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#d-large-number-of-cached-blocks","title":"D. Large number of cached blocks","text":"<p>Symptom: driver memory tied to BlockManagerMasterMetadata; <code>Storage</code> tab shows many blocks. Fixes:</p> <ul> <li>Reduce caching, unpersist unused cached RDDs/DataFrames.</li> <li>Use <code>MEMORY_AND_DISK_SER</code> or <code>DISK_ONLY</code> for huge caches.</li> <li>Consider checkpointing rather than caching many small blocks.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#e-structured-streaming-and-state-blowup","title":"E. Structured Streaming and state blowup","text":"<p>Symptom: streaming query state grows (map of keys), driver shows many state snapshots. RocksDB helps on executors but driver still holds metadata. Fixes:</p> <ul> <li>Tune watermarks &amp; state TTL.</li> <li>Use RocksDB state store (<code>stateStore.rocksdb.enabled=true</code>) to reduce executor heap; ensure checkpointing.</li> <li>Monitor state size and prune old state.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#3-root-causes-in-order-of-frequency","title":"3) Root causes in order of frequency","text":"<ol> <li>Collecting huge result sets (most common rookie error).</li> <li>Broadcast of a too-large dataset (common when <code>autoBroadcastJoinThreshold</code> too high or broadcast hinted).</li> <li>Too many partitions / tasks or excessively large DAG (scale-related).</li> <li>Large number of cached blocks (storage metadata explosion).</li> <li>Driver-side programming bug (storing big objects in driver variables/closures).</li> <li>Streaming / long-running app accumulating state, listeners, progress logs.</li> <li>Python-side memory usage (PySpark) \u2014 separate Python process OOM.</li> <li>Operating in client mode on a weak edge node (driver has limited resources).</li> </ol>"},{"location":"spark/02-Spark_Driver_OOM/#4-diagnostics-what-to-check-first-quick-checklist","title":"4) Diagnostics \u2014 what to check first (quick checklist)","text":"<ul> <li>Check logs/stack trace: look for <code>OutOfMemoryError</code> stack frames (e.g., <code>TorrentBroadcast</code>, <code>ObjectOutputStream</code>, <code>BlockManager</code>).</li> <li> <p>Spark UI (Driver):</p> </li> <li> <p>Storage tab: many blocks?</p> </li> <li>Executors tab: driver metrics?</li> <li>SQL/Jobs tabs: huge number of tasks?</li> <li>YARN / cluster manager logs (if on YARN): <code>yarn logs -applicationId &lt;app&gt;</code> for driver container logs.</li> <li>Is it JVM OOM or Python OOM? Python OOM shows <code>MemoryError</code>; JVM OOM shows <code>java.lang.OutOfMemoryError</code>.</li> <li>Check driver heap usage / GC logs: increase log level, enable GC logs, capture heap dump (<code>jmap -dump</code>) or thread dump (<code>jstack</code>).</li> <li>Look for actions preceding OOM: <code>collect</code>, <code>broadcast</code>, <code>toPandas</code>, large <code>take</code>, <code>count</code> on big DF, many <code>.cache()</code> calls.</li> <li>Check number of partitions: <code>df.rdd.getNumPartitions()</code> or examine job shuffle partitions.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#5-remedies-practical-fixes","title":"5) Remedies &amp; practical fixes","text":""},{"location":"spark/02-Spark_Driver_OOM/#immediate-quick-fixes","title":"Immediate (quick) fixes","text":"<ul> <li>Avoid <code>collect()</code> / <code>toPandas()</code>; use <code>limit()</code> or write out to storage.</li> <li>Reduce/disable broadcasting:</li> </ul> <p><pre><code>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")  # lower it\n</code></pre> * Increase driver memory:</p> <ul> <li><code>spark-submit --driver-memory 8g</code> or <code>spark.driver.memory=8g</code> (or change driver node type in Databricks).</li> <li>Set <code>spark.driver.maxResultSize</code> to a safe limit (default \\~1g). If result may exceed, either increase or avoid collecting so big results.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#code-level-patterns-to-avoid-driver-oom","title":"Code-level patterns to avoid driver OOM","text":"<ul> <li>Use <code>foreachPartition()</code> to process data on executors instead of collecting to driver.</li> <li>Use streaming writes to disk / object store rather than collecting.</li> <li>Use <code>df.write.format(...).mode(\"append\").save(...)</code> to persist results.</li> <li>Use distributed joins/aggregations; avoid forcing data to driver.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#clusterconfig-tuning","title":"Cluster/config tuning","text":"<ul> <li>Increase <code>spark.driver.memory</code> and <code>spark.driver.memoryOverhead</code> (on YARN set memoryOverhead accordingly).</li> <li>For broadcast issues: decrease <code>spark.sql.autoBroadcastJoinThreshold</code> or remove <code>broadcast()</code> hints.</li> <li>For many small partitions: coalesce to fewer partitions before actions (use <code>coalesce(n)</code> if decreasing, <code>repartition(n)</code> if rebalancing needed).</li> <li>If using structured streaming with large state: enable RocksDB and tune <code>stateStore.rocksdb.*</code> settings; increase checkpointing.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#for-pyspark-users","title":"For PySpark users","text":"<ul> <li>Avoid <code>collect()</code> \u2192 <code>toPandas()</code> is especially dangerous for big datasets.</li> <li>Use <code>toLocalIterator()</code> to stream partition rows to Python without loading all at once \u2014 but process and discard them rather than accumulating.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#6-example-scenarios-exact-config-suggestions","title":"6) Example scenarios &amp; exact config suggestions","text":"<p>Scenario A \u2014 Broadcast OOM</p> <ul> <li>Symptom: OOM with <code>TorrentBroadcast.writeBlocks</code>.</li> <li>Fix:</li> </ul> <pre><code>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"5MB\")\n# or remove broadcast hint\ndf.join(other_df, \"key\")  # let it shuffle join\n</code></pre> <p>Scenario B \u2014 collect() blew driver</p> <ul> <li>Symptom: OOM right after a <code>collect()</code> call.</li> <li>Fix: Use:</li> </ul> <pre><code>for part in df.toLocalIterator():\n    process_and_write(part)   # stream, do not save all to list\n</code></pre> <p>or write to file:</p> <pre><code>df.write.mode(\"overwrite\").parquet(\"/tmp/output\")\n</code></pre> <p>Scenario C \u2014 Too many tasks</p> <ul> <li>Symptom: driver memory climbs during scheduling; job has millions of tasks.</li> <li> <p>Fix:</p> </li> <li> <p>Reduce partitions: <code>df.repartition(1000)</code> (or <code>coalesce</code> if only reducing and you don\u2019t need balanced).</p> </li> <li>Increase <code>spark.executor.cores</code> or adjust parallelism.</li> </ul> <p>Scenario D \u2014 Block metadata explosion</p> <ul> <li>Symptom: Storage tab shows huge number of blocks; driver memory high.</li> <li>Fix: <code>df.unpersist()</code> unused caches, or reduce cache footprint and use serialized storage level:</li> </ul> <pre><code>df.persist(StorageLevel.MEMORY_AND_DISK_SER)\ndf.unpersist()  # when done\n</code></pre>"},{"location":"spark/02-Spark_Driver_OOM/#7-how-to-debug-step-by-step-practical-workflow","title":"7) How to debug step-by-step (practical workflow)","text":"<ol> <li>Reproduce with smaller job locally or with logging turned on.</li> <li>Inspect Spark driver logs for stack trace.</li> <li> <p>Check Spark UI:</p> </li> <li> <p>Storage (# blocks),</p> </li> <li>SQL/Jobs (number of tasks, task sizes),</li> <li>Executors (memory usage).</li> <li>If broadcast suspected, check <code>explain()</code> and physical plan (<code>df.explain(True)</code> \u2014 look for <code>BroadcastHashJoin</code>).</li> <li>Dump driver heap (<code>jmap -dump</code>) and analyze with MAT if you can. Look for big retained objects: byte[] arrays (serialized broadcasts) or HashMaps of block metadata.</li> <li>Fix code/config and re-run.</li> </ol>"},{"location":"spark/02-Spark_Driver_OOM/#8-extra-notes-gotchas","title":"8) Extra notes / gotchas","text":"<ul> <li>Client vs Cluster mode: In client mode the driver runs where you launched <code>spark-submit</code> (edge node). If that node is small you\u2019ll OOM easily. Prefer cluster mode in prod.</li> <li>Driver vs Executor OOM: Executors OOM during task processing; driver OOM usually due to driver responsibilities (collection, broadcast, metadata). Different fixes.</li> <li>Off-heap memory: Spark (Tungsten) can use off-heap memory. Driver JVM heap OOM is different from OS OOM. Check overall process RSS if native memory also grows.</li> <li>Spark History / UI retention: Long-running apps accumulate a lot of in-memory history/history server metadata \u2014 may increase memory usage.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#9-quick-checklist-what-to-try-first","title":"9) Quick checklist (what to try first)","text":"<ul> <li>Did I call <code>collect()</code>/<code>toPandas()</code>? If yes, remove/limit it.</li> <li>Is a broadcast happening? Check <code>df.explain(True)</code>. Lower <code>spark.sql.autoBroadcastJoinThreshold</code>.</li> <li>Are there millions of partitions/tasks? Repartition/coalesce.</li> <li>Are many DataFrames cached? Unpersist unused caches or change storage level.</li> <li>Increase <code>spark.driver.memory</code> if legitimately needed.</li> <li>For streaming, enable RocksDB for heavy state, and tune watermark/timeToLive.</li> </ul>"},{"location":"spark/02-Spark_Driver_OOM/#example-decision-trees-for-common-symptoms","title":"Example decision trees for common symptoms","text":"<p>Symptom: <code>java.lang.OutOfMemoryError</code> with <code>TorrentBroadcast</code> in stack. \u2192 Cause: broadcast too large. \u2192 Quick fix: <code>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")</code> or remove broadcast hint.</p> <p>Symptom: OOM after calling <code>df.collect()</code> or <code>df.toPandas()</code> \u2192 Don\u2019t collect entire dataset. Use streaming writes or <code>toLocalIterator()</code> + process.</p> <p>Symptom: Driver memory slowly climbs during scheduling of a huge job (many tasks) \u2192 Reduce partitions; increase driver memory; break job into smaller batches.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/","title":"Types of Memory in Spark","text":"<p>Great question \ud83d\ude80 \u2014 Spark\u2019s memory model is tricky but very important for tuning and avoiding OOMs. Let\u2019s break it down.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#types-of-memory-in-spark_1","title":"\ud83d\udd39 Types of Memory in Spark","text":"<p>Broadly, Spark memory can be thought of at two levels:</p> <ol> <li>Execution vs Storage memory (inside the JVM heap managed by Spark)</li> <li>Other JVM memory categories (outside Spark\u2019s unified memory)</li> </ol>"},{"location":"spark/03-Types_Of_Memory_Spark/#1-execution-memory","title":"1. Execution Memory","text":"<ul> <li> <p>Used for:</p> </li> <li> <p>Shuffle operations (sort, join, aggregation)</p> </li> <li>Hash tables for joins and aggregations</li> <li>Temporary buffers when spilling to disk</li> <li>When it runs out: Data is spilled to disk.</li> </ul> <p>\ud83d\udc49 Example: When Spark does a <code>groupByKey</code> or <code>sortByKey</code>, it needs execution memory to build in-memory data structures.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#2-storage-memory","title":"2. Storage Memory","text":"<ul> <li> <p>Used for:</p> </li> <li> <p>Caching/persisting RDDs/DataFrames (<code>df.cache()</code>, <code>rdd.persist()</code>)</p> </li> <li>Broadcast variables (e.g., in broadcast joins)</li> <li> <p>Unrolling RDD elements before caching</p> </li> <li> <p>When it runs out:</p> </li> <li> <p>Cached blocks may be evicted (LRU).</p> </li> <li>Broadcast variables may spill.</li> </ul> <p>\ud83d\udc49 Example: If you do:</p> <pre><code>val cachedDF = df.cache()\n</code></pre> <p>The DataFrame sits in storage memory.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#3-unified-memory-management","title":"3. Unified Memory Management","text":"<p>Since Spark 1.6+, execution and storage memory share a unified pool (<code>spark.memory.fraction</code>, default 60% of JVM heap).</p> <ul> <li>If execution needs more \u2192 it can borrow from storage (by evicting cached blocks).</li> <li>If storage needs more \u2192 it can borrow from execution, but only if execution isn\u2019t using it.</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#4-user-memory","title":"4. User Memory","text":"<ul> <li> <p>Used for:</p> </li> <li> <p>Data structures created by your Spark code inside UDFs, accumulators, custom objects, etc.</p> </li> <li>Spark doesn\u2019t manage this \u2014 it\u2019s just regular JVM heap outside the unified pool.</li> </ul> <p>\ud83d\udc49 Example: If you write a UDF that builds a big in-memory map, it goes into user memory.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#5-reserved-memory","title":"5. Reserved Memory","text":"<ul> <li>A fixed amount Spark reserves for internal operations (default \\~300 MB per executor).</li> <li>Not configurable (except by changing Spark code).</li> <li>Ensures Spark doesn\u2019t use 100% of JVM heap and leave nothing for itself.</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#6-off-heap-memory","title":"6. Off-Heap Memory","text":"<ul> <li> <p>Used for:</p> </li> <li> <p>Tungsten\u2019s optimized binary storage format (off-heap caching)</p> </li> <li>When <code>spark.memory.offHeap.enabled=true</code></li> <li>Managed outside JVM heap \u2192 avoids GC overhead.</li> <li>Configurable with <code>spark.memory.offHeap.size</code>.</li> </ul> <p>\ud83d\udc49 Example: When you enable off-heap caching, Spark stores columnar data in native memory instead of the JVM heap for efficiency.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#spark-memory-layout-executor-jvm-heap","title":"\ud83d\udd39 Spark Memory Layout (Executor JVM Heap)","text":"<pre><code>+-------------------------------------------------------------+\n| JVM Heap                                                    |\n|                                                             |\n|  Reserved Memory (~300MB, always kept aside)                |\n|-------------------------------------------------------------|\n| Unified Memory Region (spark.memory.fraction ~ 60%)         |\n|   - Execution Memory  &lt;---- shareable ----&gt; Storage Memory  |\n|-------------------------------------------------------------|\n| User Memory (UDF objects, data structures, not Spark-managed)|\n+-------------------------------------------------------------+\n</code></pre> <p>Outside JVM Heap:</p> <ul> <li>Off-Heap Memory (optional, managed by Spark)</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#where-they-are-used-in-practice","title":"\ud83d\udd39 Where They Are Used in Practice","text":"<ul> <li>Execution Memory \u2192 Sorting, shuffling, joins, aggregations</li> <li>Storage Memory \u2192 Caching/persist, broadcast variables</li> <li>User Memory \u2192 UDFs, custom data structures, accumulators</li> <li>Reserved Memory \u2192 Spark internal bookkeeping</li> <li>Off-Heap Memory \u2192 Tungsten, columnar cache, avoids GC overhead</li> </ul> <p>\u2705 Summary:</p> <ul> <li>Spark divides memory into execution (processing/shuffle) and storage (cache/broadcast).</li> <li>These share a unified pool for efficiency.</li> <li>User memory and reserved memory sit outside Spark\u2019s control.</li> <li>Off-heap memory is optional but useful for performance.</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#1-execution-memory_1","title":"\ud83d\udd39 1. Execution Memory","text":"<p>Definition: Memory used for processing computations in Spark.</p> <p>What it stores:</p> <ul> <li>Shuffle operations (sorts, aggregations, joins)</li> <li>Hash tables for joins and aggregations</li> <li>Temporary buffers for sorting, spilling data to disk</li> </ul> <p>Behavior:</p> <ul> <li>Borrowable from storage memory if storage is not using all of its share (because Spark uses unified memory pool)</li> <li>If execution memory runs out, Spark spills intermediate data to disk to avoid crashing</li> </ul> <p>Example:</p> <pre><code>df.groupBy(\"state\").agg(sum(\"revenue\"))\n</code></pre> <ul> <li>Spark builds a hash map of states \u2192 execution memory is used.</li> <li>If too many states to fit in memory \u2192 spills to disk.</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#2-storage-memory_1","title":"\ud83d\udd39 2. Storage Memory","text":"<p>Definition: Memory used for caching and storing data in memory.</p> <p>What it stores:</p> <ul> <li>Cached/persisted RDDs or DataFrames (<code>df.cache()</code>)</li> <li>Broadcast variables for joins</li> <li>Unrolled blocks before writing to cache</li> </ul> <p>Behavior:</p> <ul> <li>Evictable (Spark uses LRU \u2014 least recently used blocks get removed if execution needs memory)</li> <li>Part of unified memory pool (<code>spark.memory.fraction</code>)</li> <li>Helps avoid recomputation or re-reading data from disk</li> </ul> <p>Example:</p> <pre><code>val cachedDF = df.cache()\ncachedDF.count()  // Storage memory used to keep DF in memory\n</code></pre>"},{"location":"spark/03-Types_Of_Memory_Spark/#3-key-difference","title":"\ud83d\udd39 3. Key Difference","text":"Feature Execution Memory Storage Memory Purpose For computations (shuffle, join, aggregation) For caching/persisting data, broadcast variables Managed by Spark? Yes (part of unified memory pool) Yes (part of unified memory pool) Borrowable? Can borrow from storage if free Can be borrowed by execution if storage is not full Eviction policy Spills to disk when full LRU eviction to free memory for execution Examples groupBy, sort, join buffers df.cache(), broadcast joins"},{"location":"spark/03-Types_Of_Memory_Spark/#4-unified-memory-pool","title":"\ud83d\udd39 4. Unified Memory Pool","text":"<p>Spark &gt;= 1.6 uses unified memory management:</p> <pre><code>+-----------------------------------------+\n| Unified Memory Pool (spark.memory.fraction ~60%) |\n|-----------------------------------------|\n| Execution Memory   &lt;------&gt;  Storage Memory |\n+-----------------------------------------+\n</code></pre> <ul> <li>Execution can borrow from storage memory if needed.</li> <li>Storage can use free execution memory.</li> <li>Prevents fixed partitioning of memory and reduces OOMs.</li> </ul> <p>\u2705 In short:</p> <ul> <li>Execution Memory \u2192 used temporarily for computations like shuffles, joins, sorts.</li> <li>Storage Memory \u2192 used to store cached RDDs/DataFrames or broadcast variables for reuse.</li> <li>Both share the same unified memory pool.</li> </ul> <p>Perfect! Let\u2019s visualize Execution Memory vs Storage Memory in Spark with a clear diagram and examples.</p>"},{"location":"spark/03-Types_Of_Memory_Spark/#spark-executor-memory-layout-simplified","title":"\ud83d\udd39 Spark Executor Memory Layout (Simplified)","text":"<pre><code>Executor JVM Heap\n+-----------------------------------------------------------+\n| Reserved Memory (~300MB)                                  |\n|-----------------------------------------------------------|\n| User Memory (UDFs, accumulators, temp objects)           |\n|-----------------------------------------------------------|\n| Unified Memory Pool (~60% of executor memory)            |\n|   +---------------------+  +--------------------------+ |\n|   | Execution Memory    |  | Storage Memory           | |\n|   |---------------------|  |--------------------------| |\n|   | Used for:           |  | Used for:                | |\n|   | - Shuffle buffers   |  | - Cached RDD/DataFrames  | |\n|   | - Join/hash tables  |  | - Broadcast variables    | |\n|   | - Aggregations      |  | - Unrolled blocks        | |\n|   | If full -&gt; spills   |  | If needed -&gt; evict LRU   | |\n|   +---------------------+  +--------------------------+ |\n+-----------------------------------------------------------+\n</code></pre>"},{"location":"spark/03-Types_Of_Memory_Spark/#examples-of-memory-usage","title":"\ud83d\udd39 Examples of Memory Usage","text":"Operation / Action Memory Used Notes <code>df.groupBy(\"state\").agg(sum(\"revenue\"))</code> Execution Memory Hash map for aggregation stored here. If too large \u2192 spill to disk. <code>df.sort(\"date\")</code> Execution Memory Sort buffers stored in memory before writing or returning results. <code>df.cache()</code> Storage Memory Cached DataFrame resides here for reuse. <code>broadcast(df)</code> Storage Memory Broadcasted DataFrame for joins stored here. Temporary object inside a UDF User Memory Not managed by Spark\u2019s unified memory."},{"location":"spark/03-Types_Of_Memory_Spark/#unified-memory-behavior","title":"\ud83d\udd39 Unified Memory Behavior","text":"<ul> <li>Execution can borrow from storage if storage has free space.</li> <li>Storage can borrow from free execution memory if execution isn\u2019t using it.</li> <li>Helps prevent OOM errors and improves memory efficiency.</li> </ul>"},{"location":"spark/03-Types_Of_Memory_Spark/#quick-visual-summary","title":"\ud83d\udd39 Quick Visual Summary","text":"<pre><code>Execution Memory   &lt;----&gt; Storage Memory\n (shuffle, join)         (cache, broadcast)\n      |                        |\n      v                        v\n  spills to disk           evict LRU\n</code></pre> <p>\u2705 Key Takeaways:</p> <ul> <li>Execution Memory: Temporary, computation-related, spills to disk if needed.</li> <li>Storage Memory: Persistent, caching/broadcast, evictable.</li> <li>Unified Memory Pool: Flexible sharing to reduce memory pressure.</li> </ul> <p>When can we neither spill to disk or evict storage memory? Link</p> <p>Would you like me to also give you a real-world scenario of an executor OOM and show which type of memory usually causes it (shuffle-heavy job vs cache-heavy job vs UDF-heavy job)?</p>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/","title":"Dynamic Partition Pruning in Spark","text":""},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#1-the-core-problem","title":"\ud83d\udd39 1. The Core Problem","text":"<p>Imagine you have a fact table partitioned by <code>date</code> (billions of rows, thousands of partitions).</p> <p>Example:</p> <pre><code>SELECT *\nFROM sales\nWHERE sales.date IN (SELECT promo_date FROM promotions);\n</code></pre> <ul> <li>Here, <code>promo_date</code> values are not known at query compile time (because they come from another table).</li> <li>Spark cannot prune partitions statically.</li> </ul> <p>Without DPP, Spark scans all partitions of sales, then filters \u2014 very expensive.</p>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#2-why-dpp-works-internals","title":"\ud83d\udd39 2. Why DPP Works (Internals)","text":"<p>Spark 3.x introduced a mechanism where:</p> <ol> <li> <p>Query is split into two stages:</p> </li> <li> <p>Stage 1: Collect partition filter values (from <code>promotions</code>).</p> </li> <li> <p>Stage 2: Push those values at runtime into the scan of <code>sales</code>.</p> </li> <li> <p>Broadcasting comes in:</p> </li> <li> <p>The small dimension table (<code>promotions</code>) is broadcast to all worker nodes.</p> </li> <li> <p>This lets every executor know exactly which partitions of <code>sales</code> to scan.</p> </li> <li> <p>Dynamic Filter Expression:</p> </li> <li> <p>Spark inserts a <code>DynamicPruningExpression</code> node in the query plan.</p> </li> <li>This acts as a runtime filter for partition values.</li> </ol>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#3-before-vs-after-dpp","title":"\ud83d\udd39 3. Before vs After DPP","text":""},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#without-dpp","title":"\u274c Without DPP","text":"<p>Execution Plan (simplified):</p> <pre><code>Scan sales (ALL partitions)\n   Filter: date IN (subquery(promotions))\n</code></pre> <ul> <li>All partitions are scanned (huge I/O).</li> <li>Filtering happens after reading data.</li> </ul>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#with-dpp","title":"\u2705 With DPP","text":"<p>Execution Plan (simplified):</p> <pre><code>BroadcastHashJoin\n   Left: Scan sales (partition filters: date = dynamicpruning#...)\n   Right: Broadcast(promotions)\n</code></pre> <ul> <li><code>promotions</code> table is broadcasted (small).</li> <li>Spark evaluates filter values at runtime.</li> <li>Only matching partitions are scanned.</li> </ul> <p>You\u2019ll see in <code>.explain(true)</code> something like:</p> <pre><code>:dynamicpruning#... (isnotnull(promotions.date))\n</code></pre>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#4-why-broadcasting-matters","title":"\ud83d\udd39 4. Why Broadcasting Matters","text":"<ul> <li>DPP relies on dimension table being small enough to broadcast.</li> <li>Broadcast ensures all executors get the filter values quickly.</li> <li>If the table is large, Spark may fallback to non-broadcast mode (still works, but slower).</li> </ul>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#5-configs-controlling-dpp","title":"\ud83d\udd39 5. Configs Controlling DPP","text":"<pre><code>SET spark.sql.optimizer.dynamicPartitionPruning.enabled = true;   -- enable/disable DPP\nSET spark.sql.optimizer.dynamicPartitionPruning.useStats = true; -- prune only if selectivity helps\nSET spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly = true; -- requires broadcast\nSET spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio = 0.5; -- fallback if not selective\n</code></pre>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#6-real-example-in-databricks","title":"\ud83d\udd39 6. Real Example in Databricks","text":"<pre><code>-- Fact table partitioned by date\nCREATE TABLE sales (\n  order_id STRING,\n  amount DECIMAL,\n  date DATE\n)\nUSING delta\nPARTITIONED BY (date);\n\n-- Dimension table\nCREATE TABLE promotions (promo_date DATE);\n\n-- Query with DPP\nSELECT s.order_id, s.amount\nFROM sales s\nJOIN promotions p\n  ON s.date = p.promo_date;\n</code></pre> <ul> <li>If <code>promotions</code> has only 3 dates, DPP ensures only those 3 partitions of <code>sales</code> are read.</li> <li>Without DPP \u2192 all partitions scanned.</li> </ul>"},{"location":"spark/04-Spark_Dynamic_Partition_Pruning/#7-exact-reason-dpp-improves-performance","title":"\ud83d\udd39 7. Exact Reason DPP Improves Performance","text":"<ul> <li>Reduces I/O \u2192 fewer partitions scanned.</li> <li>Reduces shuffle \u2192 only data from relevant partitions enters the join.</li> <li>Pushdown at runtime \u2192 avoids loading terabytes unnecessarily.</li> </ul> <p>\u2705 In summary:</p> <ul> <li>DPP works because Spark can delay partition pruning until runtime.</li> <li>It collects filter values (from another query/dimension) \u2192 broadcasts them \u2192 applies as partition filters before scanning the fact table.</li> <li>This is why execution plans show <code>DynamicPruningExpression</code>.</li> </ul> <p></p> <p>At runtime a subquery is sent to main table that looks like this:</p> <pre><code>select * from main_table where sales_date in (select dates from small_table)\n</code></pre> <p>So essentially to make this work, the main table should be partitioned and the smaller table should be broadcastable.</p>"},{"location":"spark/05-Spark_Salting_Technique/","title":"Salting in PySpark","text":"<p>Check below condition we get 15 records.</p> <p></p> <p>The id=1 on left side is skewed and assume that the table2 is &gt; 10 MB</p> <p>Now salt the data, append a random number between 1-10 to the id on both the sides.</p> <p></p> <p>All the salted keys go into different partitions and not just one like before.</p> <p></p> <p>Now there is an evident problem where we get only 3 records instead of the actual 15. So to tackle this on table2 we need to generate 10 salted keys for each id so that the join  is possible.</p> <p>Before Salting</p> <p></p> <p>After Salting</p> <p></p>"},{"location":"spark/05-Spark_Salting_Technique/#2-where-memory-comes-into-play","title":"\ud83d\udd11 2. Where Memory Comes Into Play","text":"<p>Salting helps with shuffle balance, but OOM can still happen depending on join strategy:</p> <ul> <li> <p>Broadcast Hash Join (BHJ)</p> </li> <li> <p>Spark broadcasts the smaller table to all executors.</p> </li> <li>\u274c If the second table is large (multi-GBs), broadcasting it will OOM on driver/executors.</li> <li> <p>Spark usually auto-switches to Sort-Merge Join (SMJ) if the table is bigger than <code>spark.sql.autoBroadcastJoinThreshold</code> (default \\~10MB).</p> </li> <li> <p>Sort-Merge Join (SMJ)</p> </li> <li> <p>Both tables are shuffled and sorted.</p> </li> <li>Salting works well here because it reduces shuffle skew.</li> <li>\u2705 No broadcast \u2192 less chance of OOM.</li> <li> <p>But if partitions are still unbalanced (or too wide), you can hit OOM during shuffle spill or sort buffer.</p> </li> <li> <p>Shuffle Hash Join (rare)</p> </li> <li> <p>If memory per executor is low, hash table building can cause OOM.</p> </li> </ul>"},{"location":"spark/05-Spark_Salting_Technique/#3-so-if-the-second-table-is-large","title":"\ud83d\udd11 3. So, If the Second Table is Large\u2026","text":"<ul> <li>Yes, you can get OOM if Spark mistakenly tries to broadcast it (or if you force broadcast with a hint).</li> <li> <p>If Spark chooses Sort-Merge Join, OOM is less likely, but still possible if:</p> </li> <li> <p>The salted distribution is still skewed (bad salt choice).</p> </li> <li>Shuffle partitions are too few (<code>spark.sql.shuffle.partitions</code> too low).</li> <li>Executor memory is too small for sort buffers.</li> </ul>"},{"location":"spark/05-Spark_Salting_Technique/#4-how-to-avoid-oom-in-large-second-table","title":"\ud83d\udd11 4. How to Avoid OOM in Large Second Table","text":"<p>\u2705 Best practices:</p> <ol> <li> <p>Do not broadcast large table</p> </li> <li> <p>Check plan with <code>explain()</code>.</p> </li> <li> <p>Disable broadcast for large tables:</p> <pre><code>SET spark.sql.autoBroadcastJoinThreshold = -1;\n</code></pre> </li> <li> <p>Use Sort-Merge Join + Salting</p> </li> <li> <p>Works best for large joins.</p> </li> <li> <p>Tune partitions</p> </li> <li> <p>Increase shuffle partitions (<code>spark.sql.shuffle.partitions</code>).</p> </li> <li> <p>Repartition the large table on join key before join.</p> </li> <li> <p>Use Adaptive Query Execution (AQE)</p> </li> <li> <p>Spark 3+ can dynamically coalesce or split skewed partitions.</p> </li> <li> <p>Enable:</p> <pre><code>SET spark.sql.adaptive.enabled = true;\nSET spark.sql.adaptive.skewJoin.enabled = true;\n</code></pre> </li> <li> <p>Check salt factor</p> </li> <li> <p>Too small \u2192 still skewed.</p> </li> <li>Too big \u2192 data explosion (Cartesian effect).</li> <li>Rule of thumb: <code>salt_factor \u2248 (skewed_key_rows / avg_rows_per_key)</code>.</li> </ol> <p>\u2705 Conclusion:</p> <ul> <li>If the second table is large, Spark will not broadcast it by default (so no OOM in normal SMJ with salting).</li> <li> <p>You\u2019ll only hit OOM if:</p> </li> <li> <p>You force broadcast on a large table, or</p> </li> <li>Shuffle/sort partitions are misconfigured.</li> <li>With AQE + proper salting, Spark can handle large second tables safely.</li> </ul>"},{"location":"spark/06-What_Is_Spark/","title":"What Is Spark","text":""},{"location":"spark/06-What_Is_Spark/#lecture-1-what-is-apache-spark","title":"Lecture 1 : What is Apache Spark","text":""},{"location":"spark/06-What_Is_Spark/#unified","title":"Unified :","text":""},{"location":"spark/06-What_Is_Spark/#computing-engine","title":"Computing Engine:","text":"<p>Spark is not storage platform we can store the data in hdfs, rdbms etc...</p> <p>Spark can process terabytes of data in distributed manner.</p>"},{"location":"spark/06-What_Is_Spark/#compute-cluster","title":"Compute Cluster:","text":"<ul> <li>each slave has 16 GB RAM, 1TB storage and 4 core CPU</li> <li>even master has some data and RAM</li> <li>the above cluster can compute 64 gb of data at a time.</li> <li>the master divides the data among the slave nodes and then slaves process the data.</li> </ul>"},{"location":"spark/07-Why_Apache_Spark/","title":"Why Apache Spark","text":""},{"location":"spark/07-Why_Apache_Spark/#lecture-2-why-apache-spark","title":"Lecture 2 : Why Apache Spark?","text":"<p>Different Databases </p> <p>new formats like video, audio, json,avro started coming in but rdms cannot handle it.</p> <p>volume of data also increased. </p>"},{"location":"spark/07-Why_Apache_Spark/#what-is-big-data","title":"What is Big Data?","text":"<p>Data Lake works on Extract Load Transform architecture</p>"},{"location":"spark/07-Why_Apache_Spark/#issues-with-rdbms","title":"Issues with RDBMS","text":"<ul> <li>Storage</li> <li>Processing - RAM and CPU</li> </ul> <p>Enter Spark...</p> <p></p>"},{"location":"spark/07-Why_Apache_Spark/#monolith-vs-microservice-architecture","title":"Monolith vs Microservice Architecture","text":""},{"location":"spark/08-Hadoop_Vs_Spark/","title":"Hadoop Vs Spark","text":""},{"location":"spark/08-Hadoop_Vs_Spark/#lecture-3-hadoop-vs-spark","title":"Lecture 3 : Hadoop vs Spark","text":""},{"location":"spark/08-Hadoop_Vs_Spark/#misconception","title":"Misconception:","text":"<ul> <li>Hadoop is a database - its not a database just a filesystem (hdfs)</li> <li>Spark is 100 times faster than hadoop</li> <li>Spark processes data in RAM but Hadoop doesnt</li> </ul>"},{"location":"spark/08-Hadoop_Vs_Spark/#differences","title":"Differences","text":""},{"location":"spark/08-Hadoop_Vs_Spark/#performance","title":"Performance","text":"<p>Hadoop does lot of read write IO operations and sends data back and forth to the disk. </p> <p>But in spark each executor has its own memory. </p> <p>Where is there no difference?</p> <p>When we have very less data like 10 GB, there is no difference because the hadoop cluster also doesnt write to the disk it fits first time in memory.</p>"},{"location":"spark/08-Hadoop_Vs_Spark/#batch-vs-stream-processing","title":"Batch vs Stream Processing","text":""},{"location":"spark/08-Hadoop_Vs_Spark/#ease-of-use","title":"Ease of Use","text":"<p>Spark has both low level and high level API in Python which is easier than using Hive. Low level programming is on RDD level.</p>"},{"location":"spark/08-Hadoop_Vs_Spark/#security","title":"Security","text":"<ul> <li> <p>Hadoop has in built Kerberos Authentication via YARN whereas Spark doesnt have any security mechanism.</p> </li> <li> <p>The authentication helps create ACL lists at directory level in HDFS.</p> </li> <li> <p>Spark uses HDFS Storage so it gets ACL feature / ability and when it uses YARN it gets Kerberos Authentication.</p> </li> </ul>"},{"location":"spark/08-Hadoop_Vs_Spark/#fault-tolerance","title":"Fault Tolerance","text":"<p>Data Replication in Hadoop </p> <p>HDFS keeps track of which node / rack has the data from A B C and D</p> <p></p> <p>DAG in Spark</p> <ul> <li>So Spark computes / transforms in multiple processes Process 1 -&gt; Process 2 -&gt; Process 3 ....</li> <li>After each process the data is stored in a data structure called RDD which is immutable. So even if there is a failure Spark engine knows how to reconstruct the data for a particular process from the RDD at that stage.</li> </ul>"},{"location":"spark/09-Spark_Ecosystem/","title":"Spark Ecosystem","text":""},{"location":"spark/09-Spark_Ecosystem/#lecture-4-spark-ecosystem","title":"Lecture 4 : Spark Ecosystem","text":"<p>High Level API : We cna write any SQL queries in python,java etc... there are ML and GraphX librries also.</p> <p>We can write code in many languages. Low Level API : we can make RDD's and work on them.</p> <p></p>"},{"location":"spark/09-Spark_Ecosystem/#where-does-spark-run","title":"Where does Spark run?","text":"<p>Spark Engine would need some memory for transformation.</p> <ul> <li>suppose it needs 4 worker nodes each 20 GB and a driver node of 20 gb.</li> <li>it goes to the cluster manager and asks for total 100 GB of memory, if available then the manager will assign that muuch storage.</li> <li>cluster manager is also called YARN, K8S, Standalone managers</li> </ul>"},{"location":"spark/10-Spark_Ecosystem/","title":"Spark Ecosystem","text":""},{"location":"spark/10-Spark_Ecosystem/#lecture-5-read-modes-in-spark","title":"Lecture 5 : Read Modes in Spark","text":"<p>format data file format : csv,json,jdbc and odbc connection. Format is optional parameter, by default its parquet format</p> <p>option  inferschema, mode and header [optional field]</p> <p>schema manual schema can be passed here</p> <p>load path from where we need to read the data [not optional]</p>"},{"location":"spark/10-Spark_Ecosystem/#dataframereader-api","title":"DataframeReader API","text":"<p>Access it using 'spark.read' in the spark session</p> <p></p>"},{"location":"spark/10-Spark_Ecosystem/#mode-in-spark","title":"<code>mode</code> in Spark","text":""},{"location":"spark/11-Spark_Architecture/","title":"Spark Architecture","text":""},{"location":"spark/11-Spark_Architecture/#lecture-6-spark-architecture","title":"Lecture 6 : Spark Architecture","text":""},{"location":"spark/11-Spark_Architecture/#spark-cluster","title":"Spark Cluster","text":"<ul> <li>20 core per machine and 100 GB RAM / each machine</li> <li>Total Cluster : 200 cores and 1TB RAM</li> </ul> <ul> <li>The master is controlled by Resource Manager and the workers are controlled by Node Manager.</li> </ul>"},{"location":"spark/11-Spark_Architecture/#what-happens-when-user-submits-code","title":"What happens when user submits code?","text":"<ul> <li>The user submits some Spark code for execution to the Resource Manager. It needs 20 GB RAM, 25 GB executor, 5 total executors and 5 CPU cores.</li> <li>So the manager goes to W5 and asks to create 20GB container as the driver node.</li> </ul>"},{"location":"spark/11-Spark_Architecture/#what-happens-inside-the-container","title":"What happens inside the container?","text":""},{"location":"spark/11-Spark_Architecture/#driver-allocation","title":"Driver Allocation","text":"<p>Now this 20 GB driver is called Application Master </p> <p>There are two main() functions inside the master, one is for PySpark and other is for JVM like Java,Scala etc...</p> <p>The JVM main() is called Application Driver.</p> <p>The Spark Core has a Java Wrapper and the Java Wrapper has a Python Wrapper.</p> <p>When we write code in PySpark it gets converted to Java Wrapper.</p> <p>The PySpark driver is not a requirement but the Java Wrapper is required to run any code.</p>"},{"location":"spark/11-Spark_Architecture/#worker-allocation","title":"Worker Allocation","text":"<ul> <li>Now the Application master asks for the executors to be assigned and the resource manager allocates.</li> </ul>"},{"location":"spark/11-Spark_Architecture/#executor-container","title":"Executor Container","text":"<p>Each executor has 5 core CPU and 25GB RAM. Each one of them runs on separate container.</p> <p>THe above is when we have pure Java code and dont use Python UDF.</p> <p>But what if we use Python UDF functions?</p> <p> We need a Python worker inside the executor to be able to run the code.</p>"},{"location":"spark/12-Schema_In_Spark/","title":"Schema In Spark","text":""},{"location":"spark/12-Schema_In_Spark/#lecture-7-schema-in-spark","title":"Lecture 7 : Schema in Spark","text":""},{"location":"spark/12-Schema_In_Spark/#structtype-and-structfield","title":"StructType and StructField","text":"<p>Example:</p> <p></p> <p></p> <p>How to skip the header row?</p> <pre><code>df = spark.read.option(\"skipRows\", 2).csv(\"file.csv\")\n</code></pre>"},{"location":"spark/13-Handling_Corrupt_Records_Spark/","title":"Handling Corrupt Records Spark","text":""},{"location":"spark/13-Handling_Corrupt_Records_Spark/#lecture-8-handling-corrupter-records-in-spark","title":"Lecture 8 : Handling Corrupter Records in Spark","text":""},{"location":"spark/13-Handling_Corrupt_Records_Spark/#how-many-records-in-each-mode","title":"How many records in each mode?","text":""},{"location":"spark/13-Handling_Corrupt_Records_Spark/#permissive-mode","title":"Permissive Mode","text":""},{"location":"spark/13-Handling_Corrupt_Records_Spark/#dropmalformed","title":"DropMalformed","text":""},{"location":"spark/13-Handling_Corrupt_Records_Spark/#how-to-print-corrupted-records","title":"How to Print Corrupted Records","text":"<p>Output </p>"},{"location":"spark/13-Handling_Corrupt_Records_Spark/#how-to-store-corrupted-records","title":"How to Store Corrupted Records","text":"<p>The corrupted records are in json format </p>"},{"location":"spark/14-Spark_Transformations_Actions/","title":"Spark Transformations Actions","text":""},{"location":"spark/14-Spark_Transformations_Actions/#lecture-9-transformations-and-actions-in-spark","title":"Lecture 9 : Transformations and Actions in Spark","text":""},{"location":"spark/14-Spark_Transformations_Actions/#types-of-transformations","title":"Types of Transformations","text":"<ul> <li>Narrow Transformation</li> <li>Wide Transformation</li> </ul> <p>Example: </p> <p>Suppose data is of 200MB. 200MB / 128MB = 2 partitions</p> <p></p> <p>Let's say both partitions go to separate executors.</p> <p>Q1 : Filtering Records  There is no data movement here.</p> <p>Q2: Find Total Income of each employee </p> <p>One id = 2 record is in one partition and the other is in the second partition so we need to do wide transformation </p> <p>Data needs to be shuffled and records with same id must be moved to same partition.</p> <ul> <li>filter,select,union etc are narrow transformations</li> <li>join,groupby,distinct</li> </ul>"},{"location":"spark/15-Spark_DAG_Lazy_Eval/","title":"Spark Dag Lazy Eval","text":""},{"location":"spark/15-Spark_DAG_Lazy_Eval/#lecture-10-dag-and-lazy-evaluation-in-spark","title":"Lecture 10 : DAG and Lazy Evaluation in Spark","text":"<ul> <li>For every action there is a new job, here there are three actions : read,inferSchema,sum and show</li> <li>When used with groupBy().sum(): It is considered an action because it triggers computation to aggregate data across partitions and produce a result. This operation forces Spark to execute the transformations leading up to it, effectively creating a job.</li> <li> <p>When used as a column expression df.select(sum(\"value\")): It acts more like a transformation in Spark's context, especially if part of a larger query or pipeline that does not immediately trigger execution. In this case, it only defines the operation and does not create a job until an action (like show() or collect()) is called.</p> </li> <li> <p>Job for reading file  Whole Stage Codegen - generate Java ByteCode</p> </li> <li> <p>Inferschema </p> </li> <li> <p>GroupBy and Count As explained above this is an action.</p> </li> <li> <p>Show Final action to display df</p> </li> </ul> <p> After we read the csv and inferSchema there are no jobs created since filter and repartition both are transformations not actions.</p> <p>When there are two filters on same dataset</p> <p></p> <p>This is the job </p>"},{"location":"spark/15-Spark_DAG_Lazy_Eval/#optimizations-on-the-filter","title":"Optimizations on the Filter","text":"<p>Both the filters are on the same task  The optimizations can be applied because Spark is lazily evaluated.</p>"},{"location":"spark/16-Spark_JSON_Data/","title":"Spark Json Data","text":""},{"location":"spark/16-Spark_JSON_Data/#lecture-11-working-with-json-data-in-spark","title":"Lecture 11: Working with JSON Data in Spark","text":"<p>Two types of JSON notation:</p> <ul> <li> <p>Line Delimited JSON </p> </li> <li> <p>Multi Line JSON </p> </li> </ul> <pre><code>[\n{\n  \"name\": \"Manish\",\n  \"age\": 20,\n  \"salary\": 20000\n},\n{\n  \"name\": \"Nikita\",\n  \"age\": 25,\n  \"salary\": 21000\n},\n{\n  \"name\": \"Pritam\",\n  \"age\": 16,\n  \"salary\": 22000\n},\n{\n  \"name\": \"Prantosh\",\n  \"age\": 35,\n  \"salary\": 25000\n},\n{\n  \"name\": \"Vikash\",\n  \"age\": 67,\n  \"salary\": 40000\n}\n]\n</code></pre> <p>Line Delimited JSON is more efficient in terms of performance because the compiler knows that each line has one JSON record whereas in multiline json the compiler needs to keept track of where the record ends and the next one starts.</p>"},{"location":"spark/16-Spark_JSON_Data/#different-number-of-keys-in-each-line","title":"Different number of keys in each line","text":"<p>Here what happens is that the line with the extra key has the value while for the rest its null </p>"},{"location":"spark/16-Spark_JSON_Data/#multiline-incorrect-json","title":"Multiline Incorrect JSON","text":"<p>We dont pass a list here rather its just dictionaries <pre><code>{\n  \"name\": \"Manish\",\n  \"age\": 20,\n  \"salary\": 20000\n},\n{\n  \"name\": \"Nikita\",\n  \"age\": 25,\n  \"salary\": 21000\n},\n{\n  \"name\": \"Pritam\",\n  \"age\": 16,\n  \"salary\": 22000\n},\n{\n  \"name\": \"Prantosh\",\n  \"age\": 35,\n  \"salary\": 25000\n},\n{\n  \"name\": \"Vikash\",\n  \"age\": 67,\n  \"salary\": 40000\n}\n</code></pre> When we process the json it just reads the first dictionary as a record and the rest is not processed.</p> <p></p>"},{"location":"spark/16-Spark_JSON_Data/#corrupted-records","title":"Corrupted Records","text":"<p>We dont need to define <code>_corrupted_record</code> in the schema, it will add the column on its ownn</p> <p></p>"},{"location":"spark/17-Spark_SQL_Engine/","title":"Spark Sql Engine","text":""},{"location":"spark/17-Spark_SQL_Engine/#lecture-12-spark-sql-engine","title":"Lecture 12: Spark SQL Engine","text":""},{"location":"spark/17-Spark_SQL_Engine/#how-is-code-converted-into-byte-code","title":"How is code converted into Byte Code?","text":""},{"location":"spark/17-Spark_SQL_Engine/#how-is-spark-code-compiled","title":"How is Spark Code compiled?","text":"<ul> <li>The catalyst optimizer creates a plan and creates RDD lineage</li> </ul>"},{"location":"spark/17-Spark_SQL_Engine/#phases-in-catalyst-optimizer","title":"Phases in Catalyst Optimizer","text":""},{"location":"spark/17-Spark_SQL_Engine/#workflow-diagram","title":"Workflow Diagram","text":"<ul> <li>Unresolved Logical Plan : Bunch of crude steps to execute the SQL code</li> <li>Catalog : The table, files and database metadata information si stored in the catalog. Suppose we call read.csv on file that doesnt exist. The procedure that gives / throws the error is assisted via the catalog. In Analysis phase, we go through these steps. If some file/table is not found then we get Analysis Exception This error occurs when the Logical plan provided is not able to be resolved.</li> <li>Reoslved Logical Plan : This is the phase when we finished analysing the catalog objects.</li> <li>Logical Optimization: There are many examples. Suppose we need just two columns in select output, the spark engine does not fetch all the columns rather jsut fetches the two columns from memory that we need. Another example is when we use multiple filters on the same column in different lines of code. When we execute this code, we see that all of it is executed with or statements in one single line of code.</li> <li>Physical Plan: This involves taking decision like the type of join to use: Broadcast Join is one example. From the logical plan, we can build multiple physical plans. Thebest Physical Plan is a set of RDDs to be run on different executors on the cluster.</li> </ul>"},{"location":"spark/18-Spark_RDD/","title":"Spark Rdd","text":""},{"location":"spark/18-Spark_RDD/#lecture-13-resilient-distributed-dataset","title":"Lecture 13: Resilient Distributed Dataset","text":""},{"location":"spark/18-Spark_RDD/#data-storage-of-list","title":"Data Storage of List","text":""},{"location":"spark/18-Spark_RDD/#data-storage-in-rdd","title":"Data Storage in RDD","text":"<p>Suppose we have 500MB of data and 128MB partition, so we will have 4 partitions.</p> <p>The data is scattered on various executors. </p> <p>Its not in single contiguous location like elements of a list. The data structure used ot process this data is called RDD </p> <p></p> <p>Why is RDD recoverable?</p> <ul> <li> <p>RDD is immutable. If we apply multiple filters each dataset after filtering is a different dataset </p> </li> <li> <p>In below case if rdd2 fails then we can restore rdd1 because of the lineage. </p> </li> </ul>"},{"location":"spark/18-Spark_RDD/#disadvantage-of-rdd","title":"Disadvantage of RDD","text":"<ul> <li>No optimization done by Spark on RDD. The dev must specify explicitly on how to optimize RDD.</li> </ul>"},{"location":"spark/18-Spark_RDD/#advantage","title":"Advantage","text":"<ul> <li>Works well with unstructured data where there are no columns and rows / key-value pairs</li> <li>RDD is type safe, we get error on compile time rather than runtime which happens with Dataframe API.</li> </ul>"},{"location":"spark/18-Spark_RDD/#avoiding-rdds","title":"Avoiding RDDs","text":"<ul> <li>RDD : How to do? Dataframe API: Just specify what to do?</li> </ul> <p> You can see in above case that we have a join and filter but we are specifically saying that first join then filter so it triggers a shuffle first and then filter which is not beneficial.</p>"},{"location":"spark/19-Spark_Writing_Data_Disk/","title":"Spark Writing Data Disk","text":""},{"location":"spark/19-Spark_Writing_Data_Disk/#lecture-15-how-to-write-data-on-the-disk","title":"Lecture 15 : How to write data on the disk?","text":""},{"location":"spark/19-Spark_Writing_Data_Disk/#modes-to-write-data","title":"Modes to write data","text":"<p>Create three files </p> <pre><code>  write_df = read_df.repartition(3).write.format(\"csv\")\\\n    .option(\"header\", \"True\")\\\n    .mode(\"overwrite\")\\  # Using .mode() instead of .option() for overwrite mode\n    .option(\"path\", \"/FileStore/tables/Write_Data/\")\\\n    .save()\n</code></pre>"},{"location":"spark/20-Spark_Partitioning_Bucketing/","title":"Spark Partitioning Bucketing","text":""},{"location":"spark/20-Spark_Partitioning_Bucketing/#lecture-15-how-to-write-data-on-the-disk","title":"Lecture 15 : How to write data on the disk?","text":""},{"location":"spark/20-Spark_Partitioning_Bucketing/#modes-to-write-data","title":"Modes to write data","text":"<p>Create three files </p> <pre><code>  write_df = read_df.repartition(3).write.format(\"csv\")\\\n    .option(\"header\", \"True\")\\\n    .mode(\"overwrite\")\\  # Using .mode() instead of .option() for overwrite mode\n    .option(\"path\", \"/FileStore/tables/Write_Data/\")\\\n    .save()\n</code></pre>"},{"location":"spark/21-Spark_Session_vs_Context/","title":"Spark Session Vs Context","text":""},{"location":"spark/21-Spark_Session_vs_Context/#lecture-17-spark-session-vs-spark-context","title":"Lecture 17 : Spark Session vs Spark Context","text":"<ul> <li>Spark Session is entry point to the Spark cluster where we provide the parameters to create and operate our cluster.</li> <li>Spark session will have different context like one for SQL, PySpark etc...</li> </ul>"},{"location":"spark/22-Spark_Job_Stage_Task/","title":"Spark Job Stage Task","text":""},{"location":"spark/22-Spark_Job_Stage_Task/#lecture-18-job-stage-and-tasks","title":"Lecture 18: Job, Stage and Tasks","text":"<ul> <li>One Application is created.</li> <li>One job is created per action.</li> <li>One stage is defined for every transformation like filter.</li> <li>Task is the actually activity on the data that's happening.</li> </ul>"},{"location":"spark/22-Spark_Job_Stage_Task/#example-of-jobaction-and-task","title":"Example of Job,Action and Task","text":""},{"location":"spark/22-Spark_Job_Stage_Task/#complete-flow-diagram","title":"Complete flow diagram","text":"<p>Every job has minimum one stage and one task.</p> <p> Repartition to filter is one job because we dont hit an action in between.</p> <p>Every wide dependency transformation has its own stage. All narrow dependency transformations come in one stage as a DAG.</p> <p></p>"},{"location":"spark/22-Spark_Job_Stage_Task/#how-do-tasks-get-created-read-and-write-exchange","title":"How do tasks get created? [Read and Write Exchange]","text":"<ul> <li>The repartition stage actually is a wide dependency transformation and creates two partitions from one, its a Write exchange of data.</li> <li>Now the filter and select stage reads this repartitioned data(Read exchange) and filter creates two tasks because we have two partitions.</li> <li>Next we need to find out how many folks earn &gt; 90000 and age &gt; 25 so we need to do a groupby that's a wide dependency transformation and it creates another stage. By default there are 200 partitions created.</li> <li>So some partitions may have data and some wont.</li> </ul>"},{"location":"spark/23-Spark_Transformations/","title":"Spark Transformations","text":""},{"location":"spark/23-Spark_Transformations/#lecture-17-dataframe-transformations-in-spark-part-1","title":"Lecture 17: Dataframe Transformations in Spark Part 1","text":"<p> Data gets stored in Row() format in the form of bytes</p> <p></p> <p>Columns are expressions. Expressions are set of transformations on more than one value in a record.</p>"},{"location":"spark/23-Spark_Transformations/#ways-to-select-values-columns","title":"Ways to select values / columns","text":"<p>Column Manipulations</p> <p></p> <p>Other methods </p> <p>selectExpr </p> <p>Aliasing Columns </p>"},{"location":"spark/23-Spark_Transformations/#lecutre-18-dataframe-transformations-in-spark-part-ii","title":"Lecutre 18 : Dataframe Transformations in Spark Part II","text":""},{"location":"spark/23-Spark_Transformations/#filter-where-no-difference","title":"<code>filter()</code> / <code>where()</code> no difference","text":""},{"location":"spark/23-Spark_Transformations/#multiple-filter-conditions","title":"Multiple filter conditions","text":""},{"location":"spark/23-Spark_Transformations/#literals-in-spark","title":"Literals in spark","text":"<p>Used to pass same value in all the columns </p>"},{"location":"spark/23-Spark_Transformations/#adding-columns","title":"Adding Columns","text":"<p>If the column already exists then it gets overwritten. </p>"},{"location":"spark/23-Spark_Transformations/#renaming-columns","title":"Renaming Columns","text":""},{"location":"spark/24-Spark_Union_vs_UnionAll/","title":"Spark Union Vs Unionall","text":""},{"location":"spark/24-Spark_Union_vs_UnionAll/#lecture-19-union-vs-unionall","title":"Lecture 19: union vs unionAll()","text":"<p>We can see that here we have a duplicate id </p> <p>In PySpark union and unionAll behaves in the same way, both retain duplicates </p> <p>But in Spark SQL when we do union it drops the duplicate records </p> <p></p>"},{"location":"spark/24-Spark_Union_vs_UnionAll/#selecting-data-and-unioning-the-same-table","title":"Selecting data and unioning the same table","text":""},{"location":"spark/24-Spark_Union_vs_UnionAll/#what-happens-when-we-change-the-order-of-the-columns","title":"What happens when we change the order of the columns?","text":"<p><code>wrong_manager_df</code> actually has the wrong order of columns but still we get the union output but in a wrong column values. </p> <p>If we give different number of columns an exception is thrown. </p> <p>If we use unionByName then the column names on both dfs must be the same. </p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/","title":"Spark Repartition Vs Coalesce","text":""},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#lecture-19-repartitioning-and-coalesce","title":"Lecture 19: Repartitioning and Coalesce","text":"<p>Suppose we have 5 partitions and one of them is skewed a lot 100MB, let's say this is the best selling product records. This partition takes lot of time to compute. So the other executors have to wait until this executor finishes processing. </p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#repartitioning-vs-coalesce","title":"Repartitioning vs Coalesce","text":""},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#repartitioning","title":"Repartitioning","text":"<p>Suppose we have the above partitions and total data is 100mb. let's say we do repartition(5) so we will have 5 partitions now for the data with 40mb per partition.</p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#coalesce","title":"Coalesce","text":"<p>In case of coalesce there is no equal splitting of partition memory, rather the already existing partitions get merged together. </p> <p>There is no shuffling in coalesce but in repartitioning there is shuffling of data.</p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#pros-and-cons-in-repartitioning","title":"Pros and Cons in repartitioning","text":"<ul> <li>There is evenly distributed data.</li> <li>Con is that IO operations are more, its expensive.</li> <li>Con of coalesce is that the data is unevenly distributed.</li> </ul> <p>Repartitioning can increase or decrease the partitions but coalescing can only decrease the partitions.</p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#how-to-get-number-of-partitions","title":"How to get number of partitions?","text":"<p><code>flight_df.rdd.getNumPartitions()</code> gets the initial number of partitions and then we can repartition <code>flight_df.repartition(4)</code>. Data is evenly distributed.</p> <p></p> <p>Repartitioning based on columns</p> <p></p> <p>Since we asked for 300 partitions and we have 255 records some partitions will have null record. </p>"},{"location":"spark/25-Spark_Repartition_vs_Coalesce/#coalescing","title":"Coalescing","text":"<p> Suppose we have 8 partitions and we coalesce into 3 partitions. Coalesce has only one arg.</p> <p>Uneven distribution of data in partitions. </p>"},{"location":"spark/26-Spark_Case_When/","title":"Spark Case When","text":""},{"location":"spark/26-Spark_Case_When/#lecture-20-case-when-if-else-in-spark","title":"Lecture 20 : Case when / if else in Spark","text":""},{"location":"spark/26-Spark_Case_When/#apply-logic-on-one-column-then-process-if-else-logic","title":"Apply logic on one column then process if else logic","text":""},{"location":"spark/26-Spark_Case_When/#spark-sql-logic","title":"Spark SQL Logic","text":""},{"location":"spark/27-Spark_Unique_Sorted_Records/","title":"Spark Unique Sorted Records","text":""},{"location":"spark/27-Spark_Unique_Sorted_Records/#lecture-21-unique-and-sorted-records","title":"Lecture 21 : Unique and Sorted Records","text":""},{"location":"spark/27-Spark_Unique_Sorted_Records/#distinct","title":"distinct()","text":"<p>Original Data </p> <p>Distinct Data </p> <p>Distinct Based on certain columns </p> <p>\u26a0\ufe0f Distinct takes no arguments we need to select the columns first and then apply distinct.</p>"},{"location":"spark/27-Spark_Unique_Sorted_Records/#dropping-duplicate-records","title":"Dropping duplicate records","text":"<p>Point to note is that the dataframe <code>manager_df</code> has no changes, it just shows the records after dups have been dropped. </p>"},{"location":"spark/27-Spark_Unique_Sorted_Records/#sort","title":"sort()","text":"<p>Descending order </p> <p>Sorting on multiple columns</p> <p>Here first the salary is srranged in desc order then we arrange the name in asc order from those records with same salary. </p>"},{"location":"spark/28-Spark_Agg_Functions/","title":"Spark Agg Functions","text":""},{"location":"spark/28-Spark_Agg_Functions/#lecture-22-aggregate-functions","title":"Lecture 22 : Aggregate functions","text":""},{"location":"spark/28-Spark_Agg_Functions/#count-as-both-action-and-transformation","title":"Count as both Action and Transformation","text":"<p>\u26a0\ufe0f When we are doing count on a single column and there is a null in it, its not considered in the count. But for all columns we have nulls in the count. </p> <p></p> <p>Above case when we do <code>df.count()</code> the rows that have all duplicates are counted and we get 10 records but when we do <code>df.select('name').count()</code> then we get 8 because there are two nulls in name column.</p> <p>Job created in first case and its not created in second case below. </p>"},{"location":"spark/30-Spark_Group_By/","title":"Spark Group By","text":""},{"location":"spark/30-Spark_Group_By/#lecture-23-group-by-in-spark","title":"Lecture 23: Group By In Spark","text":"<p>Sample Data</p> <p></p>"},{"location":"spark/30-Spark_Group_By/#questions","title":"Questions","text":"<p>Salary per department using groupBy() </p>"},{"location":"spark/30-Spark_Group_By/#where-do-we-use-window-functions","title":"Where do we use window functions?","text":"<p>Suppose we need to find out the percentage of total salary from a particular dept that the person is earning. we can use window function to specify the total salary per department in the particular record itself like I've shown below. </p> <p>This way we dont need to perform a join.</p> <p></p>"},{"location":"spark/30-Spark_Group_By/#grouping-by-two-columns","title":"Grouping by two columns","text":""},{"location":"spark/31-Spark_Joins_Intro/","title":"Spark Joins Intro","text":""},{"location":"spark/31-Spark_Joins_Intro/#lecture-24-joins-in-spark-part-1","title":"Lecture 24 : Joins in Spark part 1","text":"<p>Which customers joined platform but never brought anything?</p> <p></p> <p>Whenever we need information from another table, we use joins and there should be some common column.</p> <p>Join is a costly wide dependency operation.</p>"},{"location":"spark/31-Spark_Joins_Intro/#how-do-joins-work","title":"How do joins work?","text":"<p>How many records do we get after inner joining the below two tables. </p> <p>We get a total of 9 records. </p> <p>Sometimes data gets duplicated when we do joins, so we should use distinct() but remember distinct is wide dependency transform.</p>"},{"location":"spark/31-Spark_Joins_Intro/#lecture-25-types-of-join-in-spark","title":"Lecture 25 : Types of Join in Spark","text":""},{"location":"spark/31-Spark_Joins_Intro/#inner-join","title":"Inner Join","text":""},{"location":"spark/31-Spark_Joins_Intro/#left-join","title":"Left Join","text":"<p> All records in left table + those that join with right table, whereever we dont get match on right table the columns become null.</p>"},{"location":"spark/31-Spark_Joins_Intro/#right-join","title":"Right Join","text":""},{"location":"spark/31-Spark_Joins_Intro/#full-outer-join","title":"Full Outer Join","text":""},{"location":"spark/31-Spark_Joins_Intro/#left-semi-join","title":"Left Semi Join","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"LeftSemiJoinExample\").getOrCreate()\n\n# Left DataFrame: Orders\norders = spark.createDataFrame([\n    (1, \"iPhone\"),\n    (2, \"Pixel\"),\n    (3, \"OnePlus\"),\n    (4, \"Nokia\")\n], [\"customer_id\", \"product\"])\n\n# Right DataFrame: Valid Customers\nvalid_customers = spark.createDataFrame([\n    (1,), (3,)\n], [\"customer_id\"])\n\n# Perform left semi join\nfiltered_orders = orders.join(valid_customers, on=\"customer_id\", how=\"left_semi\")\nfiltered_orders.show()\n</code></pre> <p>Output</p> <pre><code>+-----------+--------+\n|customer_id|product |\n+-----------+--------+\n|          1|iPhone  |\n|          3|OnePlus |\n+-----------+--------+\n</code></pre>"},{"location":"spark/31-Spark_Joins_Intro/#left-anti-join","title":"Left Anti Join","text":"<p>Find out all customers who have never purchased any product.</p>"},{"location":"spark/31-Spark_Joins_Intro/#cross-join","title":"Cross Join","text":"<p>Never use cross join! </p> <p></p>"},{"location":"spark/32-Spark_Join_Strategies/","title":"Spark Join Strategies","text":""},{"location":"spark/32-Spark_Join_Strategies/#lecture-26-join-strategies-in-spark","title":"Lecture 26 : Join Strategies in Spark","text":"<p>Joins are expensive due to shuffling.</p> <p>4 partitions are there in each dataframe.</p> <p></p> <p>Executors in the cluster</p> <p></p> <p>Now we need to join employee and salary df to get the output but they are on different executors, so we need to do data shuffling.</p> <p>Each executor has 200 partitions. Goal is to get all same keys in one executor. </p> <p></p> <p></p> <ul> <li>Since we want to get id for 1 we divide 1/200 = 1 and then send all the data to that executor 1.</li> </ul> <p></p> <p>Suppose we want to map the salary for id = 7 so the data from the employee df with id = 7 and also salary df with id=7 will come into the executor 7.</p> <p>Similarly id = 201 will go into 201/200 = executor no 1.</p>"},{"location":"spark/32-Spark_Join_Strategies/#types-of-join-strategies","title":"Types of Join Strategies","text":"<p>Joins generally result in shuffling</p> <p>There are two dataframes df1 and df2 each with 4 partitions.</p> <p></p> <p>We have two executors.</p> <p>In join goal is to join with same keys.</p> <p></p> <p>We can see that red P1 has corresponding id for salary in the other executor.</p> <p></p> <p>We need to get same keys fetched from other executors.</p> <p>When a dataframe is sent to executors by default 200 partitions are created per dataframe.</p> <p></p> <p>Now let's say we want to find salary for id = 1 we can divide 1/200 on blue = 1 and 1/200 on red = 1, so both data will come into executor 1 in the partition 1.</p> <p></p> <p>Similarly for id = 7 also we will send the data on blue and red P7</p> <p>But if id = 201 then 201/200 = 1 so this id will come into P1 only.</p> <p>If we have id = 102 then 102/200 = 102 partition on 2nd executor.</p> <p></p> <p>The executors can be on different worker nodes also, we need to then move data across from one worker node to other.</p>"},{"location":"spark/32-Spark_Join_Strategies/#strategies","title":"Strategies","text":"<p>Broadcast nested loop join is costly because we dont do a straight join, rather its based on &lt; an &gt; conditions, its O(n^2)</p>"},{"location":"spark/32-Spark_Join_Strategies/#shuffle-sort-merge-join","title":"Shuffle Sort Merge Join","text":"<p>TC : O(nlogn)</p>"},{"location":"spark/32-Spark_Join_Strategies/#shuffle-hash-join","title":"Shuffle Hash Join","text":"<p>The smaller table gets a hash table created with hashed keys in memory.</p> <p>Now from df1 we checked which keys match with O(1) lookup using the hash table.</p> <p></p>"},{"location":"spark/32-Spark_Join_Strategies/#broadcast-join","title":"Broadcast Join","text":"<p>The tables that are less than 100mb can be broadcast.</p> <p>Scenario : Suppose one table is 1GB size so we will have 1000MB / 128MB = 8 partitions and there is another table of size 5mb.</p> <p>So if we dont broadcast, then the df with 100gb should be shuffled around with 5mb data across executors for joining. Instead of that we will just send the small df in all the executors so that there is no shuffling.</p> <p></p> <p>The amount of data that can be broadcast depends on the memory of executor and driver. Make sure that there is no case where driver memory is 2GB and we are trying to broadcast 1GB data.</p>"},{"location":"spark/32-Spark_Join_Strategies/#demo","title":"Demo","text":"<p>There are total 200 partitions when we join </p> <p></p> <p>Normal Sort Merge Join Execution Plan</p> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Project [sale_id#10484L, sale_date#10485, amount#10486L, country_name#10514]\n   +- SortMergeJoin [country_id#10487L], [country_id#10513L], Inner\n      :- ColumnarToRow\n      :  +- PhotonResultStage\n      :     +- PhotonSort [country_id#10487L ASC NULLS FIRST]\n      :        +- PhotonShuffleExchangeSource\n      :           +- PhotonShuffleMapStage\n      :              +- PhotonShuffleExchangeSink hashpartitioning(country_id#10487L, 1024)\n      :                 +- PhotonFilter isnotnull(country_id#10487L)\n      :                    +- PhotonRowToColumnar\n      :                       +- LocalTableScan [sale_id#10484L, sale_date#10485, amount#10486L, country_id#10487L]\n      +- ColumnarToRow\n         +- PhotonResultStage\n            +- PhotonSort [country_id#10513L ASC NULLS FIRST]\n               +- PhotonShuffleExchangeSource\n                  +- PhotonShuffleMapStage\n                     +- PhotonShuffleExchangeSink hashpartitioning(country_id#10513L, 1024)\n                        +- PhotonFilter isnotnull(country_id#10513L)\n                           +- PhotonRowToColumnar\n                              +- LocalTableScan [country_id#10513L, country_name#10514]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n        Unsupported node: SortMergeJoin [country_id#10487L], [country_id#10513L], Inner.\n\nReference node:\n    SortMergeJoin [country_id#10487L], [country_id#10513L], Inner\n</code></pre> <p>Spark UI Diagram</p> <p></p> <p></p> <p>Broadcast Join Execution Plan</p> <pre><code>== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [sale_id#10484L, sale_date#10485, amount#10486L, country_name#10514]\n         +- PhotonBroadcastHashJoin [country_id#10487L], [country_id#10513L], Inner, BuildRight, false, true\n            :- PhotonFilter isnotnull(country_id#10487L)\n            :  +- PhotonRowToColumnar\n            :     +- LocalTableScan [sale_id#10484L, sale_date#10485, amount#10486L, country_id#10487L]\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage\n                  +- PhotonShuffleExchangeSink SinglePartition\n                     +- PhotonFilter isnotnull(country_id#10513L)\n                        +- PhotonRowToColumnar\n                           +- LocalTableScan [country_id#10513L, country_name#10514]\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n</code></pre> <p></p>"},{"location":"spark/33-Spark_Window_Functions/","title":"Spark Window Functions","text":""},{"location":"spark/33-Spark_Window_Functions/#window-functions-in-spark","title":"Window functions in Spark","text":""},{"location":"spark/33-Spark_Window_Functions/#rank-vs-dense-rank","title":"Rank vs Dense Rank","text":"<p>Dense rank does not leave any gaps between the ranks.</p> <p></p>"},{"location":"spark/33-Spark_Window_Functions/#lead-and-lag","title":"Lead and Lag","text":""},{"location":"spark/33-Spark_Window_Functions/#range-and-row-between","title":"Range and Row Between","text":"<p>Q1</p> <p></p> <p>Using first and last functions let's try to acheive this.</p> <p>Data:</p> <p></p> <p>This solution is wrong, ideally we should get 111000 in all rows of <code>latest_sales</code> column.</p> <p></p> <p>Let's look at explain plan.</p> <p>We can see that the window here is <code>unbounded preceeding and current row</code></p> <p></p> <p>What do these terms mean?</p> <p></p> <ul> <li>Unbounded preceeding : If i'm standing at a current row in a window I will return the result of any operation on the window from here to all the rows before me in the window.</li> <li>current_row : the row im standing at.</li> <li>Unbounded following : opposite of unbounded preceeding.</li> <li>rows_between(start_row,end_row) : basically the row we are currently at is 0, all rows before that are negative numbers and all rows after that is positive numbers.</li> </ul> <p></p> <p>If we dont give anything then it just goes from current row to either unbounded preceeding (first row) of window or unbounded following (last row) of window.</p> <p>Converting from string to unixtime when we have two fields date and time.</p> <p></p> <p><code>emp_df = emp_df.withColumn(\"timestamp\",from_unixtime(unix_timestamp(expr(\"CONCAT(date,' ',time)\"),\"dd-MM-yyyy HH:mm\")))</code></p> <p>The timestamp column is a string.</p> <p></p>"},{"location":"spark/34-Spark_Memory_Management/","title":"Spark Memory Management","text":""},{"location":"spark/34-Spark_Memory_Management/#spark-memory-management","title":"Spark Memory Management","text":"<p>If we do <code>df.range(100000)</code> and then do <code>df.collect()</code> on 1Gb driver we get OOM error</p> <p></p> <p>Spark Architecture</p> <p></p> <p>Driver memory is of two types:</p> <ul> <li>spark.driver.memory</li> <li>spark.driver.memoryOverhead</li> </ul> <p></p> <p>With collect all records go into the driver. But with show just one partition gets sent to the heap space.</p> <p>\ud83c\udfaf Think of the Spark Driver Like a Worker</p> <p>Imagine the Spark driver is a person doing a big task at a desk.</p> <p>The desk = spark.driver.memory (main memory)</p> <p>The room around the desk = spark.driver.memoryOverhead (extra space to move, store tools, use side tables)</p> <p>\ud83e\udde0 Why Just the Desk Isn\u2019t Enough</p> <p>Let\u2019s say the driver (person) is:</p> <p>Writing on paper (standard Spark tasks)</p> <p>Using a laptop (Python/PySpark or native code)</p> <p>Holding tools and files (temporary data, buffers, network stuff)</p> <p>Only giving them a desk (spark.driver.memory) isn't enough:</p> <p>The laptop (native code, Python UDFs) might need space outside the desk</p> <p>The tools (Spark internals, shuffle, serialization) don\u2019t fit on the desk \u2014 they use off-heap memory</p> <p>If you don\u2019t give them enough room around the desk (memoryOverhead), they might trip over stuff and fail the task.</p> <p>\ud83e\uddea Real Spark Example When you run PySpark like this:</p> <pre><code>df.withColumn(\"double\", my_udf(df[\"col\"]))\n</code></pre> <p>That Python UDF runs outside the JVM. It needs extra native memory, not regular Java memory.</p> <p>Spark says:</p> <p>\u201cI\u2019ll use driver.memory for my JVM, but I need some memoryOverhead for the native stuff.\u201d</p> <p>\u2705 Summary (in 1 line)</p> <pre><code>spark.driver.memory is for Spark's own work (Java),\nspark.driver.memoryOverhead is for everything outside the JVM \u2014 like Python, shuffle, native code.\n</code></pre> <p>The memory overhead is <code>max(384mb,10% of driver memory)</code></p> <p></p> <p>Let's say there is <code>df1</code> and we want to join it with two small tables <code>df2</code> and <code>df3</code>.</p> <p>We send both df2 and df3 to the driver.</p> <p></p> <p>Let's say we now give 5 dayasets worth 250 mb and the total driver space is 1G.</p> <p>If rest 750mb is not enough for other processes then the driver will give OOM exception.</p> <p>\ud83d\udca5 So\u2026 How Can GC Cause Out of Memory (OOM)?</p> <p>You\u2019d think GC helps prevent OOMs \u2014 and it does! But in high-memory-pressure situations, it can actually cause or worsen them.</p> <p>\ud83d\udea8 Here\u2019s how it happens: 1. Too Many Objects / Too Much Data in Memory You load huge datasets or perform wide transformations (e.g., groupBy, join).</p> <p>Spark stores a lot of intermediate data in RAM (JVM heap).</p> <p>\ud83d\udc49 JVM tries to make space by running GC again and again.</p> <ol> <li>GC Takes Too Long If GC runs too often or too long (e.g., &gt; 30s), the JVM thinks something\u2019s wrong.</li> </ol> <p>You get:</p> <pre><code>java.lang.OutOfMemoryError: GC overhead limit exceeded\n</code></pre> <p>This means:</p> <p>\u201cGC is using 98% of the CPU but only recovering 2% of memory \u2014 I give up.\u201d</p> <ol> <li>GC Can\u2019t Free Anything Some objects (like cached RDDs or references from your code) stay in memory.</li> </ol> <p>GC runs but can't collect them because they're still \"referenced\".</p> <p>Eventually, JVM runs out of space and crashes with:</p> <pre><code>java.lang.OutOfMemoryError: Java heap space\n\u26a0\ufe0f Common Scenarios in Spark\nCause   Result\nLarge shuffles / joins  Too many objects in memory\nCaching huge RDDs   Heap filled, GC can't recover\nImproper partitions Few tasks \u2192 huge memory per task\nMemory leaks (bad code) Uncollectable references\n</code></pre> <p>Example code</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.storagelevel import StorageLevel\nimport random\n\nspark = SparkSession.builder \\\n    .appName(\"OOM-GC-Demo\") \\\n    .config(\"spark.driver.memory\", \"1g\") \\\n    .getOrCreate()\n\n# Create a large DataFrame with few partitions (causes memory pressure)\ndata = [(i % 10, random.randint(1, 1000)) for i in range(10_000_000)]  # 10 million rows\ndf = spark.createDataFrame(data, [\"group_id\", \"value\"])\n\n# Force a wide transformation + cache\nresult = df.groupBy(\"group_id\").count().persist(StorageLevel.MEMORY_ONLY)\n\n# Trigger action\nresult.count()\n</code></pre> <p>\u2705 How to Fix</p> <p>Increase spark.executor.memory or spark.driver.memory</p> <p>Use persist(StorageLevel.DISK_ONLY) if RAM is tight</p> <p>Avoid huge wide transformations without enough partitions</p> <p>Tune GC (G1GC is often better for large heaps)</p>"},{"location":"spark/35-Spark_Executor_OOM/","title":"Spark Executor Oom","text":""},{"location":"spark/35-Spark_Executor_OOM/#executor-memory-oom","title":"Executor Memory OOM","text":"<p>10 GB per executor and 4 cores</p> <p>Expanding one executor</p> <p></p> <p></p> <p>Exceeding either 10GB or 1GB leads to OOM</p> <p></p>"},{"location":"spark/35-Spark_Executor_OOM/#how-is-10gb-divided","title":"How is 10GB divided?","text":""},{"location":"spark/35-Spark_Executor_OOM/#what-does-each-part-of-the-user-memory-do","title":"What does each part of the user memory do?","text":"<ol> <li>Reserved Memory</li> </ol> <p>Minimum 450mb must be our memory of executor.</p> <p></p> <ol> <li>User Memory </li> </ol> <p></p> <ol> <li>Storage Memory Usage</li> </ol> <p></p> <ol> <li>Executor Memory Usage</li> </ol> <p></p>"},{"location":"spark/35-Spark_Executor_OOM/#what-does-each-part-of-the-spark-memory-do","title":"What does each part of the spark memory do?","text":"<p>\u2699\ufe0f Background: Memory in Spark Executors</p> <p>Each executor in Spark has a limited memory budget. This memory is split for:</p> <ul> <li> <p>Execution Memory: used for joins, aggregations, shuffles</p> </li> <li> <p>Storage Memory: used for caching RDDs or DataFrames</p> </li> <li> <p>User Memory: everything else (broadcast vars, UDFs, JVM overhead)</p> </li> </ul> <p>\ud83d\udd04 1. Static Memory Manager (Old)</p> <p>This was Spark's memory model before Spark 1.6.</p> <p>\ud83d\udd27 How It Works:</p> <ul> <li>Fixed memory boundaries set in config.</li> <li>You manually allocate how much memory goes to:</li> <li>Storage (RDD cache)</li> <li>Execution (shuffles, joins)</li> <li>If storage fills up \u2192 cached blocks are evicted.</li> <li>No sharing between execution and storage.</li> </ul> <p>Example fractions</p> <pre><code>spark.storage.memoryFraction = 0.6\nspark.shuffle.memoryFraction = 0.2\n</code></pre> <p>\ud83d\udd04 2. Unified Memory Manager (Modern - Default)</p> <p>Introduced in Spark 1.6+ and is default since Spark 2.0.</p> <p>\ud83d\udd27 How It Works:</p> <p>Combines execution + storage into a single unified memory pool.</p> <p>Dynamic memory sharing: if execution needs more, storage can give up memory \u2014 and vice versa.</p> <p>Much more flexible and efficient.</p> <p>\u2705 Benefits:</p> <ul> <li>Less tuning needed</li> <li>Avoids wasted memory in one region while another needs more</li> <li>Better stability under pressure</li> </ul> <p>In bwlo case execution memory is empty so storage mmemory uses more of execution memory for caching</p> <p></p> <p>Now executor does some work in blue boxes</p> <p></p> <p>Now entire memory is full, so we need to evict some data that has been cached. This happens in LRU fashion.</p> <p></p> <p>Now let's say executor has entire memory used 2.9 something gb... but it needs more memory.</p> <p></p> <p>If the storage pool memory is free it can utilize that.</p> <p></p> <p>If the storage pool is also full, then we get OOM!!!</p>"},{"location":"spark/35-Spark_Executor_OOM/#when-can-we-neither-evict-the-data-nor-spill-to-disk","title":"When can we neither evict the data nor spill to disk?","text":"<p>Suppose we have two dataframes df1 and df2 and the key id = 1 is heavily skewed in both dataframes, and its 3GB</p> <p>Since we need to get all the data from df1 and df2 with id = 1 onto the same executor to perform the join, we have just 2.9GB but the data is 3gb so it gives OOM.</p> <p></p> <p></p> <p>We can handle 3-4 cores per executor beyond that we get memory executor error.</p> <p>\u2753 When can Spark neither evict nor spill data from executor memory?</p> <p>This happens when both eviction and spilling are not possible, and it leads to:</p> <p>\ud83d\udca5 OutOfMemoryError in executors.</p> <p>\u2705 These are the main scenarios:</p> <p>\ud83e\uddf1 1. Execution Memory Pressure with No Spill Support</p> <p>Execution memory is used for:</p> <ul> <li>Joins (SortMergeJoin, HashJoin)</li> <li>Aggregations (groupByKey, reduceByKey)</li> <li>Sorts</li> </ul> <p>Some operations (like hash-based aggregations) need a lot of memory, and not all are spillable.</p> <p>\ud83d\udd25 Example:</p> <p><pre><code>df.groupBy(\"user_id\").agg(collect_set(\"event\"))\n</code></pre> If collect_set() builds a huge in-memory structure (e.g., millions of unique events per user)</p> <p>And that structure can\u2019t be spilled to disk</p> <p>And execution memory is full</p> <p>\ud83d\udc49 Spark can\u2019t evict (no caching), and can\u2019t spill (not supported for this op) \u2192 \ud83d\udca3 OOM</p> <p>\ud83d\udd01 2. Execution Takes Priority, So Storage Can't Evict Enough</p> <p>In Unified Memory Manager, execution gets priority over storage.</p> <p>But sometimes, even after evicting all cache, execution still doesn\u2019t get enough memory.</p> <p>\ud83d\udd25 Example: - You cached a large DataFrame. - Then you do a massive join.</p> <p>Spark evicts all cached data, but still can't free enough memory.</p> <p>\ud83d\udc49 No more memory to give \u2192 \ud83d\udca5</p> <p>User Code holding References</p> <p>\ud83c\udf55 Imagine Spark is a Pizza Party Spark is throwing a pizza party. You and your friends (the executors) are each given a plate (memory) to hold some pizza slices (data).</p> <p>The rule is:</p> <p>\u201cEat your slice, then give your plate back so someone else can use it.\u201d</p> <p>\ud83d\ude2c But You Keep Holding Your Plate You finish your slice, but instead of giving the plate back, you say:</p> <p>\u201cHmm\u2026 I might want to lick the plate later,\u201d so you hold on to it.</p> <p>And you keep doing this with every plate \ud83c\udf7d\ufe0f.</p> <p>Now, you have 10 plates stacked up, all empty, but you're still holding them.</p> <p>\ud83c\udf55 But There\u2019s a Problem\u2026 Spark wants to serve more pizza (more data), but now there are no plates left. Even though you\u2019re not using yours, Spark can\u2019t take them back, because you\u2019re still holding on.</p> <p>\ud83d\udca5 Result? Spark gets frustrated and says:</p> <p>\u201cI\u2019m out of plates! I can\u2019t serve any more pizza!\u201d</p> <p>That\u2019s when Spark crashes with a memory error (OOM) \u2014 because it can\u2019t clean up the memory you're holding onto.</p> <p>\u2705 What Should You Do? Let go of the plates as soon as you're done eating (i.e., don\u2019t store data in variables or lists forever).</p> <p>That way, Spark can reuse memory and everyone gets more pizza. \ud83c\udf55</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"HoldingReferencesOOM\") \\\n    .config(\"spark.driver.memory\", \"1g\") \\\n    .getOrCreate()\n\n# Create a large DataFrame\ndf = spark.range(1_000_000)  # 1 million rows\n\n# \u274c BAD: Holding all rows in a Python list\nall_data = df.collect()  # Loads entire DataFrame into driver memory\n\n# Still holding reference to a big object\n# Spark can't clean this up because Python is holding it\n\n# Do more operations\ndf2 = df.selectExpr(\"id * 2 as double_id\")\ndf2.show()\n</code></pre> <p>Spark wants to free memory, but it can\u2019t, because your code is still holding a reference to the list <code>all_list</code> is still a reference and even though we may not use it later Java GC doesnt know that. its like we finish playing with a teddy bear but still hold onto it, the teacher thinks we are still playing with it, so they cant take it back.</p> <pre><code>df = spark.range(1_000_000)\n\n# \u2705 Process data without collecting everything into memory\ndf.filter(\"id % 2 == 0\").show(10)  # only shows first 10 rows\n</code></pre>"},{"location":"spark/36-Spark_Submit_Command/","title":"Spark Submit Command","text":""},{"location":"spark/36-Spark_Submit_Command/#lecture-27-spark-submit","title":"Lecture 27: Spark Submit","text":"<p>Spark submit is a command line tool to run spark applications, it packages the spark code and runs on cluster.</p> <p>The spark cluster can be standalone,local,K8s or YARN.</p>"},{"location":"spark/36-Spark_Submit_Command/#spark-submit-command","title":"Spark Submit Command","text":"<p>Master can run on <code>yarn</code>,<code>local</code> or <code>k8s</code></p> <p><code>deploy-mode</code> -&gt; specifies where driver runs</p> <p><code>--class</code> -&gt; not required for python, just scala or java</p> <p><code>--jars</code> -&gt; my sql connector jar files</p> <p><code>spark.dynamicAllocation.enabled</code> -&gt; free's up some memory if we are not using it</p> <p></p> <p>We provide two arguments to <code>main.py</code> file. </p> <p></p> <p>We can provide syntax to generate log file. </p> <p>The local system computer from where we run the command is called edge node.</p>"},{"location":"spark/37-Spark_Deployment_Modes/","title":"Spark Deployment Modes","text":""},{"location":"spark/37-Spark_Deployment_Modes/#lecture-28-deployment-modes-in-spark","title":"Lecture 28 : Deployment Modes in Spark","text":"<p>Below is the normal Spark Architecture</p> <p></p> <p>Here we have a separate EC2 instance called edge node. Its configuration is not as much as the other nodes.</p> <p></p> <p>User does not connect directly to the cluster rather connects to the edge node now.</p> <p>They can login to the edge node and perform tasks. Kerberos is used for Authentication and Authorization.</p> <p>Any data that needs to be submitted to cluster also goes through edge node.</p> <p>The /bin/spark-submit folder is on the edge node, it contains hadoop client libaries YARN is not installed here.</p> <p></p>"},{"location":"spark/37-Spark_Deployment_Modes/#client-mode-deployment","title":"client mode deployment","text":"<p>Driver is made on the edge node.</p>"},{"location":"spark/37-Spark_Deployment_Modes/#cluster-mode","title":"cluster mode","text":"<p>In cluster mode, the driver is created on the cluster.</p>"},{"location":"spark/37-Spark_Deployment_Modes/#pros-and-cons-of-client-mode","title":"pros and cons of client mode","text":"<p>Pro : </p> <ul> <li>The user can see the cluster logs on their own system.</li> </ul> <p>Con : </p> <ul> <li>Once the driver in the local system shuts down, the executors also go down.</li> <li>When we submit on client mode we will have network latency. Two way communication creates lot of delay.</li> </ul> <p></p> <p>In cluster mode, we are given an application id and using that we can see the spark ui details.</p> <p></p>"},{"location":"spark/38-Spark_Adaptive_Query_Execution/","title":"Spark Adaptive Query Execution","text":""},{"location":"spark/38-Spark_Adaptive_Query_Execution/#lecture-29-adaptive-query-execution","title":"Lecture 29: Adaptive Query Execution","text":""},{"location":"spark/38-Spark_Adaptive_Query_Execution/#features-of-aqe","title":"Features of AQE","text":""},{"location":"spark/38-Spark_Adaptive_Query_Execution/#dynamically-coalescing-shuffle-partitions","title":"Dynamically Coalescing Shuffle Partitions","text":"<p>Sugar is best selling product it has highest data in the partition.</p> <p></p> <p>Now there is a GroupBy / Shuffling of data. All the Sugar data comes to one partition.</p> <p></p> <p>By default there are 200 partitions, but 195 are empty.</p> <p>The resources are getting wasted because these 195 partitions also need to be shuffled.</p> <p>The 5 partitions become 5 tasks but Partition 1 takes lot of time to run.</p> <p></p> <p>Now AQE coalesces the partitions.</p> <p></p> <p>Two tasks are now reduced and also 2 cores become free.</p> <p>But even after coalescing we may end up with data skew.</p> <p></p> <p></p> <p>Once we coalesce we end up with 2 partitions and 1/2 completes fast, the one with sugar takes time.</p>"},{"location":"spark/38-Spark_Adaptive_Query_Execution/#data-splitting","title":"Data Splitting","text":"<p>If median is 5MB and one partition is &gt; 25MB then the data splits.</p>"},{"location":"spark/38-Spark_Adaptive_Query_Execution/#dynamically-switching-join-strategy","title":"Dynamically Switching Join Strategy","text":"<p>By default spark does sort merge join.</p> <p>Now if we compress table2 to become 10mb, even though sort merge join DAG is built, if AQE is enabled, we can check runtime statistics.</p> <p></p> <p>Since data is only 10MB we can broadcast the data but shuffling still happens only sorting and merging is avoided.</p>"},{"location":"spark/38-Spark_Adaptive_Query_Execution/#dynamically-optimizing-skew-join","title":"Dynamically Optimizing Skew Join","text":"<p>We are considering two tables where key = Sugar and just 128MB of data.</p> <p>Let's show other partitions also</p> <p></p> <p>Now when we do the Sort Merge Join and get all keys together the Sugar partition size increases.</p> <p></p> <p>All tasks except the one with Sugar completes fast.</p> <p></p> <p>This leads to OOM error.</p>"},{"location":"spark/38-Spark_Adaptive_Query_Execution/#solutions","title":"Solutions","text":"<ul> <li>Salting</li> <li>AQE</li> </ul> <p>AQE has ShuffleReader, it has statistics on the memory and size of each partition. This parttion gets automatically split in both tables.</p> <p></p>"},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/","title":"Spark Dynamic Resource Allocation","text":""},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/#lecture-31-dynamic-resource-allocation","title":"Lecture 31 : Dynamic Resource Allocation","text":""},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/#cluster-configuration","title":"Cluster Configuration","text":"<p>There is no problem with above configuration.</p> <p>But let's say another user comes and asks for more resources...</p> <p></p> <p>The red person can't get assigned any more memory since cluster is already full.</p> <p>The resource manager works on FIFO process.</p> <p>Now this small process that needs only 25GB may have to wait for hours.</p> <p></p> <p>In dynamic memory allocation, the data that is not used is released.</p> <p>Resource Manager has no role, spark internal algo does this.</p> <p></p> <p>Let's say we have free 750GB and driver demands 500GB from resource manager but there might be other processes waiting for the memory so it may not get it.</p> <p>We can use min executors and max executors to get around this. We set min executors in such a way that process does not fail.</p> <p>Now let's say there is a process which has completed execution and so Dynamic Resource Allocator frees the data. But we want it for further calculations. Do we calculate it again? No.</p> <p>We can use External Shuffle Service. This works independently on every worker node and data in this doesnt get deleted.</p> <p></p> <p>If executors idle from 60s then we release the data.</p>"},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/#how-does-executor-ask-for-resources","title":"How does executor ask for resources?","text":"<p>If the executor does not get its required memory within 1 sec then it starts asking in two fold manner.</p> <p></p> <p>First it asks for 1GB then 2GB then 4GB and so on...</p> <p><code>spark.scheduler.backlogTimeout</code> = 2s the executor waits till 2s before asking for memory.</p>"},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/#parallel-execution-and-multi-threading","title":"Parallel Execution and Multi Threading","text":""},{"location":"spark/39-Spark_Dynamic_Resource_Allocation/#when-to-avoid-dynamic-resource-allocation","title":"When to avoid dynamic resource allocation?","text":"<p>For critical jobs that needs to be run within certain SLA avoid it.</p>"},{"location":"spark/40-Spark_Dynamic_Partition_Pruning/","title":"Spark Dynamic Partition Pruning","text":""},{"location":"spark/40-Spark_Dynamic_Partition_Pruning/#lecture-32-dynamic-partition-pruning","title":"Lecture 32 : Dynamic Partition Pruning","text":"<p>In below code we have a filter applied to select only 19th April 2023 data,</p> <p></p> <p>Below we can see that only one file that is for 19th April 2023 is read, not all of them.</p> <p></p> <p></p>"},{"location":"spark/40-Spark_Dynamic_Partition_Pruning/#dpp-with-2-tables","title":"DPP with 2 tables","text":"<p>Partition pruning does not happen on first table but will happen on table 2. Dynamic Partition Pruning helps us to update filter on runtime.</p> <p>Two conditions:</p> <ul> <li>Data should be partitioned.</li> <li>2nd Table should be broadcasted.</li> </ul> <p></p> <p></p> <p>Without Dynamic Partition Pruning</p> <p>Total 123 files read from first table not one like previous case.</p> <p></p> <p>With Dynamic Partition Pruning</p> <p></p> <p></p> <p>The smaller dimdate table is broadcasted and hash join performed. Only 3 files are read this time.</p> <p></p> <p>At runtime a subquery is run...</p> <p></p> <p></p> <p>Now because of the runtime filter only 4 partitions are read/scanned.</p>"},{"location":"streaming/","title":"Streaming","text":"<p>This is the overview page for Streaming.</p>"},{"location":"streaming/architecture/","title":"Architecture","text":"<p>This is the overview page for Architecture.</p>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/","title":"\ud83d\udd39 What is Event Streaming?","text":"<p>Event streaming is a data processing paradigm where data is captured and processed in real time as a continuous flow of events.</p> <ul> <li>Event = A record of something that happened (e.g., a user clicks a button, a trade is executed, a payment is posted).</li> <li>Event streaming = Collecting, storing, processing, and delivering these events continuously instead of waiting for batch jobs.</li> </ul> <p>Think of it as a data pipeline that never sleeps \u2014 events flow from producers (apps, IoT devices, databases) to consumers (analytics dashboards, ML models, storage systems) instantly.</p>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#key-characteristics","title":"\ud83d\udd39 Key Characteristics","text":"<ol> <li>Continuous \u2192 Unlike batch, events are processed as they arrive.</li> <li>Real-time or Near Real-time \u2192 Low latency, milliseconds to seconds.</li> <li>Scalable \u2192 Can handle millions of events per second (e.g., Kafka, Redpanda, Flink).</li> <li>Replayable \u2192 Many platforms store event streams so consumers can \u201crewind\u201d and reprocess.</li> </ol>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#event-streaming-architecture","title":"\ud83d\udd39 Event Streaming Architecture","text":"<p>Producers \u2192 Event Broker \u2192 Consumers</p> <ul> <li>Producers: Generate events (apps, services, IoT, databases).</li> <li>Event Broker: Middleware (Kafka, Redpanda, Pulsar) that stores and routes events.</li> <li>Consumers: Applications that subscribe, transform, and act on events (analytics, fraud detection, alerting).</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#common-use-cases","title":"\ud83d\udd39 Common Use Cases","text":""},{"location":"streaming/architecture/01-Use_Cases_Streaming/#1-financial-services-payments","title":"1. Financial Services &amp; Payments","text":"<ul> <li>Real-time fraud detection: Stream every credit card swipe \u2192 check anomalies \u2192 block fraudulent transactions instantly.</li> <li>Market data processing: Process stock ticks, crypto trades in milliseconds for trading systems.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#2-e-commerce-retail","title":"2. E-commerce &amp; Retail","text":"<ul> <li>Real-time personalization: Recommend products as the customer browses.</li> <li>Inventory management: Update stock counts as orders come in.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#3-telecom-iot","title":"3. Telecom &amp; IoT","text":"<ul> <li>Device telemetry: Stream metrics from millions of IoT devices for monitoring.</li> <li>Predictive maintenance: Detect patterns in sensor data to prevent failures.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#4-log-monitoring-systems","title":"4. Log &amp; Monitoring Systems","text":"<ul> <li>Centralized logging: Apps push logs into Kafka \u2192 consumers analyze them.</li> <li>Alerting: Trigger alerts when error rates spike.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#5-data-engineering-pipelines","title":"5. Data Engineering Pipelines","text":"<ul> <li>Ingest data from databases via CDC (Change Data Capture) into a data lake/warehouse in near real-time.</li> <li>Stream ETL: Clean/transform data continuously instead of nightly batch.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#6-healthcare","title":"6. Healthcare","text":"<ul> <li>Patient monitoring: Stream vitals from hospital devices.</li> <li>Real-time analytics on EHR updates.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#7-transportation-mobility","title":"7. Transportation &amp; Mobility","text":"<ul> <li>Ride-sharing apps: Stream driver &amp; rider events \u2192 real-time matching &amp; pricing.</li> <li>Fleet tracking: Monitor vehicles live.</li> </ul>"},{"location":"streaming/architecture/01-Use_Cases_Streaming/#why-event-streaming-is-important","title":"\ud83d\udd39 Why Event Streaming is Important","text":"<ul> <li>Speed \u2192 Businesses can react instantly instead of hours later.</li> <li>Scalability \u2192 Handles massive data volumes in motion.</li> <li>Flexibility \u2192 Same event stream can feed many consumers (ML, dashboards, alerts).</li> <li>Decoupling \u2192 Producers and consumers don\u2019t need to know about each other; the broker handles delivery.</li> </ul> <p>\u2705 In short: Event streaming turns raw, real-time events into actionable insights, instantly. It\u2019s the backbone of modern systems like fraud detection, recommendation engines, trading systems, and IoT monitoring.</p> <p>Would you like me to make a visual diagram of how an event streaming pipeline works (Producer \u2192 Broker \u2192 Consumer with real-time examples like payments &amp; fraud detection)?</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/","title":"Kafka vs Redpanda Architecture","text":""},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#what-is-apache-kafka","title":"\ud83d\udd39 What is Apache Kafka?","text":"<ul> <li>Kafka is the most popular distributed event streaming platform (open source, from LinkedIn originally, now under Apache).</li> <li>It stores and streams records (events) in topics.</li> <li>It requires a cluster of brokers, ZooKeeper (legacy) or KRaft (newer) for metadata management, and usually has separate dependencies like JVM, OS tuning.</li> <li>Kafka is known for high throughput, fault tolerance, and ecosystem support (Connectors, Streams, ksqlDB).</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#what-is-redpanda","title":"\ud83d\udd39 What is Redpanda?","text":"<ul> <li>Redpanda is a Kafka API-compatible event streaming platform (drop-in replacement for Kafka).</li> <li> <p>You can use the same Kafka clients, but under the hood it has a different architecture:</p> </li> <li> <p>Written in C++ (Kafka is Java/Scala).</p> </li> <li>No ZooKeeper (built-in Raft consensus).</li> <li>Optimized for modern hardware (NVMe SSDs, fast CPUs).</li> <li>Lower operational overhead (single binary, no JVM tuning).</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#high-level-similarities","title":"\ud83d\udd39 High-level Similarities","text":"Feature Kafka Redpanda API Kafka API Kafka API (100% compatible) Concepts Topics, Partitions, Producers, Consumers Same Use cases Event streaming, ETL, real-time analytics Same Ecosystem Kafka Connect, ksqlDB, Kafka Streams Works with same tools <p>\ud83d\udc49 From an application developer\u2019s perspective, Redpanda is Kafka. But architecturally, they diverge a lot.</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#detailed-architectural-differences","title":"\ud83d\udd39 Detailed Architectural Differences","text":""},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#1-language-runtime","title":"1. Language &amp; Runtime","text":"<ul> <li>Kafka: Written in Java/Scala, runs on the JVM. Needs tuning (GC, heap sizes, etc.).</li> <li>Redpanda: Written in C++, runs natively. No JVM/GC overhead. Lower latency, more predictable performance.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#2-metadata-management","title":"2. Metadata Management","text":"<ul> <li>Kafka (Legacy): Uses ZooKeeper for metadata (cluster state, topics, partitions, configs).</li> <li>Kafka (New): Moving to KRaft mode (Kafka Raft), but still evolving.</li> <li>Redpanda: Always used Raft consensus internally (no ZooKeeper ever). Metadata is embedded \u2192 simpler operations.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#3-storage-engine","title":"3. Storage Engine","text":"<ul> <li>Kafka: Uses log segments stored on disk. Relies on Linux page cache for performance. Requires tuning of log cleaner, retention policies.</li> <li>Redpanda: Custom storage engine built with Seastar framework. Direct I/O to NVMe SSDs, bypasses page cache. Optimized for zero-copy reads/writes.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#4-threading-model","title":"4. Threading Model","text":"<ul> <li>Kafka: Traditional thread-per-connection model. Needs locks \u2192 more context switching, harder scaling under high concurrency.</li> <li>Redpanda: Uses Seastar\u2019s shard-per-core model (each CPU core runs independently, event-driven, no locks). Extremely efficient on modern multicore CPUs.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#5-deployment-operations","title":"5. Deployment &amp; Operations","text":"<ul> <li> <p>Kafka:</p> </li> <li> <p>Needs multiple services (brokers + ZooKeeper).</p> </li> <li>Requires JVM tuning, OS tuning, storage tuning.</li> <li>Typically runs with Confluent or other management platforms.</li> <li> <p>Redpanda:</p> </li> <li> <p>Single binary, no external dependencies.</p> </li> <li>Lower ops overhead, easier for small teams.</li> <li>Cloud-native (Kubernetes-friendly).</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#6-performance-latency","title":"6. Performance &amp; Latency","text":"<ul> <li>Kafka: Good throughput, but higher latency (ms \u2192 tens of ms) under load due to JVM GC and thread model.</li> <li>Redpanda: Lower latency (sub-ms to few ms), especially with SSD/NVMe, because of C++ + Seastar.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#7-ecosystem-features","title":"7. Ecosystem Features","text":"<ul> <li>Kafka: Has mature ecosystem \u2192 Kafka Connect, Kafka Streams, ksqlDB.</li> <li>Redpanda: Compatible with most Kafka tools, plus extras like Redpanda Console (UI for debugging streams), WASM engine for in-line stream transforms.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#visual-difference","title":"\ud83d\udd39 Visual Difference","text":"<p>Kafka (Classic):</p> <pre><code>Producer \u2192 Broker (JVM, log segments) \u2192 ZooKeeper \u2192 Consumer\n</code></pre> <p>Redpanda:</p> <pre><code>Producer \u2192 Redpanda Broker (C++, Raft, Seastar, native storage) \u2192 Consumer\n</code></pre> <p>\ud83d\udc49 Kafka has more moving parts; Redpanda is simpler, leaner, faster.</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#when-to-choose-what","title":"\ud83d\udd39 When to Choose What?","text":"<p>\u2705 Kafka (Apache/Confluent)</p> <ul> <li>If you need the mature ecosystem (Kafka Streams, Connectors, ksqlDB).</li> <li>If your org already has Kafka expertise.</li> <li>Large-scale legacy deployments.</li> </ul> <p>\u2705 Redpanda</p> <ul> <li>If you want Kafka API without Kafka ops pain.</li> <li>If low latency and simplicity are critical (e.g., trading, fraud detection, IoT).</li> <li>If you\u2019re cloud-native, Kubernetes-heavy, or want to avoid JVM tuning.</li> </ul> <p>\ud83d\udc49 In short:</p> <ul> <li>Kafka = battle-tested, ecosystem-rich, but heavier to run.</li> <li>Redpanda = Kafka reimagined for modern hardware: faster, simpler, no ZooKeeper, no JVM.</li> </ul> <p>Great follow-up \ud83d\ude4c \u2014 Seastar is at the heart of why Redpanda performs so differently from Kafka. Let\u2019s break it down carefully.</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#what-is-seastar","title":"\ud83d\udd39 What is Seastar?","text":"<ul> <li>Seastar is a high-performance, open-source C++ framework for building asynchronous, event-driven applications.</li> <li>It was originally created by ScyllaDB (a high-performance NoSQL database that replaced Cassandra) and is now used by Redpanda for streaming.</li> <li> <p>Seastar\u2019s design principle:</p> </li> <li> <p>Exploit modern hardware (multi-core CPUs, NVMe SSDs, high-throughput networks).</p> </li> <li>Avoid traditional multithreading pitfalls (locks, context switching, kernel overhead).</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#the-thread-per-core-model","title":"\ud83d\udd39 The Thread-per-Core Model","text":"<p>In traditional multithreaded apps (like Kafka\u2019s JVM model):</p> <ul> <li>Many threads run across CPU cores.</li> <li>Threads often compete for resources (memory, I/O).</li> <li>Coordination requires locks and synchronization, which create contention.</li> <li>The OS scheduler context-switches threads \u2192 overhead increases as concurrency rises.</li> </ul> <p>\ud83d\udc49 This becomes a bottleneck for very high-throughput systems.</p> <p>Seastar\u2019s Approach (Thread-per-Core):</p> <ul> <li>Each CPU core runs its own independent shard of the application.</li> <li>A shard handles its own memory, I/O, and data \u2014 no shared state, no locks.</li> <li>If data needs to move between shards, Seastar uses explicit message passing (like an internal network).</li> <li>Each shard runs in a single-threaded event loop, processing tasks asynchronously (like Node.js, but per-core and highly optimized).</li> </ul> <p>So instead of threads competing, each core is fully utilized and works independently.</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#storage-with-seastar","title":"\ud83d\udd39 Storage with Seastar","text":"<p>Seastar integrates with modern storage (NVMe SSDs, high-bandwidth disks) using:</p> <ol> <li> <p>Direct I/O (bypassing OS page cache):</p> </li> <li> <p>Kafka relies on the Linux page cache to buffer disk I/O.</p> </li> <li>Redpanda (via Seastar) uses direct disk access with async I/O, avoiding double-buffering.</li> <li>This reduces kernel overhead and improves predictability of latency.</li> </ol> <p>Got it \u2014 let\u2019s break this down step by step.</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#1-how-kafka-handles-disk-io","title":"1. How Kafka handles disk I/O","text":"<ul> <li>Kafka writes its logs to disk, but it doesn\u2019t directly manage the disk I/O.</li> <li> <p>Instead, it uses the Linux OS page cache:</p> </li> <li> <p>When Kafka writes data, the OS writes it into memory first (the page cache).</p> </li> <li>Later, the OS flushes it to disk.</li> <li>Similarly, when Kafka reads, the OS might serve it from the cache instead of the physical disk.</li> <li>\u2705 Advantage: simple, no need for Kafka to manage buffers.</li> <li> <p>\u274c Downside:</p> </li> <li> <p>Data gets double buffered (once in Kafka\u2019s memory, again in the OS cache).</p> </li> <li>Extra kernel/user-space context switches.</li> <li>Latency can be unpredictable because flushing is controlled by the OS, not Kafka.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#2-how-redpanda-handles-disk-io","title":"2. How Redpanda handles disk I/O","text":"<ul> <li>Redpanda uses Seastar, a high-performance C++ framework designed for low-latency workloads.</li> <li> <p>Instead of relying on the OS page cache, it uses Direct I/O:</p> </li> <li> <p>Data is written directly to disk (or read directly), bypassing the page cache.</p> </li> <li>Seastar manages its own memory for buffering and caching.</li> <li> <p>\u2705 Advantages:</p> </li> <li> <p>No double-buffering (only one copy in Redpanda\u2019s memory).</p> </li> <li>Lower kernel overhead because fewer syscalls and context switches.</li> <li>Predictable latency \u2014 Redpanda decides when data is flushed, not the OS.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#3-the-key-tradeoff","title":"3. The key tradeoff","text":"<ul> <li>Kafka: simpler, leverages the OS, but introduces variability.</li> <li>Redpanda: more complex (self-manages I/O), but yields higher throughput and consistent low latency because it avoids surprises from the kernel\u2019s page cache flush policy.</li> </ul> <p>\ud83d\udd11 In short:</p> <ul> <li>Kafka \u2192 \u201cI trust Linux to handle disk I/O efficiently.\u201d (But OS adds overhead + jitter).</li> <li>Redpanda \u2192 \u201cI\u2019ll handle my own I/O, bypass the OS page cache.\u201d (More control, more predictability).</li> </ul> <p>Do you want me to also show this with a diagram of the data path (Kafka vs Redpanda I/O flow) so it\u2019s crystal clear?</p> <p></p> <ol> <li> <p>Zero-Copy Data Path:</p> </li> <li> <p>Events are read/written directly from disk/network buffers without unnecessary copies in user space.</p> </li> <li> <p>Example: A message can move from disk \u2192 network without extra CPU copy overhead.</p> </li> <li> <p>Shard-local Storage:</p> </li> <li> <p>Each shard (CPU core) manages its own chunk of logs.</p> </li> <li>Parallelism is achieved by letting each core handle its own partitions.</li> <li>This is why Redpanda can fully saturate NVMe SSD throughput.</li> </ol>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#why-its-powerful","title":"\ud83d\udd39 Why It\u2019s Powerful","text":"<ol> <li>No Locks \u2192 Removes contention bottlenecks.</li> <li>No Context Switching \u2192 No wasted cycles by the OS.</li> <li>NUMA Awareness \u2192 Each shard/core uses memory local to it (faster access).</li> <li>Full Hardware Utilization \u2192 Each core is \u201cits own Kafka broker,\u201d so scaling cores = scaling throughput.</li> <li>Predictable Latency \u2192 No GC pauses (like Kafka\u2019s JVM) and no kernel page cache unpredictability.</li> </ol>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#analogy","title":"\ud83d\udd39 Analogy","text":"<ul> <li>Kafka (JVM + thread pools): Imagine a busy restaurant where all waiters share one notebook for orders. They constantly fight for access, scribble, and waste time.</li> <li>Redpanda (Seastar): Each waiter (CPU core) has their own notebook, serves their own tables (partitions), and only passes notes to others when absolutely necessary. Way faster and smoother.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#example-in-redpanda","title":"\ud83d\udd39 Example in Redpanda","text":"<p>When Redpanda ingests events:</p> <ol> <li>Producer sends data \u2192 assigned to a partition.</li> <li>That partition is \u201cowned\u201d by one shard (CPU core).</li> <li>The shard writes it directly to its NVMe segment via async I/O.</li> <li>Replication across nodes is handled via Raft consensus, also shard-local.</li> <li>Consumer reads \u2192 served by the same shard \u2192 zero-copy stream out.</li> </ol> <p>Let\u2019s break that line down carefully:</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#1-what-a-shard-is","title":"1. What a shard is","text":"<ul> <li>In Redpanda (via Seastar), the application is built around shards, which are like lightweight execution units.</li> <li>Each shard is pinned to a CPU core, so it handles all work (networking, scheduling, disk I/O) without locking or context switches.</li> <li>This is different from Kafka, where threads can float across cores and share resources.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#2-nvme-segment","title":"2. NVMe segment","text":"<ul> <li>Data in Redpanda is stored in segments (just like Kafka log segments).</li> <li>But Redpanda is designed to map each shard to a slice of the log \u2192 so a shard writes only its own portion of the log to disk.</li> <li>These segments are placed on NVMe SSDs (very fast storage designed for parallelism).</li> <li>This allows the shard to take advantage of NVMe\u2019s native capability: lots of independent parallel I/O queues.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#3-async-io-direct-io","title":"3. Async I/O (Direct I/O)","text":"<ul> <li> <p>Instead of using the OS page cache and blocking syscalls (<code>write()</code>, <code>fsync()</code>), Redpanda does async direct I/O:</p> </li> <li> <p>The shard issues a non-blocking write request straight to NVMe.</p> </li> <li>The kernel does not buffer the data in the page cache.</li> <li>Completion is signaled via an event (polling or completion queue).</li> </ul> <p>\u2705 Benefits:</p> <ul> <li>No double buffering.</li> <li>No thread blocking \u2192 the shard keeps processing other requests.</li> <li>Full use of NVMe parallelism (each shard can queue operations independently).</li> <li>Latency predictability: flush happens when Redpanda decides, not when the OS decides.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#putting-it-together","title":"Putting it together","text":"<p>\ud83d\udc49 \u201cThe shard writes it directly to its NVMe segment via async I/O\u201d means: A CPU-core\u2013pinned shard in Redpanda takes incoming data, and instead of passing it through the OS page cache, it sends it straight to its assigned log segment on an NVMe SSD using non-blocking (asynchronous) disk operations. This ensures low latency, avoids kernel overhead, and fully exploits NVMe hardware parallelism.</p> <p>\ud83d\udc49 The result: low-latency streaming with minimal CPU overhead.</p> <p>\u2705 In short:</p> <ul> <li>Seastar gives Redpanda its lock-free, thread-per-core architecture.</li> <li>This enables direct, async NVMe I/O and full CPU utilization.</li> <li>Compared to Kafka\u2019s JVM/thread model, Redpanda achieves lower latency, higher throughput, and simpler ops.</li> </ul> <p></p> <p></p> <p></p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#kafka-fysnc-model","title":"Kafka <code>fysnc</code> model","text":"<p>Perfect \u2014 let\u2019s contrast Kafka\u2019s fsync model vs Redpanda\u2019s async direct I/O model:</p>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#1-kafka-fsync-page-cache","title":"\ud83d\udd39 1. Kafka (fsync + page cache)","text":"<ul> <li> <p>Write path:</p> </li> <li> <p>Producer sends message \u2192 Kafka broker writes it into a user-space buffer.</p> </li> <li>Kafka calls <code>write()</code> \u2192 data goes to the Linux page cache, not directly to disk.</li> <li> <p>Data sits in the cache until:</p> <ul> <li>The kernel decides to flush it, or</li> <li>Kafka explicitly calls <code>fsync()</code> to force persistence.</li> </ul> </li> <li> <p>Characteristics:</p> </li> <li> <p>Simpler, because Linux handles buffering, flushing, batching.</p> </li> <li>Double buffering: message exists in Kafka\u2019s memory buffer and page cache.</li> <li>Flush timing is partially up to the OS \u2192 latency jitter.</li> <li>Works fine for HDDs/SSDs, but can\u2019t fully exploit NVMe\u2019s parallel queues.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#2-redpanda-async-direct-io-via-seastar","title":"\ud83d\udd39 2. Redpanda (async direct I/O via Seastar)","text":"<ul> <li> <p>Write path:</p> </li> <li> <p>Producer sends message \u2192 shard (CPU-core\u2013pinned task) buffers it in its own memory.</p> </li> <li>Shard issues async direct I/O request straight to its NVMe segment.</li> <li>NVMe handles writes via parallel I/O queues, completion events notify the shard.</li> <li> <p>Characteristics:</p> </li> <li> <p>No OS page cache involvement \u2192 no double buffering.</p> </li> <li>No <code>fsync()</code> blocking calls \u2192 everything is non-blocking, shard never stalls.</li> <li>Each shard gets its own log segment, mapped neatly onto NVMe queues \u2192 parallelism scales linearly with cores.</li> <li>Much more predictable latency, since flush happens when Redpanda wants it, not when the OS decides.</li> </ul>"},{"location":"streaming/architecture/02-Redpanda_vs_Kafka_Arch_Differences/#3-comparison-at-a-glance","title":"\ud83d\udd39 3. Comparison at a glance","text":"Aspect Kafka (fsync + page cache) Redpanda (async direct I/O) I/O Path User buffer \u2192 Page cache \u2192 Disk Shard buffer \u2192 Direct NVMe write Buffering Double (app + page cache) Single (app buffer only) Flush Control Kernel + fsync Application (shard) Latency Variable (depends on OS flush) Predictable (app-controlled) Parallelism Limited (threads + OS queues) High (per-core shards + NVMe queues) Overhead Syscalls, kernel memory mgmt User-space async I/O, no blocking <p>\ud83d\udd11 In short:</p> <ul> <li>Kafka \u2192 \u201cLinux, you handle the disk flushing.\u201d (Simpler, but less control, more jitter).</li> <li>Redpanda \u2192 \u201cI\u2019ll handle the disk myself with async I/O.\u201d (More complex, but lower latency, highly parallel, NVMe-optimized).</li> </ul> <p></p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/","title":"Redpanda Architecture Pt 1","text":"<p>Reference Video</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#topics","title":"Topics","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#origins-of-kafka","title":"Origins of Kafka","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#core-concepts-in-kafka","title":"Core Concepts in Kafka","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#what-is-redpanda","title":"What is Redpanda?","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#redpanda-bits-and-bytes","title":"Redpanda Bits and Bytes","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#redpanda-core","title":"Redpanda Core","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#seastar-concepts","title":"Seastar Concepts","text":"<p>Great question \u2014 let\u2019s unpack \u201cthreads are pinned to a core\u201d:</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#1-normal-threading-default-in-most-apps-like-kafka","title":"1. Normal threading (default in most apps, like Kafka)","text":"<ul> <li>An application creates multiple threads.</li> <li>The OS scheduler decides which CPU core runs each thread.</li> <li>Threads can move between cores depending on load, availability, or scheduling.</li> <li> <p>This gives flexibility, but:</p> </li> <li> <p>Causes context switches (thread gets paused, moved to another core).</p> </li> <li>Causes cache misses (data in one core\u2019s L1/L2 cache isn\u2019t available on the new core).</li> <li>Adds latency jitter.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#2-pinning-cpu-affinity","title":"2. Pinning (CPU affinity)","text":"<ul> <li>\u201cPinning\u201d means a thread is locked (affined) to a specific CPU core.</li> <li>The OS will always schedule that thread only on that one core.</li> <li> <p>Benefits:</p> </li> <li> <p>No migration \u2192 thread always runs on the same CPU.</p> </li> <li>Cache locality \u2192 data stays in that CPU\u2019s cache, improving performance.</li> <li>Predictable latency \u2192 no interruptions from the scheduler moving threads around.</li> <li> <p>Downside:</p> </li> <li> <p>Less flexible: if one core is overloaded, OS can\u2019t move its threads elsewhere.</p> </li> <li>Requires careful design to balance load across cores.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#3-in-redpanda-seastars-case","title":"3. In Redpanda / Seastar\u2019s case","text":"<ul> <li> <p>Instead of traditional multithreading, Seastar uses a \u201cshard-per-core\u201d model:</p> </li> <li> <p>Each core gets one dedicated shard (like an event loop).</p> </li> <li>That shard is pinned to the core.</li> <li>It runs everything: networking, disk I/O, scheduling for that shard.</li> <li>This eliminates almost all locking and cross-core coordination overhead.</li> <li>Each shard processes requests independently, and inter-core communication happens explicitly via message passing (not shared-memory locks).</li> </ul> <p>\u2705 So, \u201cthreads are pinned to a core\u201d means: Each execution unit (thread/shard) runs permanently on the same CPU core, giving predictable performance and cache efficiency, instead of being moved around by the OS scheduler.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#1-process-vs-thread","title":"\ud83d\udd39 1. Process vs Thread","text":"<ul> <li> <p>A process is a program in execution:</p> </li> <li> <p>Has its own memory space (heap, stack, code, etc.).</p> </li> <li>Example: <code>java -jar kafka.jar</code> starts a Kafka broker process.</li> <li> <p>A thread is a lightweight unit of execution inside a process:</p> </li> <li> <p>Shares the same memory space as other threads in that process.</p> </li> <li>Has its own stack and program counter (so it can run independently).</li> <li>Example: Kafka spawns threads for handling networking, log flushes, replication, etc.</li> </ul> <p>\ud83d\udc49 Think of a process as a house, and threads as people inside the house who share the same kitchen (memory), but can each do different tasks.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#2-what-does-a-thread-actually-do","title":"\ud83d\udd39 2. What does a thread actually do?","text":"<ul> <li>A thread executes a sequence of instructions (functions, loops, syscalls).</li> <li>The OS schedules the thread on a CPU core.</li> <li>Multiple threads in the same process can run concurrently (on different cores).</li> </ul> <p>Example:</p> <ul> <li>Thread A reads data from the network socket.</li> <li>Thread B compresses and batches the data.</li> <li>Thread C writes data to disk.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#3-does-the-application-send-data-via-a-thread","title":"\ud83d\udd39 3. Does the application \u201csend data via a thread\u201d?","text":"<p>Not exactly.</p> <ul> <li>The application creates threads to handle tasks (e.g., read, process, write).</li> <li>Each thread operates on shared data structures in the process\u2019s memory.</li> <li> <p>Threads can pass data between each other via:</p> </li> <li> <p>Shared memory (since they live in the same process).</p> </li> <li>Queues, buffers, or synchronization primitives (locks, semaphores).</li> </ul> <p>So it\u2019s not like a thread is a \u201cpipe\u201d that data flows through. \ud83d\udc49 Instead: A thread is a worker that executes instructions on data in memory. The application controls what the thread does.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#4-example-kafka","title":"\ud83d\udd39 4. Example: Kafka","text":"<ul> <li>Kafka broker process starts \u2192 JVM process.</li> <li> <p>JVM creates threads:</p> </li> <li> <p>Network thread: handles socket I/O from producers/consumers.</p> </li> <li>I/O thread: appends messages to the log.</li> <li>Replica fetcher threads: replicate data across brokers.</li> <li>Threads share the same heap memory, but each has its own execution flow.</li> </ul> <p>\u2705 In short: A thread is not a data pipe \u2014 it\u2019s a unit of execution that runs code inside a process. The application assigns tasks to threads, and those threads can work with shared memory to process or pass data around.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#thread-per-core-benefits","title":"Thread Per Core Benefits","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#shard-to-partition-mapping","title":"Shard to Partition Mapping","text":"<p>\ud83d\udd39 Partition-to-Shard Mapping in Redpanda (Seastar Model)</p> <ul> <li>In Redpanda, a partition is assigned to exactly one shard (CPU core) within a broker.</li> <li>That shard is the exclusive owner of the partition: it handles all reads, writes, and replication logic for it.</li> <li>There is no sharing of partitions across shards \u2192 avoids locks, keeps the model deterministic.</li> </ul> <p>\ud83d\udc49 So yes: 1 partition \u2192 1 shard (on a given broker).</p> <p>\ud83d\udd39 How This Works in Practice</p> <ol> <li> <p>Broker Setup</p> </li> <li> <p>Each Redpanda broker runs with multiple shards (cores).</p> </li> <li> <p>Example: A broker with 8 CPU cores \u2192 8 shards.</p> </li> <li> <p>Partition Assignment</p> </li> <li> <p>When a partition is created, Redpanda assigns it to a shard.</p> </li> <li> <p>Partition \u2192 Shard mapping is stored in metadata (using Raft consensus).</p> </li> <li> <p>Shard Locality</p> </li> <li> <p>Once assigned, all producers/consumers that interact with that partition will hit that shard directly.</p> </li> <li> <p>This guarantees:</p> <ul> <li>No cross-shard locks</li> <li>Cache locality (NUMA-aware memory)</li> <li>Predictable performance</li> </ul> </li> <li> <p>Scaling Partitions</p> </li> <li> <p>More partitions = spread across more shards (and brokers).</p> </li> <li>If you have 100 partitions and 8 shards, partitions will be distributed \\~evenly across shards.</li> </ol> <p>\ud83d\udd39 Why This is Efficient</p> <ul> <li>Each shard runs its own event loop with Seastar.</li> <li> <p>Since a shard owns the partition exclusively:</p> </li> <li> <p>No lock contention.</p> </li> <li>No need for multiple threads touching the same partition.</li> <li>CPU cache locality is preserved.</li> </ul> <p>This is very different from Kafka\u2019s JVM/thread pool model, where multiple threads may process partitions and need locks/synchronization.</p> <p>Visual (Simplified)</p> <p>Kafka Broker (JVM, Thread Pools):</p> <pre><code>Thread Pool\n  \u251c\u2500\u2500 Partition 1 (handled by multiple threads w/ locks)\n  \u251c\u2500\u2500 Partition 2 (ditto)\n  \u2514\u2500\u2500 Partition 3 ...\n</code></pre> <p>Redpanda Broker (Seastar, Shards):</p> <pre><code>Shard 0 \u2192 Partition 1, Partition 5\nShard 1 \u2192 Partition 2, Partition 6\nShard 2 \u2192 Partition 3\nShard 3 \u2192 Partition 4\n</code></pre> <p>\ud83d\udc49 Each shard handles its own partitions \u2014 no overlap, no locks.</p> <ul> <li>A partition cannot be split across shards (it\u2019s always fully owned).</li> <li>But a shard can own multiple partitions if you have more partitions than shards.</li> <li>Redpanda\u2019s scheduler balances partitions across shards.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#shard-to-partition-mapping-in-redpanda-seastar-model","title":"\ud83d\udd39 Shard to Partition Mapping in Redpanda (Seastar Model)","text":"<ul> <li>In Redpanda, a partition is assigned to exactly one shard (CPU core) within a broker.</li> <li>That shard is the exclusive owner of the partition: it handles all reads, writes, and replication logic for it.</li> <li>There is no sharing of partitions across shards \u2192 avoids locks, keeps the model deterministic.</li> </ul> <p>\ud83d\udc49 So yes: 1 partition \u2192 1 shard (on a given broker).</p> <p>\ud83d\udd39 How This Works in Practice</p> <ol> <li> <p>Broker Setup</p> </li> <li> <p>Each Redpanda broker runs with multiple shards (cores).</p> </li> <li> <p>Example: A broker with 8 CPU cores \u2192 8 shards.</p> </li> <li> <p>Partition Assignment</p> </li> <li> <p>When a partition is created, Redpanda assigns it to a shard.</p> </li> <li> <p>Partition \u2192 Shard mapping is stored in metadata (using Raft consensus).</p> </li> <li> <p>Shard Locality</p> </li> <li> <p>Once assigned, all producers/consumers that interact with that partition will hit that shard directly.</p> </li> <li> <p>This guarantees:</p> <ul> <li>No cross-shard locks</li> <li>Cache locality (NUMA-aware memory)</li> <li>Predictable performance</li> </ul> </li> <li> <p>Scaling Partitions</p> </li> <li> <p>More partitions = spread across more shards (and brokers).</p> </li> <li>If you have 100 partitions and 8 shards, partitions will be distributed \\~evenly across shards.</li> </ol> <p>\ud83d\udd39 Why This is Efficient</p> <ul> <li>Each shard runs its own event loop with Seastar.</li> <li> <p>Since a shard owns the partition exclusively:</p> </li> <li> <p>No lock contention.</p> </li> <li>No need for multiple threads touching the same partition.</li> <li>CPU cache locality is preserved.</li> </ul> <p>This is very different from Kafka\u2019s JVM/thread pool model, where multiple threads may process partitions and need locks/synchronization.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#visual-simplified","title":"\ud83d\udd39 Visual (Simplified)","text":"<p>Kafka Broker (JVM, Thread Pools):</p> <pre><code>Thread Pool\n  \u251c\u2500\u2500 Partition 1 (handled by multiple threads w/ locks)\n  \u251c\u2500\u2500 Partition 2 (ditto)\n  \u2514\u2500\u2500 Partition 3 ...\n</code></pre> <p>Redpanda Broker (Seastar, Shards):</p> <pre><code>Shard 0 \u2192 Partition 1, Partition 5\nShard 1 \u2192 Partition 2, Partition 6\nShard 2 \u2192 Partition 3\nShard 3 \u2192 Partition 4\n</code></pre> <p>\ud83d\udc49 Each shard handles its own partitions \u2014 no overlap, no locks.</p> <p>\ud83d\udd39 Important Note</p> <ul> <li>A partition cannot be split across shards (it\u2019s always fully owned).</li> <li>But a shard can own multiple partitions if you have more partitions than shards.</li> <li>Redpanda\u2019s scheduler balances partitions across shards.</li> </ul> <p>\u2705 Answer: Yes, in Redpanda\u2019s Seastar model, a partition maps to exactly one shard (core). This lock-free ownership model is what gives Redpanda its high throughput and low latency.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-rebalancing-is-needed","title":"\ud83d\udd39 Why Rebalancing is Needed","text":"<ul> <li>In any event streaming cluster, partitions need to be spread evenly for performance.</li> <li> <p>Situations that trigger rebalancing:</p> </li> <li> <p>Adding/removing brokers (scale up/down).</p> </li> <li>Adding/removing CPU cores (changing shard count).</li> <li>Increasing partitions on a topic.</li> <li>Failure recovery (a broker goes down).</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#kafka-partition-rebalancing-traditional-way","title":"\ud83d\udd39 Kafka Partition Rebalancing (Traditional Way)","text":"<ul> <li>Kafka relies on a partition reassigner (via ZooKeeper or KRaft).</li> <li> <p>When brokers are added, Kafka shifts partitions across brokers, but:</p> </li> <li> <p>Within a broker, partitions are handled by threads in pools (not pinned to a core).</p> </li> <li>Partition-to-thread mapping is dynamic, with potential contention.</li> <li>Rebalancing is often manual + disruptive (CLI commands, partition reassignment tool).</li> <li>Data movement = expensive, because Kafka copies log segments across brokers during reassignment.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#redpanda-partition-rebalancing-seastar-model","title":"\ud83d\udd39 Redpanda Partition Rebalancing (Seastar Model)","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#1-partition-to-shard-pinning","title":"1. Partition-to-Shard Pinning","text":"<ul> <li>Each partition is always owned by exactly one shard.</li> <li>When partitions are assigned to a broker, Redpanda also ensures load balancing across shards within that broker.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#2-adding-a-new-broker","title":"2. Adding a New Broker","text":"<ul> <li>Redpanda automatically reassigns some partitions to the new broker.</li> <li>Metadata (via Raft) is updated to reflect ownership.</li> <li>The new broker takes over as partition leader or replica for some partitions.</li> <li>Producers/consumers redirect automatically (via client metadata refresh).</li> </ul> <p>\ud83d\udc49 This is smoother than Kafka\u2019s rebalance because Redpanda has no external ZooKeeper layer.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#3-adding-cpu-cores-more-shards","title":"3. Adding CPU Cores (More Shards)","text":"<ul> <li>Suppose a broker runs on 4 cores (shards) and you upgrade it to 8 cores.</li> <li>Redpanda can redistribute partitions across the new shards.</li> <li>Each partition is moved to a new shard if needed, but ownership is always exclusive.</li> <li>This way, hardware scaling (more cores) leads to more parallelism without rewriting application logic.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#4-partition-expansion","title":"4. Partition Expansion","text":"<ul> <li>If you increase partitions in a topic, Redpanda assigns new partitions to shards across brokers evenly.</li> <li>Existing partitions remain pinned \u2014 no surprise reassignments unless explicitly rebalanced.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#5-failure-recovery","title":"5. Failure Recovery","text":"<ul> <li>If a broker/shard fails, Redpanda promotes replicas (via Raft consensus) to leaders.</li> <li>The partition moves to another shard/broker that has a replica.</li> <li>Clients auto-discover the new leader.</li> </ul> <p>\ud83d\udd39 Why Redpanda\u2019s Model Helps</p> Aspect Kafka Redpanda (Seastar) Partition Ownership Dynamic threads Fixed shard-per-core Rebalancing Trigger Often manual Mostly automatic Intra-broker balance Threads may contend Explicit shard assignment Scaling Cores No concept Shards = cores, easy scaling Data Movement Heavy (log copy) Lighter (replicas managed via Raft) <p>\ud83d\udd39 Example</p> <p>Suppose:</p> <ul> <li>Cluster = 2 brokers, 4 cores each \u2192 8 shards total.</li> <li>Topic = 8 partitions.</li> </ul> <p>Initial mapping:</p> <pre><code>Broker1 Shard0 \u2192 Partition0\nBroker1 Shard1 \u2192 Partition1\nBroker1 Shard2 \u2192 Partition2\nBroker1 Shard3 \u2192 Partition3\nBroker2 Shard0 \u2192 Partition4\nBroker2 Shard1 \u2192 Partition5\nBroker2 Shard2 \u2192 Partition6\nBroker2 Shard3 \u2192 Partition7\n</code></pre> <p>Now you add a third broker (4 cores):</p> <ul> <li>Redpanda rebalances so that Broker3 takes ownership of some partitions (say 2 and 6).</li> <li>Partition ownership shifts smoothly, Raft ensures replica consistency.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#kafka-thread-pooling","title":"Kafka Thread Pooling","text":"<p>Perfect \u2014 let\u2019s explain thread pools in the context of Kafka only.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-kafka-uses-thread-pools","title":"\ud83d\udd39 Why Kafka uses thread pools","text":"<ul> <li> <p>Kafka brokers handle a huge number of concurrent tasks:</p> </li> <li> <p>Accepting requests from producers.</p> </li> <li>Serving fetch requests from consumers.</li> <li>Replicating partitions between brokers.</li> <li>Flushing data to disk.</li> <li>If Kafka created a new thread for every client request, it would waste CPU and memory.</li> <li> <p>Instead, Kafka uses thread pools:</p> </li> <li> <p>A fixed number of threads created at broker startup.</p> </li> <li>Incoming work is placed into queues.</li> <li>Threads pick tasks from these queues and execute them.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#examples-of-thread-pools-inside-kafka","title":"\ud83d\udd39 Examples of thread pools inside Kafka","text":"<ol> <li> <p>Network Thread Pool</p> </li> <li> <p>Each broker has a set of network threads.</p> </li> <li>They handle socket connections, parse requests, and enqueue them for processing.</li> <li>By default, the number of network threads = <code>num.network.threads</code> (configurable).</li> <li> <p>Example: If you set <code>num.network.threads=3</code>, Kafka creates 3 reusable threads to handle all incoming client connections.</p> </li> <li> <p>I/O / Request Handler Thread Pool</p> </li> <li> <p>Requests received by network threads are handed off to I/O threads.</p> </li> <li>These handle actions like reading/writing data to partitions, updating metadata, etc.</li> <li>Controlled by <code>num.io.threads</code>.</li> <li> <p>Example: If you have 8 I/O threads, they work in parallel to serve fetch/produce requests from the queue.</p> </li> <li> <p>Replica Fetcher Thread Pool</p> </li> <li> <p>Brokers need to replicate partitions across each other.</p> </li> <li>Kafka uses a pool of replica fetcher threads, one per leader-follower connection.</li> <li> <p>They continuously pull new data from leaders and apply it to local logs.</p> </li> <li> <p>Controller Thread (special case)</p> </li> <li> <p>The broker elected as controller uses a dedicated thread to manage partition leadership and cluster metadata.</p> </li> <li>This isn\u2019t a pool but a single thread with a special role.</li> </ol>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-this-matters","title":"\ud83d\udd39 Why this matters","text":"<ul> <li>Efficiency: Threads are expensive, so Kafka recycles them.</li> <li>Throughput: A pool keeps all CPU cores busy without creating too many threads.</li> <li>Predictability: Pools prevent the system from spawning unbounded threads when under load (avoiding crashes).</li> </ul> <p>\u2705 In short (Kafka terms): Kafka uses thread pools (network, I/O, replica fetchers) to process large numbers of concurrent requests with a fixed number of reusable threads. Instead of one thread per request, requests go into a queue, and a worker thread from the pool handles them.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#kafka-thread-pools-model","title":"\ud83d\udd39 Kafka: Thread Pools Model","text":"<ul> <li>Thread pools: Network threads, I/O threads, replica fetcher threads.</li> <li> <p>Work model:</p> </li> <li> <p>A request arrives \u2192 goes into a queue \u2192 some thread in the pool picks it up.</p> </li> <li>Threads may run on different CPU cores \u2192 need locks and synchronization to coordinate access to shared structures (like logs, partitions).</li> <li> <p>Implication:</p> </li> <li> <p>More flexible, but extra overhead from context switches, locks, and memory sharing.</p> </li> <li>OS scheduler decides which threads run on which cores (unless pinned manually).</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#redpanda-seastar-shard-per-core-model","title":"\ud83d\udd39 Redpanda (Seastar): Shard-per-Core Model","text":"<ul> <li>No thread pools at all.</li> <li> <p>Instead:</p> </li> <li> <p>Each CPU core runs a single Seastar \u201creactor\u201d thread.</p> </li> <li>That thread never migrates \u2192 it is pinned to the core permanently.</li> <li>Each reactor (aka shard) runs its own event loop and manages all tasks assigned to it: networking, disk I/O, scheduling.</li> <li> <p>Work model:</p> </li> <li> <p>Incoming requests are directed to the shard that owns the partition (no global queue).</p> </li> <li>That shard executes all operations locally, without locks.</li> <li>If work needs to cross cores, shards pass messages explicitly (message passing, not shared-memory locks).</li> <li> <p>Implication:</p> </li> <li> <p>Completely avoids contention \u2192 no thread pools, no locks, no queues between workers.</p> </li> <li>Each shard has exclusive ownership of its memory and partitions.</li> <li>Predictable latency (no surprises from OS scheduling).</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#side-by-side-comparison","title":"\ud83d\udd39 Side-by-side Comparison","text":"Feature Kafka (Thread Pools) Redpanda (Shard-per-Core) Concurrency model Multiple thread pools (network, I/O, replication). One shard (reactor thread) per CPU core. Scheduling OS scheduler decides which thread runs on which core. Threads are pinned \u2192 1 thread per core forever. Work distribution Tasks placed into queues, picked by worker threads. Requests routed directly to the shard that owns the partition. Synchronization Requires locks (shared memory between threads). No locks \u2192 shard owns its state, cross-core via message passing. Context switches Frequent, threads may migrate across cores. None (thread never migrates). Analogy Call center with a pool of operators picking calls from a queue. Each operator has their own dedicated customers, no queue, no sharing."},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-redpanda-dropped-thread-pools","title":"\ud83d\udd39 Why Redpanda dropped thread pools","text":"<ul> <li>Kafka\u2019s model = general-purpose, flexible, but pays costs of locks + context switching.</li> <li>Redpanda\u2019s Seastar model = deterministic, low-latency, NVMe-optimized.</li> <li>By dedicating one reactor thread per core, Redpanda avoids the OS scheduler entirely and fully controls concurrency.</li> </ul> <p>\u2705 In short:</p> <ul> <li>Kafka \u2192 thread pools with shared state, queues, and locks.</li> <li>Redpanda \u2192 no thread pools, just one pinned reactor thread per core, using message passing instead of locking.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#in-kafka","title":"\ud83d\udd39 In Kafka","text":"<ul> <li> <p>Separate thread pools handle different responsibilities:</p> </li> <li> <p>Network threads \u2192 accept producer/consumer socket requests.</p> </li> <li>I/O threads \u2192 read/write data to partitions.</li> <li>Replica fetcher threads \u2192 replication.</li> <li>These threads share data structures \u2192 need locks + queues.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#in-redpanda-shard-per-core-model","title":"\ud83d\udd39 In Redpanda (Shard-per-Core model)","text":"<ul> <li>Each shard = one reactor thread pinned to one CPU core.</li> <li>That shard owns a subset of partitions (log segments).</li> <li> <p>And yes, it handles everything for those partitions:</p> </li> <li> <p>Networking</p> </li> <li> <p>Each shard has its own TCP/HTTP server stack (Seastar provides this).</p> </li> <li>When a producer sends a message for a partition owned by shard 3, the network request is routed directly to shard 3.</li> <li> <p>That shard parses, validates, and queues the write internally.</p> </li> <li> <p>Log append (Producer writes)</p> </li> <li> <p>Shard 3 appends the data directly to its NVMe segment using async direct I/O.</p> </li> <li> <p>No locks, no handing off to another thread.</p> </li> <li> <p>Consumer fetches (Reads)</p> </li> <li> <p>If a consumer requests data for a partition on shard 3, that same shard serves the request directly from its log segment (or in-memory cache).</p> </li> <li> <p>Again: no global queue, no cross-thread locks.</p> </li> <li> <p>Replication (Followers \u2192 Leaders)</p> </li> <li> <p>If shard 3 owns a leader partition, it handles replication requests from follower brokers itself.</p> </li> <li>Fetcher threads in Kafka become shard-owned replication tasks in Redpanda.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-this-is-powerful","title":"\ud83d\udd39 Why this is powerful","text":"<ul> <li>Everything related to a partition lives in one shard.</li> <li>The shard handles networking, persistence, and serving clients without coordination overhead.</li> <li>If work must cross shards (e.g., partition A on core 2, partition B on core 5), Redpanda uses message passing, not shared locks.</li> </ul> <p>\u2705 So yes: A shard in Redpanda is responsible for the full lifecycle of the partitions it owns:</p> <ul> <li>Accepting producer writes.</li> <li>Appending to disk.</li> <li>Serving consumer fetches.</li> <li>Handling replication.</li> </ul> <p>Kafka splits these into different thread pools \u2192 Redpanda collapses them into a single shard reactor per core.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#isnt-kafka-faster-because-of-thread-pools","title":"Isnt Kafka Faster Because of Thread Pools?","text":""},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#kafkas-concurrency-model","title":"\ud83d\udd39 Kafka\u2019s Concurrency Model","text":"<ul> <li> <p>Multiple thread pools can appear to give concurrency:</p> </li> <li> <p>Network threads enqueue requests.</p> </li> <li>I/O threads pick them up and write/read logs.</li> <li>Replica fetcher threads replicate in parallel.</li> <li> <p>But the cost is:</p> </li> <li> <p>Locks everywhere (log segments, partition metadata, socket buffers).</p> </li> <li>Context switches when handing tasks across pools.</li> <li>Cache misses because data may bounce across cores.</li> <li>OS scheduler interference.</li> </ul> <p>So while Kafka can \u201cparallelize\u201d operations on the same partition via pools, the overhead (locks, scheduling, context switching) adds latency and jitter.</p>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#redpandas-concurrency-model","title":"\ud83d\udd39 Redpanda\u2019s Concurrency Model","text":"<ul> <li> <p>Each shard (core) is a single-threaded event loop:</p> </li> <li> <p>Owns a set of partitions exclusively.</p> </li> <li>No locks, no hand-offs, no blocking.</li> <li> <p>Concurrency comes from multiple shards in parallel:</p> </li> <li> <p>If you have 16 cores, you have 16 shards running completely independently.</p> </li> <li>Each shard is simultaneously handling networking + producers + consumers + replication for its partitions.</li> <li> <p>For a single partition:</p> </li> <li> <p>Only one shard touches it (so no thread-level concurrency on that data).</p> </li> <li>But this actually improves performance: no lock contention, no context switches.</li> </ul>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#why-redpanda-isnt-slower","title":"\ud83d\udd39 Why Redpanda isn\u2019t slower","text":"<ol> <li> <p>Lock-free execution</p> </li> <li> <p>Kafka: \u201cparallel threads\u201d but guarded by locks \u2192 effectively serial at the partition level.</p> </li> <li> <p>Redpanda: one shard, lock-free, guaranteed order \u2192 faster.</p> </li> <li> <p>Core-to-core scaling</p> </li> <li> <p>Kafka: extra overhead scaling across cores because threads migrate.</p> </li> <li> <p>Redpanda: scaling is natural \u2014 add more cores \u2192 more shards \u2192 more partitions handled in parallel.</p> </li> <li> <p>NVMe optimization</p> </li> <li> <p>Kafka\u2019s I/O goes through the OS page cache and threads.</p> </li> <li>Redpanda maps shards directly to NVMe queues \u2192 multiple cores can hit storage in true parallel, without lock contention.</li> </ol>"},{"location":"streaming/architecture/03-Redpanda_Architure_In_Depth_Pt1/#analogy","title":"\ud83d\udd39 Analogy","text":"<ul> <li>Kafka = a restaurant where multiple waiters share the same kitchen (need rules/locks to avoid collisions). Looks busy, but there\u2019s overhead coordinating.</li> <li>Redpanda = each waiter has their own kitchen + their own customers. No conflicts, no coordination. Less \u201cfake concurrency,\u201d more real parallelism.</li> </ul> <p>\u2705 Answer: Redpanda is not slower. Even though a shard processes a partition\u2019s work serially, that\u2019s exactly what Kafka does too (because partitions are single-threaded units of order). The big win is that Redpanda avoids lock contention, context switches, and cache misses, so it scales much better with more cores and NVMe drives.</p>"},{"location":"streaming/kafka/","title":"Kafka","text":"<p>This is the overview page for Kafka.</p>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/","title":"\ud83d\udcdd Kafka 4.x (KRaft Mode) \u2013 Single Broker Setup","text":""},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#1-install-java-required-for-kafka","title":"1. Install Java (required for Kafka)","text":"<p>Kafka 4.x requires Java 11+.</p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jdk -y\njava -version\n</code></pre> <p>\u2705 Verify you see <code>openjdk version \"11...\"</code>.</p>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#2-download-kafka","title":"2. Download Kafka","text":"<pre><code>cd /home/ubuntu/kafka-book\nwget https://dlcdn.apache.org/kafka/4.1.0/kafka_2.13-4.1.0.tgz\ntar -xvzf kafka_2.13-4.1.0.tgz\ncd kafka_2.13-4.1.0\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#3-configure-kafka-kraft-mode-no-zookeeper","title":"3. Configure Kafka (KRaft mode, no ZooKeeper)","text":"<p>Edit <code>config/server.properties</code>:</p> <pre><code># Node identity\nprocess.roles=broker,controller\nnode.id=1\n\n# Listeners (broker + controller)\nlisteners=PLAINTEXT://:9092,CONTROLLER://:9093\nlistener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\ncontroller.listener.names=CONTROLLER\n\n# Required for KRaft mode\ncontroller.quorum.voters=1@localhost:9093\n\n# Storage directory\nlog.dirs=/tmp/kraft-combined-logs\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#4-format-the-storage-directory","title":"4. Format the Storage Directory","text":"<pre><code># Remove any old logs (important when retrying!)\nrm -rf /tmp/kraft-combined-logs\n\n# Generate a cluster ID\nbin/kafka-storage.sh random-uuid\n# Example output: 8dR1yJ7sT-u64QYy9mNQwQ\n\n# Format logs with the generated ID\nbin/kafka-storage.sh format -t &lt;uuid-from-above&gt; -c config/server.properties\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#5-start-the-kafka-broker","title":"5. Start the Kafka Broker","text":"<pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre> <p>Kafka should now start without errors.</p>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#6-create-a-test-topic","title":"6. Create a Test Topic","text":"<pre><code>bin/kafka-topics.sh --create \\\n  --topic test-topic \\\n  --partitions 1 \\\n  --replication-factor 1 \\\n  --bootstrap-server localhost:9092\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#7-produce-messages","title":"7. Produce Messages","text":"<pre><code>bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n# Type messages, hit Enter after each\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#8-consume-messages","title":"8. Consume Messages","text":"<pre><code>bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092\n</code></pre>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#mistakes-made-and-fixes","title":"\u274c Mistakes Made (and Fixes)","text":""},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#1-looking-for-zookeeper-server-startsh","title":"1. Looking for <code>zookeeper-server-start.sh</code>","text":"<ul> <li>Kafka 4.x uses KRaft mode (no ZooKeeper).</li> <li>Solution: use <code>config/server.properties</code> with <code>controller.quorum.voters</code>.</li> </ul>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#2-classpath-is-empty","title":"2. \"Classpath is empty\"","text":"<ul> <li>This happens if you download the source code instead of the binary release.</li> <li>Fix: download <code>kafka_2.13-4.1.0.tgz</code> (binary).</li> </ul>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#3-java-not-found","title":"3. \"java: not found\"","text":"<ul> <li>Java wasn\u2019t installed.</li> <li>Fix: <code>sudo apt install openjdk-11-jdk -y</code>.</li> </ul>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#4-no-readable-metaproperties-files-found","title":"4. \"No readable meta.properties files found\"","text":"<ul> <li>Means storage wasn\u2019t formatted before starting broker.</li> <li>Fix: run <code>kafka-storage.sh format</code>.</li> </ul>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#5-configkraftserverproperties-not-found","title":"5. \"config/kraft/server.properties not found\"","text":"<ul> <li>Looked for the wrong file path (some docs show <code>config/kraft/\u2026</code>).</li> <li>Fix: edit <code>config/server.properties</code>.</li> </ul>"},{"location":"streaming/kafka/01-Kafka_KRaft_Setup/#6-controllerquorumvoters-not-set","title":"6. \"controller.quorum.voters not set\"","text":"<ul> <li>Mandatory with one broker config so that kafka knows that this sole broker is to be bootstrapped.</li> <li>Fix: add</li> </ul> <pre><code>controller.quorum.voters=1@localhost:9093\n</code></pre>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/","title":"Kafka Broker Properties","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#broker-properties-in-kafka","title":"Broker Properties in Kafka","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#brokerid","title":"broker.id","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#listeners","title":"listeners","text":"<p>Example Config</p> <pre><code># The address the socket server listens on. If not configured, the host name will be equal to the value of\n# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n#   FORMAT:\n#     listeners = listener_name://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\nlisteners=PLAINTEXT://localhost:9092\n\n# Name of listener used for communication between brokers.\ninter.broker.listener.name=PLAINTEXT\n\n# Listener name, hostname and port the broker will advertise to clients.\n# If not set, it uses the value for \"listeners\".\nadvertised.listeners=PLAINTEXT://localhost:9092\n\n# A comma-separated list of the names of the listeners used by the controller.\n# This is required if running in KRaft mode. On a node with `process.roles=broker`, only the first listed listener will be used by the broker.\ncontroller.listener.names=CONTROLLER\n\n# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\nlistener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n</code></pre> <p>You can think of this as explaining \u201chow Kafka knows which door to use when someone wants to talk to it.\u201d</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#1-kafka-is-like-a-house-with-doors","title":"\ud83c\udfe0 1. Kafka is like a house with doors","text":"<p>Kafka brokers are like houses, and to talk to a broker (send messages, get data, etc.), you need to know which door to knock on.</p> <p>Each door has:</p> <ul> <li>A name (like \u201cPLAINTEXT\u201d or \u201cSSL\u201d)</li> <li>A street address (hostname or IP)</li> <li>A door number (port)</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#2-listenersplaintextlocalhost9092","title":"\ud83d\udeaa 2. <code>listeners=PLAINTEXT://localhost:9092</code>","text":"<p>This says:</p> <p>\u201cOpen a door called PLAINTEXT on address localhost, port 9092.\u201d</p> <p>That means:</p> <ul> <li>Kafka will listen on port <code>9092</code></li> <li>It will accept plain (unencrypted) connections</li> <li>Clients and other brokers can connect through that door.</li> </ul> <p>So if a producer or consumer wants to connect, it says:</p> <pre><code>bootstrap.servers=localhost:9092\n</code></pre> <p>\u2192 They\u2019re knocking on that door!</p> <p>Think of this as the door Kafka listens at.</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#3-interbrokerlistenernameplaintext","title":"\ud83d\udde3\ufe0f 3. <code>inter.broker.listener.name=PLAINTEXT</code>","text":"<p>Kafka brokers in the same cluster talk to each other \u2014 they need their own \u201cprivate line.\u201d</p> <p>This setting says:</p> <p>\u201cWhen brokers talk to each other, use the PLAINTEXT door.\u201d</p> <p>So if you have multiple brokers:</p> <ul> <li>Broker 1, Broker 2, and Broker 3</li> <li>They\u2019ll all use the PLAINTEXT channel to sync data and share cluster info.</li> </ul> <p>This just tells Kafka:</p> <p>\u201cWhich of the doors we opened should the brokers use to chat among themselves?\u201d</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#4-advertisedlistenersplaintextlocalhost9092","title":"\ud83d\udce2 4. <code>advertised.listeners=PLAINTEXT://localhost:9092</code>","text":"<p>Imagine you live inside your house, and you tell your friends:</p> <p>\u201cHey, if you want to visit me, come to localhost:9092.\u201d</p> <p>That\u2019s what this does.</p> <p>Kafka uses advertised.listeners to tell clients and other brokers \u201cthis is the address you should use to reach me.\u201d</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#why-this-matters","title":"Why this matters:","text":"<p>If Kafka runs inside Docker, Kubernetes, or the cloud, the <code>listeners</code> address might be something internal (like <code>0.0.0.0</code>), but the <code>advertised.listeners</code> should be the public or reachable hostname (like <code>my-broker.company.com</code>).</p> <p>So:</p> <ul> <li><code>listeners</code> = the actual door inside Kafka.</li> <li><code>advertised.listeners</code> = the address label you give out to the world.</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#5-controllerlistenernamescontroller","title":"\ud83e\udde0 5. <code>controller.listener.names=CONTROLLER</code>","text":"<p>Kafka needs a \u201ccontroller\u201d \u2014 one special broker that coordinates the cluster (decides leaders, handles elections, etc.).</p> <p>This line says:</p> <p>\u201cThe controller will use a listener called CONTROLLER to do its work.\u201d</p> <p>In Kafka\u2019s new mode (called KRaft mode \u2014 Kafka without ZooKeeper), the controller uses its own special private door (<code>CONTROLLER</code>) for cluster management.</p> <p>You can ignore this if you\u2019re running a simple single-node Kafka \u2014 it\u2019s just for internal communication between controller nodes.</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#6-listenersecurityprotocolmap","title":"\ud83d\udd10 6. <code>listener.security.protocol.map=...</code>","text":"<p>Now this is like a dictionary that tells Kafka:</p> <p>\u201cWhat kind of security each door uses.\u201d</p> <p>Here\u2019s what it means:</p> <pre><code>CONTROLLER:PLAINTEXT\nPLAINTEXT:PLAINTEXT\nSSL:SSL\nSASL_PLAINTEXT:SASL_PLAINTEXT\nSASL_SSL:SASL_SSL\n</code></pre> <p>So:</p> <ul> <li>Door named PLAINTEXT \u2192 normal unencrypted connection</li> <li>Door named SSL \u2192 encrypted HTTPS-style connection</li> <li>Door named SASL_SSL \u2192 encrypted + username/password</li> <li>Door named CONTROLLER \u2192 internal plain connection for controller traffic</li> </ul> <p>Basically, this says:</p> <p>\u201cEach door name matches its security type.\u201d</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#7-putting-it-all-together","title":"\ud83c\udfaf 7. Putting it all together","text":"Config Think of it as What it does <code>listeners</code> \ud83c\udfe0 The doors Kafka opens Where Kafka waits for connections <code>inter.broker.listener.name</code> \ud83d\udcde The door brokers use to talk to each other Chooses which listener for broker-to-broker communication <code>advertised.listeners</code> \ud83d\udce2 The address Kafka tells others to use How clients and brokers find this broker <code>controller.listener.names</code> \ud83e\udde0 The private control door Used by the controller in KRaft mode <code>listener.security.protocol.map</code> \ud83d\uddfa\ufe0f The security rulebook Maps each door to its security protocol"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#8-simple-example-story","title":"\ud83e\udde9 8. Simple example story","text":"<p>Imagine:</p> <ul> <li>You (Kafka broker) live in a house.</li> <li> <p>You have a few doors:</p> </li> <li> <p>\u201cFront door\u201d (PLAINTEXT) \u2192 anyone can visit</p> </li> <li>\u201cBack door\u201d (CONTROLLER) \u2192 for your best friend (controller)</li> <li>\u201cSecret door\u201d (SSL) \u2192 only for trusted people with keys</li> </ul> <p>You tell your friends:</p> <p>\u201cUse the front door at localhost:9092 to visit me!\u201d</p> <p>That\u2019s:</p> <pre><code>advertised.listeners=PLAINTEXT://localhost:9092\n</code></pre> <p>And you decide that you and your best friend (another broker) will use the same door to talk:</p> <pre><code>inter.broker.listener.name=PLAINTEXT\n</code></pre>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#9-in-short-the-kid-version","title":"\u2705 9. In short \u2014 the \u201ckid\u201d version:","text":"<ul> <li>listeners \u2192 Kafka opens this door to listen.</li> <li>advertised.listeners \u2192 Kafka tells everyone, \u201cHey! Knock on this door.\u201d</li> <li>inter.broker.listener.name \u2192 Brokers talk to each other through this door.</li> <li>controller.listener.names \u2192 The controller uses this door to manage the cluster.</li> <li>listener.security.protocol.map \u2192 Explains which door uses which kind of lock (security).</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#zookeeperconnect","title":"zookeeper.connect","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#logdirs","title":"log.dirs","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#numrecoverythreadperdatadir","title":"num.recovery.thread.per.data.dir","text":"<p>Alright, let\u2019s simplify this \ud83d\udc47</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#whats-happening","title":"\ud83d\udcdd What\u2019s happening?","text":"<ul> <li>Kafka stores messages on disk in log segments (files).</li> <li>When a broker starts or shuts down, it needs to open, check, or close all these log files.</li> <li>To do this work, Kafka uses a pool of threads.</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#where-threads-are-used","title":"\ud83d\udd11 Where threads are used:","text":"<ol> <li>Normal startup \u2192 open each partition\u2019s log files.</li> <li>Startup after a crash \u2192 carefully check + truncate log files (takes longer).</li> <li>Shutdown \u2192 close log files properly.</li> </ol>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#default-setting","title":"\u2699\ufe0f Default setting","text":"<ul> <li>By default, Kafka uses 1 thread per log directory.</li> <li>Example: if you have 3 log directories \u2192 3 threads total.</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#why-increase-threads","title":"\ud83d\ude80 Why increase threads?","text":"<ul> <li>If you have many partitions and a broker crashed, recovery can take hours with just 1 thread per directory.</li> <li>Increasing threads allows parallel recovery \u2192 much faster startup.</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#important-note","title":"\ud83d\udccc Important note","text":"<p>The config is called:</p> <pre><code>num.recovery.threads.per.data.dir\n</code></pre> <ul> <li>If you set it to <code>8</code> and you have <code>3</code> log.dirs, total = <code>8 \u00d7 3 = 24 threads</code>.</li> <li>More threads \u2192 faster startup/recovery.</li> </ul> <p>\ud83d\udc49 Layman analogy: Imagine you have 10,000 books (partitions) to put back on shelves after a storm (broker crash).</p> <ul> <li>With 1 librarian per shelf (default), it takes hours.</li> <li>With 8 librarians per shelf (more threads), all books are sorted much faster.</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#why-truncate-log-segments","title":"Why Truncate Log Segments?","text":"<p>Great question \ud83d\udc4d Let\u2019s break it down simply.</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#why-truncation-is-needed-after-a-crash","title":"\ud83d\udcdd Why truncation is needed after a crash","text":"<ul> <li>Kafka writes data to disk in log segments.</li> <li>Each segment has an ordered sequence of messages.</li> <li>When a broker crashes (power cut, OOM, kill -9, etc.), some data may have been partially written (corrupted, incomplete).</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#what-happens-after-restart","title":"\ud83d\udd0e What happens after restart","text":"<ol> <li>Kafka reopens the log files.</li> <li>It checks the last segment for incomplete or corrupted records.</li> <li>If it finds bad records at the end of the file \u2192 it truncates (cuts off) the broken part so only valid data remains.</li> </ol>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#why-this-is-important","title":"\u2705 Why this is important","text":"<ul> <li>Ensures data consistency: no half-written messages are exposed to consumers.</li> <li>Keeps the log index aligned with the actual data.</li> <li>Avoids strange errors like \u201cmessage length mismatch\u201d or \u201cinvalid checksum.\u201d</li> </ul>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#example","title":"\ud83d\udccc Example","text":"<p>Imagine writing messages to a notebook:</p> <pre><code>Page 1: OK\nPage 2: OK\nPage 3: crash halfway through sentence...\n</code></pre> <p>When you reopen, Kafka erases the half-written sentence on Page 3, so the notebook only contains complete entries.</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#safety-net","title":"\ud83d\udd12 Safety net","text":"<ul> <li>Kafka only truncates data that wasn\u2019t fully acknowledged (not committed).</li> <li>So producers/consumers won\u2019t lose confirmed messages \u2014 only the garbage left behind by the crash.</li> </ul> <p>\ud83d\udc49 In short: Truncation after a crash = clean up the mess at the end of the log so the broker can continue safely.</p>"},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#autocreatetopicsenable","title":"auto.create.topics.enable","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#autoleaderrebalanceenable","title":"auto.leader.rebalance.enable","text":""},{"location":"streaming/kafka/02-Kafka_Broker_Properties/#deletetopicenable","title":"delete.topic.enable","text":""},{"location":"streaming/kafka/03-Topic_Default_Properties/","title":"Topic Default Properties","text":""},{"location":"streaming/kafka/03-Topic_Default_Properties/#default-topic-properties","title":"Default Topic Properties","text":"<ol> <li><code>num.partitions</code></li> </ol>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#how-to-choose-number-of-partitions","title":"How to choose number of partitions?","text":"<ul> <li>What is throughput to expect from the topic? 100KbS or 1Gbps?</li> <li>What is the maximum throughput you expect to achieve when consuming from a single partition?</li> </ul> <p>If consumer writes slowly to database and the DB only handles 50MbPs for each thread writitng to it, then we are limited to 50MbPS throughput.</p> <p>If you are sending messages to partitions based on keys, adding partitions later can be very challenging, so calculate throughput based on your expected future usage, not the   current usage.</p> <p></p> <ol> <li><code>default.replication.factor</code></li> </ol> <p></p> <p>We would have three replicas of every partition since if there is a network switch update, disk failure or some other problem, we can be assured that additional replica will be available.</p> <ol> <li><code>log.retention.ms</code></li> </ol> <p>The most common configuration for how long Kafka will retain messages is by time.</p> <ul> <li>By default using <code>log.retention.hours</code> and is set to 168 hours.</li> <li>There are two other, <code>log.retention.minutes</code> and <code>log.retention.seconds</code>. The recommended is to use ms as its the smallest unit, if we specify all three ms takes precedence.</li> <li> <p>Retention by time is performed by examining the last modified time on each log segment on the disk. Under normal circumstances its the time the log segment was closed and is last message in the file.</p> </li> <li> <p><code>log.retention.bytes</code></p> </li> <li> <p>Another way to expire messages and this is set at topic level. If we have a topic with 8 partitions and log.retention.bytes set to 1GB, then the amount of data retained in the topic would be 8GB at most.</p> </li> <li>If no of partitions increase, then retention will also increase if <code>log.retention.bytes</code> is used.</li> </ul> <p></p> <ol> <li> <p><code>log.segment.bytes</code></p> </li> <li> <p>The previous config log.retention.bytes acts on log segments and not individual messages.</p> </li> <li>As messages are produced to the Kafka broker they are appended to the current log segment and once its reached <code>log.segment.bytes</code> which defaults to 1GB, its closed and a new one is opened.</li> <li>Once a log segment is closed its considered for expiration, if we have lot of small segments, then files are closed and allocated more often and disk efficiency decreases.</li> </ol> <p>We need to size the log segments carefully, take example, if topic receives 100 MB data and log.segment.bytes is 1GB, it will take 10 days for the segment to close and none of the messages in the segment can be expired until the log segment is closed.</p> <p>If log.retention.ms is set to 1 week, there will actually be upto 17 days of messages retained until closed log segment expires.</p> <p>This is because we need to wait for 10 days of all messages, the log segment must be retained for 7 days before it expires as per time based policy. Until the last message in the segment expires the segment cant be deleted.</p> <p></p> <ol> <li><code>log.roll.ms</code></li> </ol> <p>Specifies amount of time after which log segment must be closed. <code>log.segment.bytes</code> and <code>log.roll.ms</code> are not mutually exclusive.</p> <p>Kafka will close log segment when time limit is reached or size is crossed first.</p> <p>There is no default for <code>log.roll.ms</code></p> <p>Disk Performance When Using Time Based Segments</p> <p></p> <ol> <li><code>min.insync.replicas</code></li> </ol> <p></p> <ol> <li> <p><code>messages.max.bytes</code></p> </li> <li> <p>Default size which Kafka broker produces is 1mb.</p> </li> <li>A producer that produces more than this will receive error from broker.</li> </ol> <p></p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#logsegmentbytes-vs-logretentionbytes","title":"<code>log.segment.bytes</code> vs <code>log.retention.bytes</code>","text":""},{"location":"streaming/kafka/03-Topic_Default_Properties/#logsegmentbytes","title":"\ud83d\uddc2 log.segment.bytes","text":"<ul> <li>Think of this as the page size in a notebook.</li> <li>It controls the maximum size of a single log segment file on disk.</li> <li>Once the segment reaches this size, the broker rolls over and creates a new segment file.</li> <li>Default (Kafka): <code>1 GB</code>.</li> </ul> <p>\ud83d\udc49 Example: If <code>log.segment.bytes = 100 MB</code> and your topic partition receives 500 MB of messages, you\u2019ll end up with 5 segment files.</p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#logretentionbytes","title":"\ud83d\uddd1 log.retention.bytes","text":"<ul> <li>This is about retention policy: how much data the broker should keep per topic-partition.</li> <li>It defines the maximum total size of log data to retain before old segments get deleted.</li> <li>Default: <code>-1</code> (unlimited, rely on time-based retention).</li> </ul> <p>\ud83d\udc49 Example: If <code>log.retention.bytes = 1 GB</code> and each segment is <code>100 MB</code>, the broker will keep \\~10 segments (1 GB total). When a new segment is rolled, the oldest ones get deleted.</p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#key-difference","title":"\ud83d\udd11 Key Difference","text":"<ul> <li><code>log.segment.bytes</code> = size of each \"chunk\" (file).</li> <li><code>log.retention.bytes</code> = total allowed size before deleting old chunks.</li> </ul>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#together-in-action","title":"\u2696\ufe0f Together in action","text":"<p>Say you configure:</p> <ul> <li><code>log.segment.bytes = 100 MB</code></li> <li><code>log.retention.bytes = 1 GB</code></li> </ul> <p>Then:</p> <ol> <li>Messages are written to a segment until it hits 100 MB.</li> <li>Broker rolls over to a new file.</li> <li>Once total partition size exceeds 1 GB, broker starts deleting the oldest segment files to free space.</li> </ol> <p>\ud83d\udc49 So:</p> <ul> <li>segment = slice of cake</li> <li>retention.bytes = whole cake size allowed</li> </ul>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#deletion-at-message-level","title":"Deletion at Message Level?","text":"<p>Exactly \ud83d\udc4d \u2014 retention in Kafka/Redpanda is never at the message level, it\u2019s always at the segment level.</p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#why","title":"\ud83d\udd0d Why?","text":"<ul> <li>Messages are appended sequentially to log segment files (immutable, append-only).</li> <li>A segment is only eligible for deletion when all messages inside it are older/larger than the configured retention policies.</li> <li>This avoids the overhead of deleting messages one by one.</li> </ul>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#example","title":"\u2705 Example:","text":"<ul> <li><code>log.segment.bytes = 100 MB</code></li> <li><code>log.retention.hours = 24</code></li> </ul> <p>If your segment has messages from 10:00 \u2192 10:15:</p> <ul> <li>At 10:16 some messages are already &gt; 1 min old.</li> <li>But the whole segment stays, since it still contains \"younger\" messages.</li> <li>Only when the last message inside the segment is older than 24h does the broker delete the whole file.</li> </ul>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#what-does-not-happen","title":"\ud83d\udeab What does NOT happen:","text":"<ul> <li>Kafka/Redpanda doesn\u2019t scan inside a segment to throw away just one old message.</li> <li>That would break the append-only design and make reads/writes much slower.</li> </ul>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#retention-vs-compaction","title":"\ud83d\udd01 Retention vs Compaction","text":""},{"location":"streaming/kafka/03-Topic_Default_Properties/#1-retention-timesize-based","title":"1. Retention (time/size based)","text":"<ul> <li>Works at segment level.</li> <li>Old files (segments) are deleted when they exceed <code>log.retention.*</code> policies.</li> <li>Entire segments are removed, not individual messages.</li> <li>Good for event streaming where you don\u2019t care about keeping history forever.</li> </ul> <p>\ud83d\udccc Example: Keep data for 7 days, then drop old files.</p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#2-log-compaction-key-based","title":"2. Log Compaction (key based)","text":"<ul> <li>Works at the message level (but only for topics that have compaction enabled).</li> <li>For each key, Kafka/Redpanda keeps only the latest value.</li> <li>Old values for the same key are removed in the background.</li> <li>This ensures that at any point, the log contains the \"latest snapshot\" per key.</li> </ul> <p>\ud83d\udccc Example: Say topic is compacted and we write:</p> <pre><code>key=user1, value=Vedanth\nkey=user1, value=Baliga\nkey=user2, value=StoneX\n</code></pre> <p>\ud83d\udc49 After compaction finishes, only:</p> <pre><code>user1 \u2192 Baliga\nuser2 \u2192 StoneX\n</code></pre> <p>remains.</p>"},{"location":"streaming/kafka/03-Topic_Default_Properties/#when-to-use-what","title":"\u2696\ufe0f When to use what?","text":"<ul> <li>Retention \u2192 for event streams (transactions, logs, telemetry, clickstream).</li> <li>Compaction \u2192 for stateful data (user profiles, configs, account balances).</li> </ul> <p>\ud83d\udca1 Sometimes people combine both:</p> <ul> <li>Keep data for 7 days (<code>log.retention.hours=168</code>),</li> <li>But also enable compaction (<code>cleanup.policy=compact,delete</code>) \u2192 so you keep latest snapshots forever, while old junk is cleared.</li> </ul>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/","title":"Kafka Hardware Considerations","text":""},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#kafka-setup-hardware-considerations","title":"Kafka Setup Hardware Considerations","text":""},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#disk-throughput","title":"Disk Throughput","text":"<p>SSD's are used if there are lot of client connections.</p>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#disk-capacity","title":"Disk Capacity","text":""},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#memory","title":"Memory","text":""},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#1-jvm-heap-basics","title":"\ud83d\udd39 1. JVM Heap Basics","text":"<ul> <li>Kafka brokers run on the Java Virtual Machine (JVM).</li> <li>The JVM provides a heap = the memory area where Java objects live (data structures, buffers, metadata, etc.).</li> <li>The heap is managed by the Garbage Collector (GC), which automatically frees unused objects.</li> </ul>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#2-what-goes-into-kafkas-heap","title":"\ud83d\udd39 2. What goes into Kafka\u2019s Heap","text":"<p>Even though Kafka is an I/O-heavy system (most data lives on disk or page cache), it still needs the heap for several critical tasks:</p> <ol> <li> <p>Message Buffers</p> </li> <li> <p>Temporary storage for messages being read from producers before written to disk.</p> </li> <li> <p>Buffers used when serving fetch requests to consumers.</p> </li> <li> <p>Metadata</p> </li> <li> <p>Cluster metadata: topics, partitions, offsets, leader/follower info.</p> </li> <li> <p>Zookeeper/KRaft state in memory.</p> </li> <li> <p>Indexes and Caches</p> </li> <li> <p>Offset index and time index objects.</p> </li> <li> <p>In-memory caches like the <code>ReplicaFetcher</code> buffer, producer state maps, etc.</p> </li> <li> <p>Control Structures</p> </li> <li> <p>Java objects representing network connections, requests, and responses.</p> </li> <li> <p>Threads, queues, locks, and other concurrency structures.</p> </li> <li> <p>ZooKeeper/KRaft client state</p> </li> <li> <p>If Kafka is using ZooKeeper (older versions), ZooKeeper client connections use heap.</p> </li> <li>In KRaft (newer versions), metadata quorum state also lives in heap.</li> </ol>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#3-what-does-not-live-in-heap","title":"\ud83d\udd39 3. What does not live in Heap","text":"<ul> <li>Actual log data (the big message payloads) is written to disk (segment files).</li> <li>Kafka relies heavily on the OS page cache to serve log reads/writes efficiently.</li> <li>So the heap is not where Kafka keeps gigabytes of topic data \u2014 it\u2019s more for metadata and transient objects.</li> </ul>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#4-jvm-heap-gc-issues-in-kafka","title":"\ud83d\udd39 4. JVM Heap + GC Issues in Kafka","text":"<ul> <li>If heap is too small \u2192 OutOfMemoryError (OOM).</li> <li>If heap is too big \u2192 GC pauses get long (stop-the-world events).</li> <li> <p>That\u2019s why Kafka best practice is:</p> </li> <li> <p>Keep heap moderate (e.g., 4\u20138 GB for brokers, even if broker has 64\u2013128 GB RAM).</p> </li> <li>Let the OS page cache handle log segment data.</li> <li>Use G1 GC (Garbage First) for predictable pause times.</li> </ul>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#5-analogy","title":"\ud83d\udd39 5. Analogy","text":"<ul> <li> <p>Think of Kafka like a library:</p> </li> <li> <p>The heap is the librarian\u2019s desk (indexes, notes, active tasks).</p> </li> <li>The page cache + disk is the massive archive of books (actual topic data).</li> <li>The librarian\u2019s desk must be tidy and efficient (GC keeps it clean), but the heavy lifting (book storage) happens outside the desk.</li> </ul> <p>\u2705 In short: In Kafka, the JVM heap holds metadata, temporary buffers, and control structures, but not the bulk of the log data. That bulk lives in disk files + OS page cache. The JVM heap\u2019s job is to make sure Kafka can efficiently manage metadata, requests, and temporary data, while avoiding GC stalls.</p>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#networking","title":"Networking","text":"<p>We should have atleast 10Gb NICs and older 1Gb NICs is not sufficient.</p>"},{"location":"streaming/kafka/04-Kafka_Hardware_Considerations/#cpu","title":"CPU","text":""},{"location":"streaming/kafka/05-Kafka_Configuring_Clusters_Broker_Consideration/","title":"Kafka Configuring Clusters Broker Consideration","text":""},{"location":"streaming/kafka/05-Kafka_Configuring_Clusters_Broker_Consideration/#broker-considerations-while-configuring-clusters","title":"Broker Considerations while Configuring Clusters","text":"<p>Having multiple brokers allows us to scale the load across multiple servers. Secondly we can guard against data loss due to single system failures.</p> <p>Replication will allow us to perform maintainance work without downtime to consumers.</p> <p></p>"},{"location":"streaming/kafka/05-Kafka_Configuring_Clusters_Broker_Consideration/#how-many-brokers","title":"How many brokers?","text":"<p>Disk Capacity</p> <p>If cluster is require to maintain 10 TB data, and each broker can hold 5 Tb we need 5 brokers.</p> <p>Replicas can again change this math, if we need to have two replicas per broker then we would need 20Tb.</p> <p></p> <p>CPU</p> <p>Not a major cause of problem but if we have excessive client connections then we need to rev up CPU.</p> <p>Networking</p> <p></p>"},{"location":"streaming/kafka/05-Kafka_Configuring_Clusters_Broker_Consideration/#broker-configurations","title":"Broker Configurations","text":"<p>There are only two requirements for a broker to be part of the cluster:</p> <ol> <li>They must have same configuration for zookeeper.connect</li> <li>They must have unique <code>broker.id</code></li> </ol>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/","title":"Kafka Broker OS Tuning","text":""},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#virtual-memory-concepts","title":"Virtual Memory Concepts","text":"<p>Ideally Linux virtual memory system will autoscale and adjust itself depending on workload. We can tweak and make adjustments on how swap space is handled to suite Kafka needs.</p> <p></p> <ol> <li> <p>Swapping = bad</p> </li> <li> <p>When a machine runs out of RAM, the OS can \u201cswap\u201d memory pages out to disk.</p> </li> <li>Disk is way slower than RAM.</li> <li> <p>So if Kafka\u2019s memory gets swapped, everything slows down badly.</p> </li> <li> <p>Kafka depends on page cache</p> </li> <li> <p>Kafka doesn\u2019t keep all messages in JVM heap.</p> </li> <li>Instead, it relies on the Linux page cache (OS memory used to cache disk files).</li> <li> <p>This makes reading/writing logs super fast (like \u201cRAM-speed disk\u201d).</p> </li> <li> <p>If swapping happens</p> </li> <li> <p>It means RAM is too small.</p> </li> <li>Now the OS uses disk for memory \u2192 very slow.</li> <li>And since RAM is busy with swapping, there\u2019s less room left for page cache.</li> <li>Result: Kafka loses its main performance advantage.</li> </ol>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#simple-analogy","title":"\ud83d\udd39 Simple analogy","text":"<p>Think of RAM as a kitchen counter:</p> <ul> <li>Kafka keeps its working tools and most-used ingredients on the counter (page cache).</li> <li>If the counter is too small, the chef (OS) starts moving things to the basement (disk swap).</li> <li>Every time Kafka needs something, the chef has to run to the basement and back \u2192 huge slowdown.</li> <li>Plus, with stuff in the basement, there\u2019s even less room left on the counter \u2192 workflow collapses.</li> </ul> <p>\u2705 In short: Swapping in Kafka is terrible because:</p> <ul> <li>It makes memory operations slow (disk instead of RAM).</li> <li>It steals memory from the OS page cache, which Kafka relies on for fast log access.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#ram-vs-disk-vs-page-cache","title":"RAM vs Disk vs Page Cache","text":""},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#1-ram-physical-memory","title":"\ud83d\udd39 1. RAM (Physical Memory)","text":"<ul> <li>This is the actual physical memory chips installed in your machine.</li> <li>Super fast (nanoseconds).</li> <li>Used for active data \u2014 what your CPU is working on right now.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#2-os-memory","title":"\ud83d\udd39 2. OS Memory","text":"<ul> <li>When people say \u201cOS memory,\u201d they usually mean the portion of RAM managed by the Operating System.</li> <li> <p>The OS decides:</p> </li> <li> <p>Which processes get how much RAM.</p> </li> <li>What part of RAM to use for page cache (caching disk files).</li> <li>Whether to swap out inactive memory pages to disk if RAM runs low.</li> <li>So OS memory is not separate from RAM \u2014 it\u2019s RAM under the OS\u2019s control.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#3-disk-persistent-storage","title":"\ud83d\udd39 3. Disk (Persistent Storage)","text":"<ul> <li>Completely different from RAM.</li> <li>Much slower (milliseconds).</li> <li>Stores data permanently (files, logs, databases).</li> <li>Examples: HDD, SSD, NVMe.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#how-they-relate","title":"\ud83d\udd39 How they relate","text":"<ul> <li>RAM = fast but limited, wiped when machine restarts.</li> <li>Disk = big, slow, permanent.</li> <li>OS memory management = decides how to best use RAM + when to move (swap) stuff to disk if RAM runs out.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#analogy","title":"\ud83d\udd39 Analogy","text":"<ul> <li>RAM = desk space where you keep the stuff you\u2019re working on right now.</li> <li>Disk = filing cabinet in the basement where you store everything long term.</li> <li>OS memory management = office manager who decides what stays on your desk (RAM), what gets cached nearby (page cache), and what gets moved to the basement (swap).</li> </ul> <p>\u2705 In short:</p> <ul> <li>OS memory is just RAM managed by the operating system.</li> <li>RAM and disk are very different: RAM = fast, temporary; Disk = slow, permanent.</li> <li>Swapping happens when the OS moves data from RAM to disk because RAM is full \u2192 that\u2019s what hurts Kafka.</li> </ul> <p></p> <p>There can be lot of performance issues having pages swapped to disk. If the VM system is swapping to disk then there is not enough memory being allocated to page cache.</p>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#1-what-is-swap-space","title":"\ud83d\udd39 1. What is swap space?","text":"<ul> <li>Swap space = a portion of your disk reserved to act like extra RAM.</li> <li>If RAM is full, the OS can \u201cswap out\u201d some memory pages (inactive ones) to this disk space.</li> <li>This frees up RAM for active work.</li> </ul> <p>\u2705 Good: prevents crashes when memory is tight. \u274c Bad: disk is way slower than RAM \u2192 performance tanks if swapping happens.</p>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#2-why-swap-is-not-required","title":"\ud83d\udd39 2. Why swap is not required","text":"<ul> <li>A system can run without swap configured at all.</li> <li>If RAM runs out and no swap exists \u2192 the OS has no choice but to kill processes (OOM Killer).</li> <li>This is safer for performance-critical apps like Kafka, because it avoids the slowdown from swapping.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#3-why-some-swap-is-still-useful","title":"\ud83d\udd39 3. Why some swap is still useful","text":"<ul> <li>Swap acts as a safety net.</li> <li>If something unexpected happens (like a memory leak), instead of instantly killing Kafka, the OS can temporarily push some memory to disk.</li> <li>This may keep the system alive long enough for you to fix the issue.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#4-what-is-vmswappiness","title":"\ud83d\udd39 4. What is <code>vm.swappiness</code>","text":"<ul> <li><code>vm.swappiness</code> = Linux setting that controls how aggressively the OS uses swap.</li> <li> <p>Range: 0\u2013100.</p> </li> <li> <p><code>0</code> \u2192 avoid swap as much as possible.</p> </li> <li><code>100</code> \u2192 swap aggressively, even if RAM is free.</li> <li> <p>For Kafka and other high-throughput apps, best practice is:</p> </li> <li> <p>Keep swap configured (safety net).</p> </li> <li>But set <code>vm.swappiness=1</code> \u2192 OS will only swap as a last resort.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#5-analogy","title":"\ud83d\udd39 5. Analogy","text":"<ul> <li>RAM = your desk (fast access).</li> <li>Swap space = basement storage (slow to reach).</li> <li> <p>Swappiness = how eager the office manager is to move stuff to the basement.</p> </li> <li> <p>High swappiness \u2192 manager keeps clearing desk too early (slow).</p> </li> <li>Low swappiness \u2192 manager only uses basement if the desk is completely full.</li> </ul> <p>\u2705 In short: Swap space is disk space used as backup RAM. You don\u2019t have to configure it, but it\u2019s a good safety net. In Kafka, you don\u2019t want the OS to swap unless it\u2019s absolutely necessary \u2014 that\u2019s why the recommendation is to set <code>vm.swappiness=1</code>.</p>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#swap-vs-page-cache-drop-trade-off","title":"Swap vs Page Cache Drop Trade Off?","text":""},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#1-page-cache-refresher","title":"\ud83d\udd39 1. Page cache refresher","text":"<ul> <li>Kafka writes logs to disk files.</li> <li>Linux keeps recently used file data in RAM (this is the page cache).</li> <li>Page cache makes reads/writes much faster, because you don\u2019t always go to disk.</li> <li>So: more RAM for page cache = better Kafka performance.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#2-what-vmswappiness-controls","title":"\ud83d\udd39 2. What <code>vm.swappiness</code> controls","text":"<ul> <li> <p>When RAM is running low, Linux has two choices:</p> </li> <li> <p>Drop some page cache (free up memory by forgetting cached file data).</p> </li> <li> <p>Swap out memory pages from applications to disk (push part of their memory into swap).</p> </li> <li> <p><code>vm.swappiness</code> decides which strategy Linux prefers.</p> </li> <li> <p>High value (e.g. 60, default) \u2192 Linux is more likely to use swap.</p> </li> <li>Low value (e.g. 1) \u2192 Linux is more likely to drop page cache instead of swapping.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#3-why-dropping-page-cache-is-better-than-swapping","title":"\ud83d\udd39 3. Why dropping page cache is better than swapping","text":"<ul> <li> <p>Dropping page cache:</p> </li> <li> <p>You lose some cached file data.</p> </li> <li>But next time you need it, you just fetch from disk again (slower than cache, but predictable).</li> <li> <p>Swapping memory to disk:</p> </li> <li> <p>Takes active memory pages (from Kafka or other processes) and moves them to disk.</p> </li> <li>If Kafka needs those pages back \u2192 huge stall (disk is thousands of times slower than RAM).</li> <li>Causes unpredictable latency spikes \u2192 very bad for Kafka.</li> </ul> <p>\ud83d\udc49 So the recommendation: better to reduce page cache than to start using swap.</p>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#4-simplified-analogy","title":"\ud83d\udd39 4. Simplified analogy","text":"<ul> <li>Imagine RAM as your desk space.</li> <li>Page cache = reference books you keep on your desk for quick access.</li> <li>Kafka\u2019s working memory = active notes you\u2019re writing on.</li> </ul> <p>When desk space runs low:</p> <ul> <li>Option A (drop cache): Put away a few reference books (page cache). If you need them again, you fetch them from the library (disk).</li> <li>Option B (swap): Force yourself to put away half-written notes (swap). When you need them again, you must slowly re-read and re-write them from storage.</li> </ul> <p>\ud83d\udc49 Option A (drop cache) slows you down a little. \ud83d\udc49 Option B (swap) can freeze you mid-sentence.</p> <p>\u2705 In short:</p> <ul> <li><code>vm.swappiness</code> controls whether Linux prefers to swap memory to disk or drop page cache when RAM is low.</li> <li>For Kafka, it\u2019s always better to drop page cache than to use swap, because swapping makes performance unpredictable.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#swap-is-controlled-by-linuz","title":"Swap is controlled by Linuz","text":""},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#1-what-swap-actually-does","title":"\ud83d\udd39 1. What swap actually does","text":"<ul> <li>The Linux kernel decides which memory pages to swap out to disk when RAM is tight.</li> <li>It doesn\u2019t only swap \u201cunused\u201d memory \u2014 it can also swap out memory from processes that are still running.</li> <li>If the process suddenly needs that page again \u2192 it has to page fault and reload it from disk.</li> <li>That reload can take milliseconds (vs nanoseconds from RAM) \u2192 a huge delay.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#2-why-this-is-unpredictable","title":"\ud83d\udd39 2. Why this is unpredictable","text":"<ul> <li>The kernel\u2019s swapping decision depends on heuristics (like least-recently-used pages), but it isn\u2019t perfect.</li> <li>A page Kafka really needs (e.g., part of a producer buffer or replica fetcher state) might get swapped out.</li> <li>Kafka doesn\u2019t control which memory is swapped \u2014 the OS does.</li> <li>So you can suddenly get a latency spike even though Kafka is \u201chealthy\u201d otherwise.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#3-impact-on-kafka","title":"\ud83d\udd39 3. Impact on Kafka","text":"<ul> <li>Producer \u2192 sends data \u2192 broker stalls (waiting for swapped memory). Producer sees high latency.</li> <li>Consumer \u2192 fetch request delayed because Kafka\u2019s fetch buffer got swapped.</li> <li>GC (garbage collector) \u2192 if its metadata gets swapped, GC pauses are even worse.</li> </ul> <p>\ud83d\udc49 This is why in Kafka best practices:</p> <ul> <li>Swap is treated as a last resort only (swappiness = 1).</li> <li>Or completely disabled on dedicated Kafka brokers.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#analogy_1","title":"\ud83d\udd39 Analogy","text":"<p>It\u2019s like your notes are on your desk (RAM). The office manager (OS) decides, \u201cI think you don\u2019t need this notebook right now\u201d \u2192 and puts it in the basement (swap). When you actually do need it, you have to run to the basement, fetch it back, and only then continue \u2192 unpredictable stall.</p> <p>Yes - even actively used memory pages can be swapped to disk, and when Kafka needs them back, performance stalls unpredictably.</p>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#tldr","title":"TLDR!!!","text":""},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#1-page-cache-drop","title":"\ud83d\udd39 1. Page cache drop","text":"<ul> <li>The OS discards cached file data from RAM.</li> <li>That data is still on disk already (Kafka log segments).</li> <li>If Kafka needs it again \u2192 just read from disk normally.</li> <li>Cost: one normal disk read.</li> <li>Predictable: performance hit is known (disk I/O latency).</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#2-swap","title":"\ud83d\udd39 2. Swap","text":"<ul> <li>The OS actively writes process memory pages (e.g., Kafka\u2019s JVM heap objects, control structures) to swap space on disk.</li> <li>Those pages do not exist on disk already \u2014 the kernel must write them out before freeing RAM.</li> <li>If Kafka needs them back \u2192 it has to pause until the kernel reloads them from swap.</li> <li>Cost: one disk write and one disk read.</li> <li>Unpredictable: Kafka may stall at random, because it doesn\u2019t control which memory pages get swapped.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#3-why-kafka-cares","title":"\ud83d\udd39 3. Why Kafka cares","text":"<ul> <li>Kafka log data (page cache) \u2192 dropping it is okay, since the log is durable on disk.</li> <li>Kafka\u2019s heap memory (swap) \u2192 swapping it causes random stalls, because suddenly the broker can\u2019t access in-use objects until they\u2019re paged back.</li> </ul>"},{"location":"streaming/kafka/06-Kafka_Broker_OS_Tuning/#analogy_2","title":"\ud83d\udd39 Analogy","text":"<ul> <li>Page cache drop = You borrowed a reference book and left it on your desk (RAM). The office manager takes it away. If you need it again, you just check it out from the library (disk). No harm done.</li> <li>Swap = You\u2019re actively writing in your notebook (heap). The office manager snatches it, boxes it, and sends it to the basement (swap). If you need it mid-thought, you\u2019re frozen until it\u2019s retrieved.</li> </ul> <p>\u2705 So the difference:</p> <ul> <li>Dropping page cache = safe, predictable slowdown (just a disk read).</li> <li>Swapping = unsafe, unpredictable stalls (extra writes, random process freezes).</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/","title":"Kafka Os Tuning Dirty Page Handling","text":""},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#kafka-os-tuning-dirty-page-handling","title":"Kafka OS Tuning : Dirty Page Handling","text":"<p>Kafka relies on disk I/O operations to give good response time to producers sending the data.</p> <p>Hence the log segments are put on a fast disk, it can be an individual disk with fast response time like SSD or a disk subsystem with lot of NVRAM for caching like RAID.</p>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#1-what-are-dirty-pages","title":"\ud83d\udd39 1. What are \u201cdirty pages\u201d?","text":"<ul> <li>A page = a chunk of memory.</li> <li> <p>A page is called dirty if it contains data that has been changed in memory but not yet written to disk.</p> </li> <li> <p>Example: You write a log entry \u2192 it first goes into memory.</p> </li> <li>That memory page is now \u201cdirty.\u201d</li> <li>Later, the kernel flushes it to disk.</li> </ul> <p>This buffering is good, because writing to RAM is faster than writing to disk, and the kernel can group small writes together.</p>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#2-what-is-vmdirty_background_ratio","title":"\ud83d\udd39 2. What is <code>vm.dirty_background_ratio</code>?","text":"<ul> <li>It\u2019s a kernel parameter that sets:   \ud83d\udc49 \u201cHow much of total system memory can be dirty pages before the background flusher starts writing them to disk.\u201d</li> <li>Value is a percentage of total RAM.</li> <li> <p>Default = 10.</p> </li> <li> <p>Means: if 10% of system memory contains dirty pages, the kernel\u2019s background flush process starts.</p> </li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#3-what-happens-if-you-lower-it","title":"\ud83d\udd39 3. What happens if you lower it?","text":"<ul> <li>Example: set <code>vm.dirty_background_ratio = 5</code>.</li> <li>Now the kernel starts flushing earlier (when only 5% of RAM is dirty).</li> <li> <p>Result:</p> </li> <li> <p>Dirty data stays in memory for less time.</p> </li> <li>Disk writes happen more continuously instead of in big bursts.</li> <li>Good for workloads where you don\u2019t want long spikes of writes.</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#4-why-not-set-it-to-0","title":"\ud83d\udd39 4. Why not set it to 0?","text":"<ul> <li>If it\u2019s 0, the kernel would flush immediately whenever any page becomes dirty.</li> <li> <p>That means:</p> </li> <li> <p>No buffering.</p> </li> <li>Every write goes straight to disk.</li> <li>Eliminates the kernel\u2019s ability to smooth out temporary spikes (e.g., a burst of writes).</li> <li>Performance would tank, because you\u2019d lose batching benefits.</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#5-analogy","title":"\ud83d\udd39 5. Analogy","text":"<p>Think of RAM like a kitchen counter and disk like the pantry in another room:</p> <ul> <li>You prepare groceries (data) on the counter first.</li> <li>When the counter gets 10% full (default), the helper (kernel flusher) starts moving items to the pantry.</li> <li>If you lower it to 5%, the helper starts earlier \u2192 the counter never gets too messy.</li> <li>But if you set it to 0, the helper runs every single time you put something down \u2192 you can\u2019t work efficiently anymore.</li> </ul> <p>\u2705 In summary:</p> <ul> <li><code>vm.dirty_background_ratio</code> controls when the kernel starts flushing dirty pages to disk.</li> <li>Lower values \u2192 smoother, more frequent flushes.</li> <li>Too low (like 0) \u2192 kills performance because every write goes straight to disk.</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#synchronous-and-background-dirty-page-handling","title":"Synchronous and Background Dirty Page Handling","text":"<p>Perfect \ud83d\udc4d you\u2019re asking about the second half of Linux dirty page handling \u2014 <code>vm.dirty_ratio</code>. Let\u2019s break it down step by step.</p>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#1-what-is-vmdirty_ratio","title":"\ud83d\udd39 1. What is <code>vm.dirty_ratio</code>?","text":"<ul> <li>It\u2019s another Linux kernel setting (percentage of total RAM).</li> <li>It controls the absolute upper limit of dirty pages allowed in memory.</li> </ul> <p>\ud83d\udc49 Difference vs <code>vm.dirty_background_ratio</code>:</p> <ul> <li><code>dirty_background_ratio</code> \u2192 threshold when background flushing starts.</li> <li><code>dirty_ratio</code> \u2192 hard limit where normal processes are forced to stop and flush data themselves (synchronous flush).</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#2-default-value","title":"\ud83d\udd39 2. Default Value","text":"<ul> <li>Default = 20% of total memory.</li> <li>Example: If you have 16GB RAM \u2192 at most 3.2GB can be dirty pages.</li> <li> <p>If you reach this limit:</p> </li> <li> <p>Any new writes block until data is flushed \u2192 app threads do the flushing.</p> </li> <li>This is much slower, because the app itself is waiting on disk I/O.</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#3-raising-the-limit","title":"\ud83d\udd39 3. Raising the Limit","text":"<ul> <li> <p>If you set <code>vm.dirty_ratio = 60</code> or <code>80</code>:</p> </li> <li> <p>Now up to 60\u201380% of memory can be dirty pages.</p> </li> <li>More data can stay in RAM before being forced to disk.</li> <li>This means higher throughput for write-heavy apps like Kafka, because they batch more writes in memory before hitting disk.</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#4-risks","title":"\ud83d\udd39 4. Risks","text":"<ul> <li>More unflushed data in RAM = if the server crashes, more data is lost.</li> <li>Longer I/O pauses = if memory suddenly fills with dirty pages, the kernel will force a big synchronous flush \u2192 applications block until gigabytes of data are written out.</li> <li>These pauses are sometimes called \u201cI/O storms\u201d or \u201cwrite cliffs.\u201d</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#5-why-mention-kafka-replication","title":"\ud83d\udd39 5. Why Mention Kafka Replication?","text":"<ul> <li>Kafka brokers rely on disk writes for durability.</li> <li>If you allow too many dirty pages (high <code>vm.dirty_ratio</code>), and the machine crashes, all that unflushed data is lost.</li> <li>Replication across brokers ensures that even if one machine loses data, others have a copy.</li> <li>So the text is saying:   \ud83d\udc49 \u201cIf you increase <code>dirty_ratio</code> to improve throughput, you must rely on replication to stay safe against data loss.\u201d</li> </ul>"},{"location":"streaming/kafka/07-Kafka_OS_Tuning_Dirty_Page_Handling/#6-analogy","title":"\ud83d\udd39 6. Analogy","text":"<ul> <li>Imagine you\u2019re taking notes (data) on paper (RAM) before typing them into the computer (disk).</li> <li>Rule 1: At 5 pages of notes \u2192 your friend (background flush) starts typing them up slowly.</li> <li>Rule 2: At 20 pages \u2192 you\u2019re not allowed to write more notes until you type everything in yourself (synchronous flush).</li> <li> <p>If you increase the limit to 60 pages:</p> </li> <li> <p>You can keep writing more before being forced to stop.</p> </li> <li>But if you trip the limit, you\u2019ll have to type 60 pages at once \u2014 long pause.</li> <li>If you spill coffee (server crash), you lose a lot more notes.</li> </ul> <p>\u2705 In summary:</p> <ul> <li><code>vm.dirty_ratio</code> = maximum dirty memory before synchronous flush kicks in.</li> <li> <p>Default = 20%. Increasing to 60\u201380% improves write throughput for Kafka, but:</p> </li> <li> <p>Increases risk of data loss on crash.</p> </li> <li>Increases risk of long blocking flushes.</li> <li>That\u2019s why Kafka clusters should use replication if you tune this aggressively.</li> </ul>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/","title":"Kafka File Descriptors Overcommit Memory","text":""},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#kafka-file-descriptors-and-overcommit-memory","title":"Kafka File Descriptors and Overcommit Memory","text":"<p>Alright \ud83d\udc4d let\u2019s simplify this down so it\u2019s clear, no kernel jargon needed.</p>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#1-kafka-and-file-descriptors","title":"\ud83d\udd39 1. Kafka and File Descriptors","text":"<ul> <li>Every log segment (chunk of a partition stored on disk) = needs a file descriptor.</li> <li>Every client connection (producer, consumer, replication) = also needs a file descriptor.</li> <li>So if a broker has lots of partitions and lots of connections \u2192 it needs a very large number of file descriptors open at once.</li> </ul>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#2-why-vmmax_map_count","title":"\ud83d\udd39 2. Why <code>vm.max_map_count</code>?","text":"<ul> <li>Linux limits how many memory-mapped files (which Kafka uses for log segments) a process can have.</li> <li>If this limit is too low, Kafka crashes or can\u2019t open new log segments.</li> <li>Setting <code>vm.max_map_count = 400,000</code> or <code>600,000</code> gives Kafka enough room for large clusters.</li> </ul> <p>\ud83d\udc49 Think of it like: \u201cHow many drawers can Kafka keep open at once?\u201d If too few, Kafka gets stuck. Raising the limit gives Kafka more drawers.</p>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#3-why-vmovercommit_memory0","title":"\ud83d\udd39 3. Why <code>vm.overcommit_memory=0</code>?","text":"<ul> <li>This tells Linux:   \ud83d\udc49 \u201cDon\u2019t promise applications more memory than you actually have.\u201d</li> <li>If set to <code>1</code> or <code>2</code>, the OS may over-commit (promise more than available).</li> <li> <p>For Kafka, this is bad because:</p> </li> <li> <p>Kafka needs predictable memory for high ingestion.</p> </li> <li>If the OS over-promises, it may run out and start killing processes (OOM Killer).</li> </ul> <p>So <code>0</code> = safe mode: kernel checks available memory before giving it to Kafka.</p>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#4-putting-it-together","title":"\ud83d\udd39 4. Putting It Together","text":"<ul> <li>Kafka needs a lot of open files \u2192 increase <code>vm.max_map_count</code>.</li> <li>Kafka needs reliable memory allocation \u2192 set <code>vm.overcommit_memory=0</code>.</li> </ul>"},{"location":"streaming/kafka/08-Kafka_File_Descriptors_Overcommit_Memory/#5-simple-analogy","title":"\ud83d\udd39 5. Simple Analogy","text":"<ul> <li> <p>Imagine Kafka is running a library.</p> </li> <li> <p>Every log segment = a book on the table.</p> </li> <li> <p>Every client connection = another open book.</p> </li> <li> <p>If Linux says: \u201cYou can only keep 65,000 books open,\u201d Kafka will choke. \u2192 raise <code>vm.max_map_count</code> to 400k+ so all books can stay open.</p> </li> <li> <p>Memory is like seats in the library.</p> </li> <li> <p>If the librarian over-commits (\u201cSure, 200 people can sit here\u201d when only 100 seats exist), people fight for space \u2192 chaos.</p> </li> <li> <p>Setting <code>vm.overcommit_memory=0</code> ensures only as many people as seats \u2192 stable Kafka.</p> </li> </ul> <p>\u2705 In short:</p> <ul> <li>Raise <code>vm.max_map_count</code> so Kafka can keep lots of log segments + connections open.</li> <li>Keep <code>vm.overcommit_memory=0</code> so Kafka only uses real, available memory \u2192 avoids crashes.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/","title":"Kafka Production Concerns","text":""},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#production-concerns-in-kafka","title":"Production Concerns in Kafka","text":""},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#1-garbage-collection-before-g1gc","title":"1. Garbage Collection Before G1GC","text":"<ul> <li>Tuning Java GC was difficult and required a lot of trial and error.</li> <li>Developers had to carefully adjust GC options based on how the application used memory.</li> <li>Different workloads often required different GC tuning strategies.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#2-introduction-of-g1gc","title":"2. Introduction of G1GC","text":"<ul> <li>G1GC (Garbage-First Garbage Collector) was introduced in Java 7.</li> <li>Initially, it was not stable enough for production.</li> <li>By Java 8 and Java 11, it had matured significantly.</li> <li>Kafka now recommends G1GC as the default GC.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#3-why-g1gc-is-better","title":"3. Why G1GC is Better","text":"<ul> <li>Adaptive: Automatically adjusts to different workloads without requiring heavy manual tuning.</li> <li>Consistent Pause Times: Designed to provide predictable, shorter GC pauses (important for systems like Kafka that need low latency).</li> <li> <p>Scales with Large Heaps:</p> </li> <li> <p>Older collectors would stop and scan the entire heap during GC.</p> </li> <li>G1GC divides the heap into smaller regions (zones).</li> <li>It collects garbage region by region instead of the whole heap at once, which reduces pause times.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#4-configuration-simplicity","title":"4. Configuration Simplicity","text":"<ul> <li>Unlike older collectors, G1GC requires minimal manual tuning for most use cases.</li> <li>Out-of-the-box defaults are good enough for many production environments, including Kafka.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#parameters","title":"Parameters","text":"<p>MaxGCPauseMillis : Preferred but not strict pause time for each GC cycle. By default its 200ms.</p> <p>InitiatingHeapOccupancyPercent : Specifies total heap that may be in use before garbage collection comes into force. Default is 45. This includes both old and new zone.</p>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#production-concerns-datacenter-layout","title":"Production Concerns : Datacenter Layout","text":""},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#1-what-is-rack-awareness","title":"1. What is Rack Awareness","text":"<ul> <li>Kafka stores multiple replicas of a partition for fault tolerance.</li> <li>If all replicas of a partition are placed on brokers in the same rack, they could all fail together if that rack loses power or network connectivity.</li> <li>Rack awareness ensures that replicas for a partition are spread across different racks or fault domains.</li> <li>To enable this, each broker must be configured with its rack location using the <code>broker.rack</code> property.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#2-how-it-works-in-practice","title":"2. How It Works in Practice","text":"<ul> <li>When new partitions are created, Kafka places their replicas across racks so they don\u2019t share the same rack.</li> <li>In cloud environments, <code>broker.rack</code> can be mapped to a cloud \u201cfault domain\u201d or \u201cavailability zone\u201d for the same benefit.</li> <li> <p>Important limitation:</p> </li> <li> <p>Rack awareness is only applied at partition creation.</p> </li> <li>Kafka does not automatically re-check or fix replicas if they later end up on the same rack (for example, due to manual partition reassignment).</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#3-maintaining-rack-awareness","title":"3. Maintaining Rack Awareness","text":"<ul> <li>Because Kafka does not self-correct rack misplacements, you need external tools to maintain balance over time.</li> <li>One common tool is Cruise Control, which helps monitor and rebalance partitions while respecting rack awareness.</li> <li>Regular balancing ensures that partitions remain fault-tolerant across racks.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#4-best-practices-for-infrastructure","title":"4. Best Practices for Infrastructure","text":"<ul> <li>Ideally, each Kafka broker should be in a different rack to maximize fault tolerance.</li> <li>At minimum, brokers should avoid sharing the same single points of failure for power and networking.</li> <li> <p>Recommendations:</p> </li> <li> <p>Use dual power connections (to different power circuits).</p> </li> <li>Use dual network switches and configure bonded interfaces for failover.</li> <li>Even with redundancy, placing brokers in separate racks provides stronger resilience.</li> <li>This is also useful for planned maintenance, where a rack may need to be taken offline temporarily.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#5-key-takeaway","title":"5. Key Takeaway","text":"<ul> <li>Rack awareness ensures partition replicas are distributed across failure domains.</li> <li>You must set <code>broker.rack</code> correctly for each broker.</li> <li>Kafka applies this only when partitions are created; you need rebalancing tools like Cruise Control to maintain it.</li> <li>For hardware resilience, brokers should be deployed across racks, with redundant power and networking.</li> </ul>"},{"location":"streaming/kafka/09-Kafka_Production_Concerns/#production-concerns-colocating-applications-on-zookeeper","title":"Production Concerns : Colocating Applications on Zookeeper","text":"<p>Writes to Zookeeper is are only performed when consumer groups are updated or changes on cluster is made. So we do not need dedicated Zookeeper for single Kafka cluster, it can be shared across clusters.</p> <p>Consumers have a choice of using Zookeeper or Kafka for commiting offsets. Each consumer will perform a zookeeper write for every partition it consumes and dafault is 1 minuute. In this timeframe, the consumer may read duplicate messages. If multiple consumers write at same time we may end up with concurrent write issues.</p>"},{"location":"streaming/kafka/10-Kafka_Message_Types/","title":"Constructing Kafka Producer","text":"<p>There are three primary properties to be specified.</p> <p>bootstrap.servers</p> <p>key.serializer</p> <p>Let\u2019s break this down step by step:</p> <ol> <li> <p>Kafka message format</p> </li> <li> <p>Kafka brokers store and transmit messages as byte arrays for both keys and values.</p> </li> <li> <p>This is because Kafka itself doesn\u2019t know or care what the data means; it just moves raw bytes.</p> </li> <li> <p>Producer flexibility</p> </li> <li> <p>When using the Kafka Producer API in Java, you don\u2019t have to manually create byte arrays.</p> </li> <li>Instead, the producer API lets you work with normal Java objects as keys and values (like <code>String</code>, <code>Integer</code>, or custom objects).</li> <li> <p>This makes your producer code more readable and type-safe.</p> </li> <li> <p>Serialization requirement</p> </li> <li> <p>Since Kafka only understands byte arrays, there must be a conversion step from your Java object (e.g., <code>String</code> or <code>Integer</code>) into a byte array before sending.</p> </li> <li> <p>That\u2019s what the Serializer interface is for: it defines how to transform an object into a byte array.</p> </li> <li> <p><code>key.serializer</code> configuration</p> </li> <li> <p>In the producer configuration, you set the <code>key.serializer</code> property to the class name of a serializer.</p> </li> <li>This tells the Kafka producer which serializer to use when converting your key objects into bytes.</li> <li> <p>Example serializers provided by Kafka:</p> <ul> <li><code>org.apache.kafka.common.serialization.StringSerializer</code></li> <li><code>org.apache.kafka.common.serialization.IntegerSerializer</code></li> <li><code>org.apache.kafka.common.serialization.ByteArraySerializer</code></li> </ul> </li> </ol> <p>If you use a common type like <code>String</code> or <code>Integer</code>, you don\u2019t need to write your own serializer \u2014 you just use the one provided.</p> <ol> <li> <p>Required setting</p> </li> <li> <p>Even if you don\u2019t plan to use keys in your messages (only values), you must still set <code>key.serializer</code>.</p> </li> <li>In that case, you can configure the producer to use the <code>Void</code> type for keys, along with the provided <code>org.apache.kafka.common.serialization.VoidSerializer</code>. This is a special serializer that essentially says, \u201cI\u2019m not sending a key.\u201d</li> </ol> <p>In short:</p> <ul> <li>Kafka always works with byte arrays internally.</li> <li><code>key.serializer</code> defines which class will convert your key objects into byte arrays.</li> <li>Kafka provides built-in serializers for common Java types (<code>String</code>, <code>Integer</code>, etc.), so you usually don\u2019t need to implement your own.</li> <li>You must configure <code>key.serializer</code> even if your producer doesn\u2019t use keys \u2014 in that case, you use <code>VoidSerializer</code>.</li> </ul> <p>Great, let\u2019s go through <code>value.serializer</code> in parallel with <code>key.serializer</code> so you see the complete picture.</p>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#1-kafka-expects-byte-arrays-for-both-keys-and-values","title":"1. Kafka expects byte arrays for both keys and values","text":"<ul> <li> <p>Every Kafka message has two parts:</p> </li> <li> <p>Key \u2192 used mainly for partitioning (decides which partition the record goes to).</p> </li> <li>Value \u2192 the actual payload of the message.</li> <li>Kafka itself only works with raw byte arrays, so both key and value must be converted before sending.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#2-why-do-we-need-a-serializer","title":"2. Why do we need a serializer?","text":"<ul> <li>You might be producing records with Java objects like <code>String</code>, <code>Integer</code>, or even a custom POJO (Plain Old Java Object).</li> <li>The Producer API is generic \u2014 you can use any object type for keys and values.</li> <li>Before sending, these objects need to be serialized into byte arrays, which Kafka understands.</li> <li>That\u2019s where the <code>Serializer</code> interface comes in.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#3-valueserializer-configuration","title":"3. <code>value.serializer</code> configuration","text":"<ul> <li> <p>Just like <code>key.serializer</code>, you must tell the producer how to serialize the value of your records.</p> </li> <li> <p>Example serializers included with Kafka:</p> </li> <li> <p><code>org.apache.kafka.common.serialization.StringSerializer</code></p> </li> <li><code>org.apache.kafka.common.serialization.IntegerSerializer</code></li> <li> <p><code>org.apache.kafka.common.serialization.ByteArraySerializer</code></p> </li> <li> <p>If you\u2019re using common Java types, you can use these directly.</p> </li> <li> <p>If you have a custom object (e.g., a <code>User</code> class), you\u2019d typically:</p> </li> <li> <p>Implement a custom serializer (convert object \u2192 JSON, Avro, or Protobuf \u2192 bytes).</p> </li> <li>Or use frameworks like Kafka Avro Serializer, Protobuf Serializer, or JSON Serializer from Confluent or other libraries.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#4-required-settings","title":"4. Required settings","text":"<ul> <li><code>key.serializer</code> \u2192 must always be set (even if you don\u2019t use keys \u2192 use <code>VoidSerializer</code>).</li> <li><code>value.serializer</code> \u2192 must always be set, because you\u2019ll always send some value.</li> </ul> <p>Example producer properties:</p> <pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\n\n// Key serializer (say we use strings for keys)\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n// Value serializer (say we use integers for values)\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\");\n</code></pre>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#5-example-in-action","title":"5. Example in action","text":"<p>If you create a record like this:</p> <pre><code>ProducerRecord&lt;String, Integer&gt; record =\n    new ProducerRecord&lt;&gt;(\"my-topic\", \"user123\", 42);\n</code></pre> <ul> <li><code>\"user123\"</code> \u2192 key (String) \u2192 converted to bytes by <code>StringSerializer</code>.</li> <li><code>42</code> \u2192 value (Integer) \u2192 converted to bytes by <code>IntegerSerializer</code>.</li> <li>Kafka stores/transmits them as byte arrays.</li> <li>When consumed, the consumer will use the matching deserializer (<code>StringDeserializer</code> and <code>IntegerDeserializer</code>) to convert them back.</li> </ul> <p>Summary:</p> <ul> <li><code>key.serializer</code> = defines how to convert keys \u2192 byte arrays.</li> <li><code>value.serializer</code> = defines how to convert values \u2192 byte arrays.</li> <li>Both are required properties.</li> <li>Use Kafka\u2019s built-in serializers for standard types, or write your own/custom ones for complex objects.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#three-primary-methods-of-sending-messages","title":"Three Primary Methods of Sending Messages","text":"<ol> <li>Fire and Forget</li> </ol> <p>We send message and dont care if it reached or not. Generally it does since Kafka is highly available and producer retries automatically.</p> <p>In case of Timeout or non retriable errors, the message gets lost.</p> <ol> <li>Synchronous Send</li> </ol> <p>the send() message returns a Future object. We use get() to wait on the Furture and see if send() was successful before sending next record.</p> <ol> <li>Asynchronous Send</li> </ol> <p>We call the send() method with a callback function that triggers when kafka sends a response</p> <p>Sychronous Send Code</p> <p>Let\u2019s unpack this step by step:</p>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#1-what-does-sending-a-message-synchronously-mean","title":"1. What does sending a message synchronously mean?","text":"<ul> <li>When you use the Kafka producer, you call <code>send()</code> to send a message.</li> <li><code>send()</code> is asynchronous by default \u2014 it immediately returns a <code>Future&lt;RecordMetadata&gt;</code>, and the actual send happens in the background.</li> <li>But you can force it to be synchronous by calling <code>.get()</code> on that future:</li> </ul> <p><pre><code>producer.send(record).get();\n</code></pre> * This means your program waits until Kafka acknowledges the message before continuing.</p>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#2-why-would-you-send-synchronously","title":"2. Why would you send synchronously?","text":"<ul> <li>You get immediate feedback on whether the message was successfully written or failed.</li> <li> <p>You can catch exceptions like:</p> </li> <li> <p>Broker errors (e.g., \"not enough replicas\").</p> </li> <li>Exhausted retries (Kafka gave up after multiple attempts).</li> <li>This can be useful in simple examples, tests, or when correctness is more important than speed.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#3-the-performance-trade-off","title":"3. The performance trade-off","text":"<ul> <li> <p>Kafka brokers take some time to respond to a produce request \u2014 it could be as quick as a couple of milliseconds, or as slow as several seconds (if the cluster is busy, network latency is high, or replicas need syncing).</p> </li> <li> <p>With synchronous sends:</p> </li> <li> <p>The sending thread blocks (waits) until it gets the broker\u2019s response.</p> </li> <li>During this time, it cannot send any other messages.</li> <li> <p>This makes throughput very low, because you\u2019re effectively sending one message at a time.</p> </li> <li> <p>With asynchronous sends:</p> </li> <li> <p>The producer can batch multiple records together in the background while waiting for acknowledgments.</p> </li> <li>This greatly improves throughput and efficiency.</li> </ul>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#4-why-synchronous-is-avoided-in-production","title":"4. Why synchronous is avoided in production","text":"<ul> <li>Production applications usually need to send thousands or millions of messages per second.</li> <li>If each message is sent synchronously, the throughput drops drastically because the client spends most of its time waiting.</li> <li>That\u2019s why synchronous sends are almost never used in real systems.</li> <li>They are often used only in demos, tutorials, or test programs where clarity is more important than performance.</li> </ul> <ul> <li>Synchronous send = wait for acknowledgment before sending the next message. Simple, but slow.</li> <li>Asynchronous send = fire off the message, keep working, and handle success/failure via callback. Much faster and used in production.</li> </ul> <p>There are retriable and non retriable errors in Kafka, connection and leader unresponsive errors are retriable but message size too large is not.</p>"},{"location":"streaming/kafka/10-Kafka_Message_Types/#async-send-callback-function","title":"Async Send Callback Function","text":""},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/","title":"Configuring Producers in Kafka","text":"<ol> <li><code>acks</code></li> </ol> <p>Controls how many partition replicas must receive the record before Kafka producer can consider write successful.</p> <p>By default Kafka considers it successful when the leader receives the message but this may be slow and impact durability of written messages.</p> <p>ack = 0</p> <p>The producer will not wait for the message acknowledgement to be received.</p> <p>acks = 1</p> <p>The producer will receive a success response from broker the moment leader receives the message, if the leader doesnt receive it producer retries.</p> <p>The message can get lost if leader crashes and latest messages are still not replicated to new leader.</p> <p>acks = all</p> <p>Only once all in sync replicas receive message, the write is successful. This has very high latency.</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#messgae-delivery-time-parameters","title":"Messgae Delivery Time Parameters","text":"<p>'How long will it take for a call to send() to be successful or failed?'</p> <p>Two main time intervals:</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#1-time-until-an-async-call-to-send-returns","title":"1. Time until an async call to <code>send()</code> returns","text":"<ul> <li><code>producer.send(record, callback)</code> is asynchronous.</li> <li>Normally, <code>send()</code> should return very quickly because it just places the record into a buffer (producer\u2019s local memory).</li> <li> <p>However, there are situations where <code>send()</code> can actually block the calling thread before returning:</p> </li> <li> <p>If the producer\u2019s buffer is full (e.g., you\u2019re producing faster than Kafka can send), <code>send()</code> will block until there\u2019s room, or until <code>max.block.ms</code> is exceeded.</p> </li> <li>This is the \u201ctime until send() returns\u201d.</li> <li>During this time, your application thread is stuck waiting \u2014 it can\u2019t continue until the record is accepted into the buffer.</li> </ul> <p>So: this measures how long the application thread waits just to hand off the record to the producer.</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#2-time-from-send-return-until-callback-triggered","title":"2. Time from send() return until callback triggered","text":"<ul> <li>Once <code>send()</code> has returned, the record is now queued inside the producer\u2019s buffer.</li> <li>From here, the producer batches it with other records (to the same partition) and eventually sends the batch to the Kafka broker.</li> <li>The callback you passed to <code>send()</code> will be triggered later, once Kafka responds.</li> </ul> <p>This second interval covers:</p> <ol> <li>Time the record spends in the local buffer waiting for batching.</li> <li>Network transmission time to Kafka.</li> <li>Broker processing time (writing to log, replicating if needed).</li> <li>Broker response coming back.</li> </ol> <p>At the end:</p> <ul> <li>If everything succeeds \u2192 callback gets a success with <code>RecordMetadata</code>.</li> <li>If retries fail or an unrecoverable error happens \u2192 callback gets a failure exception.</li> </ul> <p>So: this interval = \"from the moment <code>send()</code> returns successfully \u2192 until Kafka responds (success or failure).\"</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#putting-it-together","title":"Putting it together","text":"<ul> <li>Interval 1: \u201cTime until send() returns\u201d = how long your application thread is blocked just waiting to enqueue the record.</li> <li>Interval 2: \u201cTime from send() return to callback\u201d = how long Kafka + the producer pipeline takes to actually deliver the record and get an acknowledgment.</li> </ul> <ol> <li>First wait: \u201cCan I even hand this message to the producer, or is the buffer full?\u201d</li> <li>Second wait: \u201cNow that it\u2019s handed off, how long until Kafka says the message is written (or failed)?\u201d</li> </ol> <p>max.block.ms</p> <p>How long a producer may block send() thread while waiting for metadata via <code>partitionsFor()</code></p> <p>delivery.timeout.ms</p> <p>Amount of time from a point where the record is set for sending until either the broker or client gives up / times out.</p> <p>It should be greater than sum of <code>linger.ms</code> and <code>request.timeout.ms</code></p> <p>This is about how Kafka producer timeouts interact with retries and the callback you provide to <code>send()</code>. Let\u2019s break it into parts.</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#1-what-is-deliverytimeoutms","title":"1. What is <code>delivery.timeout.ms</code>?","text":"<ul> <li>It is the maximum time the producer will try to deliver a record, from the moment you call <code>send()</code> until a final outcome (success or failure).</li> <li>Default = 120,000 ms (2 minutes).</li> <li>It covers both retries and waiting in the buffer.</li> <li>If this time limit is reached, the record is considered failed, and the callback is triggered with an exception.</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#2-case-1-timeout-happens-during-retries","title":"2. Case 1: Timeout happens during retries","text":"<ul> <li>Imagine the producer sends a batch \u2192 broker responds with an error (like <code>NOT_ENOUGH_REPLICAS</code>).</li> <li>The producer will retry sending (depending on <code>retries</code> and <code>retry.backoff.ms</code>).</li> <li>But if all those retries push the elapsed time past <code>delivery.timeout.ms</code>, the producer gives up.</li> <li> <p>What happens then?</p> </li> <li> <p>The callback will get the last error exception that the broker returned before retrying.</p> </li> <li>Example: If the broker said \u201cNot enough replicas\u201d before retrying, that same exception will be delivered to you.</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#3-case-2-timeout-happens-while-waiting-to-send","title":"3. Case 2: Timeout happens while waiting to send","text":"<ul> <li> <p>Records don\u2019t always get sent immediately:</p> </li> <li> <p>They may wait in the buffer (producer batches messages).</p> </li> <li>Or they may wait because the broker is slow or the partition leader is unavailable.</li> <li> <p>If the record is still sitting in the batch (never sent) when <code>delivery.timeout.ms</code> is exceeded:</p> </li> <li> <p>The callback will get a TimeoutException.</p> </li> <li>This means: \u201cWe didn\u2019t even manage to send this record in time.\u201d</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#4-why-the-difference","title":"4. Why the difference?","text":"<ul> <li> <p>Kafka wants to give you useful error context:</p> </li> <li> <p>If retries happened \u2192 the callback returns the broker error that caused retries.</p> </li> <li>If no send happened \u2192 the callback just returns a generic timeout.</li> </ul> <p>That way, you know whether the problem was:</p> <ul> <li>\u201cBroker responded with errors but we couldn\u2019t fix it in time\u201d (Case 1).</li> <li>\u201cWe never even got a chance to send\u201d (Case 2).</li> </ul> <p>\u2705 In short:</p> <ul> <li><code>delivery.timeout.ms</code> = max time a record is allowed to live (send + retry + wait).</li> <li>If timeout hits after retries \u2192 callback gets the last broker error.</li> <li>If timeout hits while waiting to be sent \u2192 callback gets a TimeoutException.</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#deliverytimeoutms-vs-maxblockms","title":"<code>delivery.timeout.ms</code> vs <code>max.block.ms</code>","text":""},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#1-maxblockms","title":"1. <code>max.block.ms</code>","text":"<ul> <li>Applies before the record is handed over to the producer\u2019s buffer.</li> <li> <p>Specifically:</p> </li> <li> <p>If the buffer is full (<code>buffer.memory</code> exhausted) or metadata (like partition leader info) is not available, the <code>send()</code> call can block.</p> </li> <li>It will block at most for <code>max.block.ms</code> (default = 60,000 ms).</li> <li>If that time passes \u2192 <code>send()</code> throws a <code>TimeoutException</code> immediately, before the record is accepted.</li> <li>So this timeout is about: \u201cCan I enqueue this record into the producer at all?\u201d</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#2-deliverytimeoutms","title":"2. <code>delivery.timeout.ms</code>","text":"<ul> <li>Applies after the record has been accepted into the producer buffer.</li> <li>It measures the total time a record is allowed to live (waiting in buffer + batching + retries + network sends) before either success or failure.</li> <li> <p>If exceeded \u2192 the callback is triggered with either:</p> </li> <li> <p>The last broker error (if retries happened), or</p> </li> <li>A TimeoutException (if the record never got sent).</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#3-do-they-overlap","title":"3. Do they overlap?","text":"<p>No \u2014 they are separate:</p> <ul> <li>If the record cannot even enter the buffer (blocked by memory/metadata) \u2192 <code>max.block.ms</code> applies. In this case, <code>delivery.timeout.ms</code> never starts, because the record was never enqueued.</li> <li>Once the record is successfully handed to the producer \u2192 <code>delivery.timeout.ms</code> applies until final success/failure.</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#4-analogy","title":"4. Analogy","text":"<ul> <li><code>max.block.ms</code> = waiting in line to enter the store. If you wait too long, you give up before shopping.</li> <li><code>delivery.timeout.ms</code> = once you\u2019re inside, you have a maximum time to shop and check out. If you exceed it, your shopping trip fails.</li> </ul> <p>\u2705 Answer to your question: No, <code>delivery.timeout.ms</code> does not include <code>max.block.ms</code>. They apply at different stages:</p> <ul> <li><code>max.block.ms</code> = before record enters producer buffer.</li> <li><code>delivery.timeout.ms</code> = after record is accepted into the buffer, until acknowledgment.</li> </ul>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#requesttimeoutms","title":"<code>request.timeout.ms</code>","text":"<p>How long does each producer wait for reply from server before giving up.</p> <p>Does not include retries, time before sending and so on.</p> <p>If timeout reached without reply, then producer will complete with callback exception.</p>"},{"location":"streaming/kafka/11-Kafka_Configuring_Producers_Pt1/#retries-and-retriesbackoffms","title":"<code>retries</code> and <code>retries.backoff.ms</code>","text":"<p>By default, producer waits 100 ms betweenr retries.</p> <p>Instead of setting this parameter, we need to check how long it takes for Kafka cluster to recover from crash (how long until all producers get leaders) and set this time to something greater than it.</p>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/","title":"Kafka Configuring Producers Pt2","text":"<p>linger.ms</p> <p>Let\u2019s go step by step \u2014 this is about how Kafka decides when to send a batch of messages from the producer to the broker.</p>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#1-what-batching-means","title":"1. What batching means","text":"<ul> <li>The Kafka producer doesn\u2019t send each record immediately over the network.</li> <li>Instead, it groups multiple records into a batch (per partition).</li> <li>Batching reduces network overhead because fewer requests are sent.</li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#2-how-does-the-producer-decide-when-to-send-a-batch","title":"2. How does the producer decide when to send a batch?","text":"<p>Two main triggers:</p> <ol> <li> <p>Batch is full \u2192 defined by <code>batch.size</code> (in bytes).</p> </li> <li> <p>As soon as the buffer for a partition is full, it is sent immediately.</p> </li> <li> <p>Time is up \u2192 controlled by <code>linger.ms</code>.</p> </li> <li> <p>If the batch isn\u2019t full, the producer can wait a little time to collect more messages.</p> </li> <li>Once <code>linger.ms</code> expires, the batch is sent, even if it isn\u2019t full.</li> </ol>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#3-default-behavior-lingerms-0","title":"3. Default behavior (<code>linger.ms = 0</code>)","text":"<ul> <li>By default, the producer does not wait.</li> <li>As soon as a sender thread is available, the record (even if it\u2019s the only one in the batch) is sent.</li> <li>This gives low latency, but poor throughput (lots of small network requests).</li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#4-effect-of-setting-lingerms-0","title":"4. Effect of setting <code>linger.ms &gt; 0</code>","text":"<ul> <li>The producer waits for up to that many milliseconds before sending a batch.</li> <li>This gives more time for additional records to accumulate in the batch.</li> <li> <p>Benefits:</p> </li> <li> <p>Better throughput: more messages per request, fewer network calls.</p> </li> <li>Better compression: larger batches compress more effectively.</li> <li> <p>Trade-off:</p> </li> <li> <p>Increased latency: each record may sit in memory slightly longer before being sent.</p> </li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#5-example","title":"5. Example","text":"<p>Suppose:</p> <ul> <li><code>batch.size = 16 KB</code></li> <li><code>linger.ms = 5</code></li> </ul> <p>Scenario:</p> <ul> <li>A record arrives, but the batch is not full.</li> <li>Instead of sending right away, the producer waits up to 5 ms for more records.</li> <li>If more records arrive, they are added to the batch.</li> <li>After 5 ms (or earlier, if the batch fills up), the batch is sent.</li> </ul> <ul> <li><code>linger.ms = 0</code> \u2192 send as soon as possible (low latency, low throughput).</li> <li><code>linger.ms &gt; 0</code> \u2192 wait a little before sending (higher latency, but much better throughput and compression).</li> </ul> <p>buffer.memory</p> <p>Amount of memory producer will use to buffer messages waiting to be sent to broker.</p> <p>If messages are sent by application faster than broker responds, additional send calls will be blocked for max.block.ms and wait for space to be freed up.</p> <p>This timeout is thrown by <code>send</code> and not by Future callback function.</p> <p>compression.type</p> <p>Let\u2019s break this down clearly \u2014 this is about the <code>compression.type</code> setting in the Kafka producer.</p>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#1-default-behavior","title":"1. Default behavior","text":"<ul> <li>By default, Kafka producer sends messages uncompressed.</li> <li>That means every record is sent as raw bytes, which consumes more network bandwidth and broker storage.</li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#2-what-compressiontype-does","title":"2. What <code>compression.type</code> does","text":"<ul> <li> <p>You can set <code>compression.type</code> in producer configs to one of:</p> </li> <li> <p><code>none</code> (default)</p> </li> <li><code>snappy</code></li> <li><code>gzip</code></li> <li><code>lz4</code></li> <li> <p><code>zstd</code></p> </li> <li> <p>The producer will then compress message batches before sending them to brokers.</p> </li> <li> <p>The broker stores the compressed form, and consumers can automatically decompress when reading (as long as they use a compatible client).</p> </li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#3-why-use-compression","title":"3. Why use compression?","text":"<ul> <li>Network utilization: fewer bytes to send across the wire.</li> <li>Broker storage: compressed messages take up less disk space.</li> <li>Throughput: often improves, because network is a common bottleneck.</li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#4-trade-offs-of-different-algorithms","title":"4. Trade-offs of different algorithms","text":"<ul> <li> <p>Snappy (by Google):</p> </li> <li> <p>Good balance between compression ratio and speed.</p> </li> <li>Low CPU cost, fast to compress/decompress.</li> <li> <p>Recommended when you care about performance + reducing bandwidth.</p> </li> <li> <p>Gzip:</p> </li> <li> <p>Slower, higher CPU overhead.</p> </li> <li>But achieves better compression ratios than Snappy.</li> <li> <p>Recommended if network bandwidth is the bottleneck and you can afford extra CPU cost.</p> </li> <li> <p>LZ4:</p> </li> <li> <p>Very fast, better compression ratio than Snappy in some cases.</p> </li> <li> <p>Good when you need high throughput and low latency.</p> </li> <li> <p>Zstd (newer, from Facebook/Meta):</p> </li> <li> <p>Offers a tunable trade-off between compression ratio and speed.</p> </li> <li>Usually provides better compression ratios than gzip at similar or better speed.</li> <li>Useful in modern Kafka clusters where efficiency matters.</li> </ul>"},{"location":"streaming/kafka/12-Kafka_Configuring_Producers_Pt2/#5-how-it-works-with-batching","title":"5. How it works with batching","text":"<ul> <li>Compression is applied per batch, not per individual message.</li> <li>This means if you use <code>linger.ms</code> and <code>batch.size</code> to allow larger batches, compression gets more effective.</li> <li>Example: a batch of 100 messages compressed together \u2192 far better ratio than compressing each one individually.</li> </ul> <p>In short:</p> <ul> <li><code>compression.type</code> controls whether and how messages are compressed.</li> <li>Snappy = fast, decent compression (good default for performance + bandwidth).</li> <li>Gzip = slower, best compression ratio (good for bandwidth-limited environments).</li> <li>LZ4 = very fast, efficient for high throughput.</li> <li>Zstd = modern, tunable, often best of both worlds.</li> <li>Compression reduces network and storage usage, which are often bottlenecks in Kafka.</li> </ul> <p>batch.size</p> <p>When multiple records are sent to same partition, the producer will bundle them together.</p> <p>When the batch is full, all messages in batch will be sent. However producer will not wait for entire batch to fill up. It can even send half filled queue or even one message.</p> <p>Setting batch size to be too large will not cause memory delays it would just mean each batch will use more memory.</p> <p>If we set it too small, then producer will need to send messages more frequently and causes overhead.</p> <p>max.in.flight.requests.per.connection</p> <p>This controls how many message batches the producer will send to the server without receiving a response. Higher settings can increase memory overhead in buffer bt improves throughput.</p> <p>Default is 5.</p> <p></p> <p>max.request.size</p> <p>Caps the size of the largest message that can be sent in one request. If its 1MB largest message we can send is 1MB or 1024 messages of 1Kb each.</p> <p>Broker also has a limit on largest message it can receive. its set using max.message.bytes.</p> <p>receive.buffer.bytes and send.buffer.bytes</p> <p>These are sizes of send and receive buffers used by sockets when reading and writing data. If its -1 then OS defaults are used.</p> <p>Its a good idea to increase this when Producers and Consumers communicate with brokers in different data centers.</p> <p>There's a separate chapter on idempotence but these are the conditions to be satisfied.</p> <ul> <li><code>max.in.flight.requests.per.connection</code> to be less than or equal to 5, retries &gt; 0 and acks = all. Exception will be thrown if not satisfied.</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/","title":"Kafka Serializers Avro Pt1","text":""},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#serializing-with-apache-avro","title":"Serializing with Apache Avro","text":""},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-is-avro","title":"\ud83e\udde9 What is Avro?","text":"<p>Apache Avro is a data serialization system \u2014 meaning it defines how data is structured, stored, and transmitted between systems in a compact and efficient way.</p> <p>It was created by Doug Cutting (also the creator of Hadoop and Lucene) to solve a common problem in distributed systems:</p> <p>\u201cHow do you exchange complex structured data between systems written in different languages \u2014 without losing type information or causing compatibility issues?\u201d</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#how-avro-works","title":"\u2699\ufe0f How Avro Works","text":"<p>At its core, Avro works with two components:</p> <ol> <li>Schema \u2013 Defines the structure (fields, types, defaults) of your data.</li> <li>Data \u2013 The actual binary or JSON-encoded data following that schema.</li> </ol> <p>So Avro separates \u201cwhat the data looks like\u201d (schema) from \u201cwhat the data is\u201d (values).</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#avro-schema-example-in-json","title":"\ud83d\udcc4 Avro Schema Example (in JSON)","text":"<p>Here\u2019s how a simple Avro schema looks:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"Employee\",\n  \"namespace\": \"com.company\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"department\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n</code></pre> <p>This defines a record with three fields:</p> <ul> <li><code>id</code> (integer)</li> <li><code>name</code> (string)</li> <li><code>department</code> (optional string \u2014 can be null)</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#serialization-and-deserialization","title":"\ud83d\udcbe Serialization and Deserialization","text":"<ul> <li>Serialization: Converting data (e.g., a Python object) into Avro binary using the schema.</li> <li>Deserialization: Reading Avro binary data back into a usable form using the same or compatible schema.</li> </ul> <p>Because the schema is embedded in Avro files, they are self-describing \u2014 any system can read them if it supports Avro.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#why-avro-is-special","title":"\ud83e\udde0 Why Avro Is Special","text":"<p>Here are Avro\u2019s key advantages:</p> Feature Description \ud83d\uddc2\ufe0f Schema-based Data has a well-defined structure stored in JSON format. \u26a1 Compact Binary Format Binary encoding reduces file size and improves I/O performance. \ud83d\udd04 Schema Evolution You can change schemas over time (add, rename, remove fields) without breaking existing consumers. \ud83c\udf10 Language Neutral Works with many languages (Java, Python, C++, Go, etc.). \ud83d\udcac Great for Messaging Systems Used in Kafka, Redpanda, and Schema Registry setups. \ud83e\uddf1 Splittable and Compressible Ideal for big data systems (Hadoop, Spark, Hive)."},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#schema-evolution-in-avro","title":"\ud83d\udd04 Schema Evolution in Avro","text":"<p>This is the most powerful part \u2014 and the reason Avro is heavily used in Kafka.</p> <p>Suppose your producer\u2019s old schema was:</p> <pre><code>{\"name\": \"id\", \"type\": \"int\"}\n</code></pre> <p>Now you update it to:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"Employee\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n</code></pre> <p>\u2705 Avro allows backward and forward compatibility:</p> <ul> <li>Backward-compatible: New consumers can read old data.</li> <li>Forward-compatible: Old consumers can read new data (using defaults for new fields).</li> </ul> <p>That\u2019s why Kafka uses Avro with a Schema Registry \u2014 to ensure producers and consumers can evolve independently.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#where-avro-is-commonly-used","title":"\ud83e\uddf0 Where Avro is Commonly Used","text":"Use Case Description \ud83e\udea3 Data Lakes (HDFS, S3) Store schema-defined data for Spark/Hive. \ud83e\uddf5 Kafka Messaging Producers publish Avro messages, Schema Registry keeps schema versions. \ud83e\uddec ETL Pipelines Efficient and schema-safe data transfer between stages. \ud83e\uddee Analytics Compact binary format makes large datasets efficient to query."},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#avro-vs-other-formats","title":"\ud83d\udcca Avro vs. Other Formats","text":"Feature Avro JSON Parquet ORC Storage Type Row-based Text Columnar Columnar Schema Yes (JSON-defined) No Yes Yes Compression Yes Poor Excellent Excellent Best For Streaming, Kafka Debugging, APIs Analytics, OLAP Analytics, OLAP Human Readable No Yes No No <p>So Avro is ideal for data interchange (Kafka, APIs, log pipelines), while Parquet/ORC are better for data storage and querying.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#summary","title":"\ud83e\udde9 Summary","text":"<p>Apache Avro = Schema + Compact Binary + Evolution Support It\u2019s designed for:</p> <ul> <li>Cross-language data exchange</li> <li>Streaming and message serialization</li> <li>Schema evolution without breaking systems</li> </ul> <p>And that\u2019s why it\u2019s a favorite in modern data pipelines, especially with Kafka, Redpanda, Spark, and Schema Registry.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#caveats-while-using-avro","title":"Caveats While Using Avro","text":""},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#caveat-1","title":"\ud83e\udde9 Caveat 1:","text":"<p>\u201cThe schema used for writing the data and the schema expected by the reading application must be compatible.\u201d</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-this-means","title":"\ud83d\udd0d What this means","text":"<p>When Avro serializes (writes) and deserializes (reads) data, both sides must agree on the structure of the data \u2014 at least enough to interpret the fields correctly.</p> <p>If the producer (writer) and consumer (reader) use incompatible schemas, the data can\u2019t be decoded properly.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#example","title":"\ud83e\udde0 Example","text":""},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#writers-schema-v1","title":"Writer\u2019s Schema (v1)","text":"<pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"}\n  ]\n}\n</code></pre>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#readers-schema-v2","title":"Reader\u2019s Schema (v2)","text":"<pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"full_name\", \"type\": \"string\"}\n  ]\n}\n</code></pre> <p>Here, the field name changed from <code>\"name\"</code> \u2192 <code>\"full_name\"</code>.</p> <p>\ud83d\udca5 Result: Not compatible \u2014 Avro won\u2019t know how to map <code>\"name\"</code> to <code>\"full_name\"</code>. The reader will fail to interpret the data.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#compatible-evolution-example","title":"\u2705 Compatible Evolution Example","text":"<p>If we add a new optional field, Avro can handle that gracefully.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#writers-schema-old","title":"Writer\u2019s Schema (old)","text":"<pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"}\n  ]\n}\n</code></pre>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#readers-schema-new","title":"Reader\u2019s Schema (new)","text":"<pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n</code></pre> <p>\ud83d\udfe2 Compatible \u2014 The new field <code>email</code> has a default value, so old data still works fine.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#compatibility-rules-simplified","title":"\u2696\ufe0f Compatibility Rules (Simplified)","text":"<p>Avro defines several compatibility types:</p> Compatibility Type Meaning Backward New readers can read old data. Forward Old readers can read new data. Full Both backward + forward. Breaking Schema changes that break both directions (e.g. removing required fields). <p>You can configure which rule to enforce in systems like the Confluent Schema Registry.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#caveat-2","title":"\ud83e\udde9 Caveat 2:","text":"<p>\u201cThe deserializer will need access to the schema that was used when writing the data, even when it is different from the schema expected by the application that accesses the data.\u201d</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-this-means_1","title":"\ud83d\udd0d What this means","text":"<p>To read Avro data, you always need:</p> <ul> <li>The writer\u2019s schema (used when data was created)</li> <li>The reader\u2019s schema (used by your application)</li> </ul> <p>Avro uses both together to resolve differences.</p> <p>If your application only knows its own schema (reader schema) but not the original one, it can\u2019t interpret the binary data \u2014 because Avro binary doesn\u2019t include field names, just encoded values.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#analogy","title":"\ud83e\udde0 Analogy","text":"<p>Think of Avro data as a compressed shopping list:</p> <ul> <li>The writer\u2019s schema says the order of items:   1\ufe0f\u20e3 \"Milk\" \u2192 string   2\ufe0f\u20e3 \"Eggs\" \u2192 int</li> <li>The data only stores the values: <code>\"Amul\", 12</code></li> </ul> <p>If the reader doesn\u2019t know the original order (schema), it can\u2019t tell which value corresponds to which field.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#caveat-3","title":"\ud83e\udde9 Caveat 3:","text":"<p>\u201cIn Avro files, the writing schema is included in the file itself, but there is a better way to handle this for Kafka messages.\u201d</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-this-means_2","title":"\ud83d\udd0d What this means","text":"<p>When Avro is used for files (like on HDFS or S3), the schema is embedded inside the file header. That\u2019s fine for static data \u2014 every file carries its own schema.</p> <p>But in Kafka, embedding the full schema inside every message would be inefficient:</p> <ul> <li>Each message might be just a few KB,</li> <li>But the schema (JSON) could be several hundred bytes.</li> </ul> <p>That would cause massive duplication and network overhead.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#the-better-way-hinted-at-in-the-text","title":"\ud83e\udde0 The \u201cBetter Way\u201d (Hinted at in the text)","text":"<p>Instead of embedding schemas in every Kafka message, systems like Confluent Schema Registry store schemas centrally.</p> <p>Here\u2019s how it works:</p> <ol> <li> <p>When a producer sends a message:</p> </li> <li> <p>It registers its schema once with the Schema Registry.</p> </li> <li>It gets a schema ID (like <code>42</code>).</li> <li> <p>It sends the binary Avro data prefixed with that schema ID.</p> </li> <li> <p>When a consumer reads a message:</p> </li> <li> <p>It looks up schema ID <code>42</code> in the Schema Registry.</p> </li> <li>It retrieves the writer\u2019s schema and deserializes the message correctly.</li> </ol> <p>This way:</p> <ul> <li>\u2705 Small message sizes</li> <li>\u2705 Centralized schema management</li> <li>\u2705 Schema evolution tracking</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#summary-table","title":"\ud83c\udfd7\ufe0f Summary Table","text":"Concept In Avro Files In Kafka Messages Where is schema stored? Embedded inside file Stored in Schema Registry Message overhead Higher (includes schema) Lower (schema ID only) Schema evolution File-based Centrally managed (versioned) Typical use case Batch systems (HDFS, S3, Hive) Streaming systems (Kafka, Redpanda)"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#in-short","title":"\ud83e\udde9 In Short","text":"Caveat Meaning Solution 1\ufe0f\u20e3 Writer/Reader schema must be compatible Data won\u2019t deserialize if incompatible Follow Avro compatibility rules 2\ufe0f\u20e3 Reader needs writer\u2019s schema Required to decode binary data Include schema in file or fetch from registry 3\ufe0f\u20e3 Don\u2019t embed schema per message in Kafka Wasteful and redundant Use Schema Registry (stores schema IDs)"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#demo-producer-code-with-schema-registry-in-python","title":"Demo Producer Code with Schema Registry in Python","text":"<pre><code>from confluent_kafka.avro import AvroProducer\nimport json\n\n# Avro schema as JSON string or loaded from file\nuser_schema_str = \"\"\"\n{\n  \"namespace\": \"example.avro\",\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n\"\"\"\n\n# Kafka + Schema Registry configuration (example placeholders)\nconf = {\n    'bootstrap.servers': 'kafka-broker1:9092,kafka-broker2:9092',\n    'schema.registry.url': 'http://schema-registry-host:8081',\n    # If SR needs auth, add 'schema.registry.basic.auth.user.info': 'user:password'\n}\n\n# The AvroProducer takes the schema used for the value (and optionally key schema)\nproducer = AvroProducer(conf, default_value_schema=json.loads(user_schema_str))\n\ntopic = \"users-avro\"\n\ndef produce_user(user_obj):\n    # key_schema can also be provided; here we use string keys\n    key = str(user_obj[\"id\"])\n    producer.produce(topic=topic, value=user_obj, key=key)\n    producer.flush()   # flush per message for demo; in production batch/async flush\n\nif __name__ == \"__main__\":\n    user = {\"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\"}\n    produce_user(user)\n    print(\"Sent Avro message to\", topic)\n</code></pre>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#demo-producer-code-in-java","title":"Demo Producer Code In Java","text":"<pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"key.serializer\",\n\"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"value.serializer\",\n\"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"schema.registry.url\", schemaUrl);\nString topic = \"customerContacts\";\nProducer&lt;String, Customer&gt; producer = new KafkaProducer&lt;&gt;(props);\n// We keep producing new events until someone ctrl-c\nwhile (true) {\nCustomer customer = CustomerGenerator.getNext();\nSystem.out.println(\"Generated customer \" +\ncustomer.toString());\nProducerRecord&lt;String, Customer&gt; record =\nnew ProducerRecord&lt;&gt;(topic, customer.getName(), customer);\nproducer.send(record);\n}\n</code></pre>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#the-big-picture","title":"\ud83e\udde9 The Big Picture","text":"<p>This Java code is an Avro Kafka producer. It:</p> <ol> <li>Connects to Kafka.</li> <li>Uses Confluent\u2019s Avro serializers.</li> <li>Generates Avro objects (<code>Customer</code> records).</li> <li>Sends them continuously to a Kafka topic.</li> </ol>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#step-by-step-explanation","title":"\ud83e\uddf1 Step-by-step Explanation","text":""},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#1-create-configuration-properties","title":"1\ufe0f\u20e3 Create configuration properties","text":"<pre><code>Properties props = new Properties();\n</code></pre> <p>\ud83d\udc49 <code>Properties</code> is a Java <code>Map</code>-like object that stores key-value configuration settings for the Kafka producer.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#2-configure-kafka-connection-and-serialization","title":"2\ufe0f\u20e3 Configure Kafka connection and serialization","text":"<pre><code>props.put(\"bootstrap.servers\", \"localhost:9092\");\n</code></pre> <p>\ud83d\udd39 <code>bootstrap.servers</code> \u2014 tells the producer where Kafka brokers are located. The producer will connect to this address to find the rest of the cluster.</p> <pre><code>props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n</code></pre> <p>\ud83d\udd39 These lines configure how keys and values are converted into bytes before being sent over the network.</p> <ul> <li>Both the key and value use the Confluent Avro serializer.</li> <li> <p>The serializer:</p> </li> <li> <p>Converts Avro objects (like <code>Customer</code>) into Avro binary format.</p> </li> <li>Registers the Avro schema with the Schema Registry.</li> <li>Sends the schema ID (a small integer) along with the serialized message.</li> </ul> <p>So the consumer can later retrieve the schema and deserialize properly.</p> <pre><code>props.put(\"schema.registry.url\", schemaUrl);\n</code></pre> <p>\ud83d\udd39 This tells the serializer where the Schema Registry is (e.g., <code>http://localhost:8081</code>).</p> <p>The serializer uses this URL to:</p> <ul> <li>Check if the schema for <code>Customer</code> already exists in the registry.</li> <li>Register it if not.</li> <li>Retrieve schema IDs for encoding messages.</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#3-create-kafka-topic-variable","title":"3\ufe0f\u20e3 Create Kafka topic variable","text":"<pre><code>String topic = \"customerContacts\";\n</code></pre> <p>Just defines the target Kafka topic to send messages to.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#4-create-the-producer-instance","title":"4\ufe0f\u20e3 Create the producer instance","text":"<pre><code>Producer&lt;String, Customer&gt; producer = new KafkaProducer&lt;&gt;(props);\n</code></pre> <p>\ud83d\udd39 This creates the Kafka producer client.</p> <ul> <li> <p><code>Producer&lt;K, V&gt;</code> \u2014 a generic class parameterized by key and value types.   Here:</p> </li> <li> <p><code>K</code> = <code>String</code> (customer name, used as key)</p> </li> <li><code>V</code> = <code>Customer</code> (the Avro object)</li> </ul> <p>The producer will use the serializers defined earlier to encode these before sending.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#5-generate-and-send-avro-messages-in-a-loop","title":"5\ufe0f\u20e3 Generate and send Avro messages in a loop","text":"<pre><code>while (true) {\n    Customer customer = CustomerGenerator.getNext();\n    System.out.println(\"Generated customer \" + customer.toString());\n    ProducerRecord&lt;String, Customer&gt; record =\n        new ProducerRecord&lt;&gt;(topic, customer.getName(), customer);\n    producer.send(record);\n}\n</code></pre> <p>Let\u2019s break this down line by line.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#customer-customer-customergeneratorgetnext","title":"\ud83e\udde0 <code>Customer customer = CustomerGenerator.getNext();</code>","text":"<p>This uses a helper class <code>CustomerGenerator</code> (not shown here) that probably:</p> <ul> <li>Creates random or mock customer data.</li> <li>Returns a <code>Customer</code> object (which is an Avro-generated Java class).</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#systemoutprintln","title":"\ud83d\udda8\ufe0f <code>System.out.println(...)</code>","text":"<p>Just logs the generated customer to the console for visibility.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#producerrecordstring-customer-record-new-producerrecordtopic-customergetname-customer","title":"\ud83e\uddfe <code>ProducerRecord&lt;String, Customer&gt; record = new ProducerRecord&lt;&gt;(topic, customer.getName(), customer);</code>","text":"<p>This creates the message record to be sent to Kafka.</p> <ul> <li>The topic: <code>\"customerContacts\"</code>.</li> <li>The key: <code>customer.getName()</code>.</li> <li>The value: the entire <code>Customer</code> Avro object.</li> </ul> <p>Kafka uses the key to determine which partition the message goes to (same keys \u2192 same partition).</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#producersendrecord","title":"\ud83d\ude80 <code>producer.send(record);</code>","text":"<p>This sends the record asynchronously to Kafka.</p> <p>The Avro serializer will:</p> <ol> <li>Serialize <code>Customer</code> into binary Avro.</li> <li>Register or look up the schema in Schema Registry.</li> <li>Encode the schema ID + binary payload.</li> <li>Send the message to the Kafka broker.</li> </ol>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#while-true","title":"\ud83c\udf00 <code>while (true) {...}</code>","text":"<p>Runs indefinitely \u2014 keeps producing customer messages until you stop it with Ctrl + C.</p> <p>In a real-world app, you might:</p> <ul> <li>Add a <code>Thread.sleep()</code> for pacing,</li> <li>Limit message count, or</li> <li>Gracefully close the producer with <code>producer.close()</code>.</li> </ul>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-the-customer-class-is","title":"\ud83e\udde9 What the <code>Customer</code> class is","text":"<p>This class is likely generated automatically from an Avro schema using the Avro Java code generator.</p> <p>For example, the Avro schema could look like:</p> <pre><code>{\n  \"namespace\": \"example.avro\",\n  \"type\": \"record\",\n  \"name\": \"Customer\",\n  \"fields\": [\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": \"string\"},\n    {\"name\": \"phoneNumber\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n</code></pre> <p>After generating Java classes from this schema (<code>avro-maven-plugin</code> or <code>avro-tools</code>), you can use the <code>Customer</code> object directly in code \u2014 the Avro serializer knows how to handle it.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#summary-table_1","title":"\ud83e\udde0 Summary Table","text":"Line What it does Why it matters <code>bootstrap.servers</code> Broker list Connect to Kafka cluster <code>key.serializer</code> / <code>value.serializer</code> Avro serializers Convert Java objects to bytes <code>schema.registry.url</code> Registry URL Store and retrieve schemas <code>KafkaProducer&lt;&gt;(props)</code> Creates producer Main client that talks to Kafka <code>CustomerGenerator.getNext()</code> Generates Avro object Produces mock data <code>new ProducerRecord&lt;&gt;(...)</code> Wraps message Defines topic, key, and value <code>producer.send(record)</code> Sends async Pushes data to Kafka <code>while (true)</code> Infinite loop Keeps producing"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#what-happens-behind-the-scenes","title":"\u2699\ufe0f What Happens Behind the Scenes","text":"<ol> <li>You produce a <code>Customer</code> Avro object.</li> <li>Serializer looks up (or registers) its schema in Schema Registry.</li> <li>Schema Registry returns a unique schema ID.</li> <li>Producer encodes:</li> </ol> <p><pre><code>[Magic Byte 0][Schema ID (4 bytes)][Avro serialized data]\n</code></pre> 5. Kafka stores the message. 6. Consumer later reads the message \u2192 uses Schema ID \u2192 fetches schema \u2192 deserializes back to a <code>Customer</code> object.</p>"},{"location":"streaming/kafka/13-Kafka_Serializers_Avro_Pt1/#in-simple-terms","title":"\ud83e\udde9 In Simple Terms","text":"<p>This Java code:</p> <p>Continuously generates random customer data, serializes each record in Avro format using Confluent Schema Registry, and sends it to a Kafka topic named <code>customerContacts</code>.</p> <p>Customer Class is not a regular Plain Old Java Object but rather specialized one generated from a schema. We can generate these classes using avro-tools.jar or Maven plugin.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/","title":"Kafka Serializers Avro Pt2","text":""},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#serializing-and-sending-data-in-avro-without-classes","title":"Serializing and Sending Data in Avro without Classes","text":"<pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"key.serializer\",\n\"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"value.serializer\",\n\"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"schema.registry.url\", url);\nString schemaString =\n\"{\\\"namespace\\\": \\\"customerManagement.avro\\\",\n\"\\\"type\\\": \\\"record\\\", \" +\n\"\\\"name\\\": \\\"Customer\\\",\" +\n\"\\\"fields\\\": [\" +\n\"{\\\"name\\\": \\\"id\\\", \\\"type\\\": \\\"int\\\"},\" +\n\"{\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"},\" +\n\"{\\\"name\\\": \\\"email\\\", \\\"type\\\": \" + \"[\\\"null\\\",\\\"string\\\"], \" +\n\"\\\"default\\\":\\\"null\\\" }\" +\n\"]}\";\nProducer&lt;String, GenericRecord&gt; producer =\nnew KafkaProducer&lt;String, GenericRecord&gt;(props);\nSchema.Parser parser = new Schema.Parser();\nSchema schema = parser.parse(schemaString);\nfor (int nCustomers = 0; nCustomers &lt; customers; nCustomers++) {\nString name = \"exampleCustomer\" + nCustomers;\nString email = \"example \" + nCustomers + \"@example.com\";\nGenericRecord customer = new GenericData.Record(schema);\ncustomer.put(\"id\", nCustomers);\ncustomer.put(\"name\", name);\ncustomer.put(\"email\", email);\nProducerRecord&lt;String, GenericRecord&gt; data =\nnew ProducerRecord&lt;&gt;(\"customerContacts\", name, customer);\nproducer.send(data);\n}\n</code></pre> <p>Perfect \u2014 this is another Kafka producer example that uses Avro serialization, but this time without pre-generated Avro classes.</p> <p>Let\u2019s unpack it line-by-line so you can see exactly what\u2019s happening and how it differs from the previous one that used the generated <code>Customer</code> class. \ud83d\udc47</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#1-overview-what-this-code-does","title":"\ud83e\udde9 1\ufe0f\u20e3 Overview \u2014 What This Code Does","text":"<p>This program:</p> <ol> <li>Defines an Avro schema as a JSON string (inline in the code).</li> <li>Uses Confluent\u2019s Avro serializer to send Avro-encoded messages to Kafka.</li> <li>Creates <code>GenericRecord</code> objects (Avro records that use the schema at runtime, instead of pre-generated classes).</li> <li>Sends multiple Avro messages to a Kafka topic called <code>\"customerContacts\"</code>.</li> </ol>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#2-step-by-step-explanation","title":"\ud83e\uddf1 2\ufe0f\u20e3 Step-by-Step Explanation","text":""},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#create-configuration-properties","title":"\ud83e\uddfe Create configuration properties","text":"<pre><code>Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\n</code></pre> <p>\ud83d\udd39 <code>Properties</code> holds the producer configuration. \ud83d\udd39 <code>bootstrap.servers</code> = address of your Kafka broker(s). It\u2019s where the producer initially connects to the Kafka cluster.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#configure-avro-serialization-and-schema-registry","title":"\ud83e\uddf0 Configure Avro serialization and schema registry","text":"<pre><code>props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\nprops.put(\"schema.registry.url\", url);\n</code></pre> <p>\ud83d\udd39 Both the key and value use Confluent\u2019s Avro serializer. \ud83d\udd39 <code>schema.registry.url</code> tells Kafka where the Schema Registry is running (e.g., <code>http://localhost:8081</code>).</p> <p>When the producer sends data:</p> <ol> <li>The serializer registers or looks up the Avro schema in the Schema Registry.</li> <li>The registry assigns a schema ID.</li> <li>The serializer encodes the message as:</li> </ol> <pre><code>[Magic Byte][Schema ID][Avro Binary Payload]\n</code></pre>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#define-the-avro-schema-as-a-string","title":"\ud83e\uddee Define the Avro schema (as a string)","text":"<pre><code>String schemaString =\n\"{\\\"namespace\\\": \\\"customerManagement.avro\\\",\" +\n\"\\\"type\\\": \\\"record\\\", \" +\n\"\\\"name\\\": \\\"Customer\\\",\" +\n\"\\\"fields\\\": [\" +\n\"{\\\"name\\\": \\\"id\\\", \\\"type\\\": \\\"int\\\"},\" +\n\"{\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"},\" +\n\"{\\\"name\\\": \\\"email\\\", \\\"type\\\": [\\\"null\\\",\\\"string\\\"], \\\"default\\\":\\\"null\\\" }\" +\n\"]}\";\n</code></pre> <p>\ud83d\udd39 This JSON string defines the Avro schema for the <code>Customer</code> record:</p> <ul> <li>namespace: <code>customerManagement.avro</code></li> <li>type: <code>record</code> (structured data type)</li> <li> <p>fields:</p> </li> <li> <p><code>id</code> \u2192 int</p> </li> <li><code>name</code> \u2192 string</li> <li><code>email</code> \u2192 nullable string (<code>[\"null\",\"string\"]</code> with default <code>\"null\"</code>)</li> </ul> <p>This schema is not pre-compiled into a Java class \u2014 instead it will be parsed and used dynamically.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#create-the-kafka-producer","title":"\ud83e\uddf1 Create the Kafka producer","text":"<pre><code>Producer&lt;String, GenericRecord&gt; producer =\n    new KafkaProducer&lt;String, GenericRecord&gt;(props);\n</code></pre> <p>\ud83d\udd39 Creates a Kafka producer instance.</p> <ul> <li>Key type: <code>String</code></li> <li>Value type: <code>GenericRecord</code> (an Avro object that follows a schema but is built dynamically).</li> </ul>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#parse-the-schema-string-into-an-avro-schema-object","title":"\ud83e\udde0 Parse the schema string into an Avro Schema object","text":"<pre><code>Schema.Parser parser = new Schema.Parser();\nSchema schema = parser.parse(schemaString);\n</code></pre> <p>\ud83d\udd39 The Avro library\u2019s <code>Schema.Parser</code> reads the JSON string and turns it into a Schema object. \ud83d\udd39 This object describes the structure of each message we\u2019ll send.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#produce-messages-in-a-loop","title":"\ud83c\udfd7\ufe0f Produce messages in a loop","text":"<pre><code>for (int nCustomers = 0; nCustomers &lt; customers; nCustomers++) {\n    String name = \"exampleCustomer\" + nCustomers;\n    String email = \"example\" + nCustomers + \"@example.com\";\n</code></pre> <p>\ud83d\udd39 Generates sample data for multiple customers. Each message will have a unique name and email.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#create-a-genericrecord-avro-record-instance","title":"\ud83e\udde9 Create a GenericRecord (Avro record instance)","text":"<pre><code>GenericRecord customer = new GenericData.Record(schema);\ncustomer.put(\"id\", nCustomers);\ncustomer.put(\"name\", name);\ncustomer.put(\"email\", email);\n</code></pre> <p>\ud83d\udd39 <code>GenericData.Record(schema)</code> creates a new Avro record using the parsed schema. \ud83d\udd39 Each field\u2019s value is added with <code>put(fieldName, value)</code>.</p> <p>This is how you create an Avro object without generating a Java class.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#create-and-send-the-kafka-message","title":"\ud83d\udce8 Create and send the Kafka message","text":"<pre><code>ProducerRecord&lt;String, GenericRecord&gt; data =\n    new ProducerRecord&lt;&gt;(\"customerContacts\", name, customer);\nproducer.send(data);\n</code></pre> <p>\ud83d\udd39 <code>ProducerRecord</code> defines the Kafka message:</p> <ul> <li>Topic: <code>customerContacts</code></li> <li>Key: the customer\u2019s name (used for partitioning)</li> <li>Value: the Avro <code>GenericRecord</code></li> </ul> <p>\ud83d\udd39 <code>producer.send(data)</code> asynchronously sends it to Kafka. The serializer:</p> <ol> <li>Registers or retrieves the Avro schema from the Schema Registry.</li> <li>Encodes the record into binary Avro format.</li> <li>Sends the data to Kafka brokers.</li> </ol>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#3-whats-the-key-difference-vs-previous-example","title":"\u2699\ufe0f 3\ufe0f\u20e3 What\u2019s the Key Difference vs. Previous Example","text":"Concept Previous Example (<code>Customer</code> class) This Example (<code>GenericRecord</code>) Schema Handling Schema compiled ahead of time (code generation) Schema defined at runtime (JSON string) Object Type <code>SpecificRecord</code> (<code>Customer.java</code>) <code>GenericRecord</code> Code Generation Required (via <code>avro-tools</code> or Maven plugin) Not required Flexibility Fixed schema, better performance Dynamic schema, more flexible Serializer Used <code>KafkaAvroSerializer</code> <code>KafkaAvroSerializer</code> (same) <p>So both use the same serializer and Schema Registry, but one uses generated Avro classes, while this one uses runtime schema parsing with GenericRecord.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#4-summary-of-what-happens-internally","title":"\ud83e\udde9 4\ufe0f\u20e3 Summary of What Happens Internally","text":"<ol> <li>Schema is parsed from JSON \u2192 Avro Schema object.</li> <li>GenericRecord is created based on that schema and filled with values.</li> <li> <p>KafkaAvroSerializer:</p> </li> <li> <p>Registers the schema in Schema Registry (if new).</p> </li> <li>Gets a schema ID.</li> <li>Serializes the record as <code>[Magic Byte][Schema ID][Binary Avro Data]</code>.</li> <li>Kafka producer sends the message to the <code>customerContacts</code> topic.</li> </ol> <p>Consumers with access to the same Schema Registry can then deserialize this data automatically back into Avro or POJO form.</p>"},{"location":"streaming/kafka/14-Kafka_Serializers_Avro_Pt2/#in-short","title":"\ud83e\udde0 In Short","text":"<p>This code shows how to produce Avro messages to Kafka without generating Java classes. It defines an Avro schema at runtime, creates <code>GenericRecord</code> objects that follow it, and sends them through Kafka using the Confluent Avro serializer and Schema Registry.</p>"},{"location":"streaming/kafka/15-Kafka_Partitions/","title":"Kafka Partitions","text":""},{"location":"streaming/kafka/15-Kafka_Partitions/#partitions-in-kafka","title":"Partitions in Kafka","text":"<p>When a message is produced to a Kafka topic, it can include an optional key. The key serves two main purposes:</p> <ol> <li> <p>Metadata or identification \u2014 It\u2019s stored along with the message and can represent something meaningful, such as a user ID, customer ID, or product code. This helps group related messages together.</p> </li> <li> <p>Partitioning logic \u2014 Kafka uses the key to decide which partition within a topic the message will go to.</p> </li> <li> <p>If messages have the same key, Kafka ensures they are always sent to the same partition.</p> </li> <li>This guarantees that all messages related to a particular key are processed in order, since order is preserved within a partition.</li> </ol> <p>This also affects how messages are consumed. If a consumer group has multiple consumers, and each consumer reads from a subset of partitions, then:</p> <ul> <li>All messages for a particular key will always be read by the same consumer (because they all reside in the same partition).</li> </ul> <p>Keys help maintain data locality (same key - same partition) and processing consistency (same consumer processes related messages).</p>"},{"location":"streaming/kafka/15-Kafka_Partitions/#in-build-kafka-partitioning-algorithms","title":"In Build Kafka Partitioning Algorithms","text":""},{"location":"streaming/kafka/15-Kafka_Partitions/#in-depth-explanation-how-kafka-picks-a-partition-null-key-vs-keyed-records","title":"In-depth explanation \u2014 how Kafka picks a partition (null key vs keyed records)","text":"<p>Below I walk through exactly what the producer does in each case, why Kafka switched from plain round-robin to the sticky approach, and the practical consequences (including the \u201cpartition unavailable\u201d risk you mentioned).</p>"},{"location":"streaming/kafka/15-Kafka_Partitions/#short-summary","title":"Short summary","text":"<ul> <li>No key (key == null): the producer chooses a partition for you. Before Kafka 2.4 this was a simple round-robin; starting in 2.4 the default partitioner uses a sticky strategy: pick a random partition and keep sending (i.e., stick) records to that partition until the current batch is completed, then switch. This increases batch sizes and reduces requests.</li> <li>Key present: the producer hashes the key and uses that hash to pick a partition deterministically (so the same key always maps to the same partition). Kafka\u2019s Java client uses Murmur2 for this hashing and computes <code>partition = positive(murmur2(keyBytes)) % numPartitions</code>. Importantly that modulo uses the total number of partitions for the topic (not only the currently available partitions).</li> </ul>"},{"location":"streaming/kafka/15-Kafka_Partitions/#when-the-key-is-null-technical-detail-and-why-sticky-helps","title":"When the key is null \u2014 technical detail and why \u201csticky\u201d helps","text":"<ol> <li>Old behavior (pre-2.4): the partitioner spread null-key records roughly evenly via round-robin across the available partitions. That produced small batches on many partitions because each successive message went to a different partition.</li> <li>Sticky partitioner (2.4+): the partitioner chooses a partition at random and sticks to it while the producer accumulates a batch for that partition. Once the batch is sent (because the batch is full, or <code>linger.ms</code> timeout expired, or the producer flushes), the partitioner chooses another partition and repeats. The sticky approach preserves a roughly even distribution over time but produces larger batches per request\u2014fewer requests, higher throughput, lower broker CPU/latency.</li> </ol> <p>Practical knobs that interact with sticky behavior:</p> <ul> <li><code>batch.size</code> \u2014 how large a batch the producer tries to fill before sending.</li> <li><code>linger.ms</code> \u2014 how long the producer will wait to try to fill a batch.   With sticky partitioning you will typically get fuller batches (and therefore fewer requests) compared with naive round-robin.</li> </ul>"},{"location":"streaming/kafka/15-Kafka_Partitions/#when-the-key-is-present-hashing-deterministic-mapping","title":"When the key is present \u2014 hashing + deterministic mapping","text":"<ul> <li>If your record has a key, the default partitioner uses a hash of the serialized key to pick the partition. In the Java client it computes something like:</li> </ul> <pre><code>int partition = toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n</code></pre> <p><code>toPositive</code> makes the 32-bit hash non-negative; <code>numPartitions</code> is the total number of partitions for the topic. Because the hash + modulo uses the total partition count, the same key consistently maps to the same partition id. This guarantees ordering for a key (ordering is only guaranteed within a partition).</p> <p>Two important consequences:</p> <ol> <li> <p>Deterministic mapping across producers: as long as all producers use the same hashing logic (Murmur2) and the same partition count, they will map the same key to the same partition \u2014 useful for consistency and joins/cogrouping. </p> </li> <li> <p>If partitions change, mapping changes: if you increase the number of partitions, <code>numPartitions</code> changes, so the modulo result changes and keys may map to different partitions afterwards. That\u2019s why adding partitions is not transparent for key affinity.</p> </li> </ol>"},{"location":"streaming/kafka/15-Kafka_Partitions/#why-the-use-all-partitions-not-just-available-matters-and-the-error-possibility","title":"Why the \u201cuse all partitions (not just available)\u201d matters \u2014 and the error possibility","text":"<ul> <li>The key-hash path explicitly uses the full partition list size (<code>numPartitions</code>) to compute the index. It does not choose only among currently available partitions when computing the hash modulo. The code then attempts to send to the chosen partition. If that partition currently has no leader or is otherwise unavailable, the broker will reject the produce request (errors like <code>NotLeaderForPartitionException</code> / <code>LeaderNotAvailable</code> / <code>UnknownTopicOrPartition</code>) until metadata refresh and leader election resolve the issue. The producer usually retries (subject to your <code>retries</code> and <code>delivery.timeout.ms</code> settings), but you can see immediate errors while the cluster recovers.</li> </ul> <p>So the short consequence: deterministic mapping is good for ordering and affinity, but it means the producer can attempt to write to a partition that is temporarily unavailable \u2014 causing retries or errors \u2014 because the mapping step did not exclude unavailable partitions.</p>"},{"location":"streaming/kafka/15-Kafka_Partitions/#code-sketch-conceptual","title":"Code sketch (conceptual)","text":"<p>Null key (sticky behavior; simplified):</p> <pre><code>if (key == null) {\n  // choose a sticky partition for this topic (random the first time)\n  // fill a batch (batch.size / linger.ms); send batch to that partition\n  // when batch sent, pick a different random partition and repeat\n}\n</code></pre> <p>Keyed:</p> <pre><code>if (key != null) {\n  partition = positive(murmur2(keyBytes)) % totalPartitions;  // totalPartitions = partitionsForTopic(topic).size()\n  // send to that exact partition (may be unavailable =&gt; broker will reject until metadata refresh/leader elected)\n}\n</code></pre>"},{"location":"streaming/kafka/15-Kafka_Partitions/#practical-advice-short-checklist","title":"Practical advice (short checklist)","text":"<ul> <li>Use keys when you need ordering or co-location of events for the same entity.</li> <li>If you don\u2019t need per-key ordering and you care about raw throughput, null keys + sticky partitioner give larger batches and lower CPU/latency. </li> <li>Ensure an adequate replication factor (\u22652 or 3) so leader loss does not immediately make a partition unavailable. Monitor leader elections. </li> <li>Tune <code>batch.size</code> and <code>linger.ms</code> to balance latency and batch fullness for sticky behavior. </li> <li>If you need special routing (e.g., avoiding a subset of partitions), implement a custom <code>Partitioner</code>.</li> </ul>"},{"location":"streaming/kafka/16-Kafka_Headers/","title":"Kafka Headers","text":""},{"location":"streaming/kafka/16-Kafka_Headers/#headers-in-kafka","title":"Headers in Kafka","text":"<p>Record headers give you the ability to add some metadata about the Kafka record, without adding any extra information to the key/value pair of the record itself. Headers are often used for lineage to indicate the source of the data in the record, and for routing or tracing messages based on header information without having to parse the message itself (perhaps the message is encrypted and the router doesn\u2019t have permissions to access the data). Headers are implemented as an ordered collection of key/value pairs. The keys are always a String, and the values can be any serialized object\u2014just like the message value.</p> <pre><code>from confluent_kafka import Producer\n\n# Create Kafka Producer\nproducer = Producer({'bootstrap.servers': 'localhost:9092'})\n\n# Define message headers\nheaders = [\n    ('event_type', b'user_signup'),\n    ('source', b'web'),\n    ('schema_version', b'1.0')\n]\n\n# Send message with headers\nproducer.produce(\n    topic='user_events',\n    key='user_123',              # optional\n    value='{\"user_id\": 123, \"name\": \"Alice\"}',\n    headers=headers\n)\n\n# Wait for delivery\nproducer.flush()\n</code></pre>"},{"location":"streaming/kafka/17-Kafka_Interceptors/","title":"Kafka Interceptors","text":""},{"location":"streaming/kafka/17-Kafka_Interceptors/#kafka-interceptors","title":"Kafka Interceptors","text":"<p>Kafka interceptors allow you to modify or extend the behavior of Kafka producers (or consumers) without changing the main application code. They act as middleware hooks that intercept records before they\u2019re sent to Kafka and after acknowledgments are received.</p>"},{"location":"streaming/kafka/17-Kafka_Interceptors/#why-use-interceptors","title":"Why use interceptors","text":"<p>There are cases where you want to:</p> <ul> <li>Add the same custom logic across multiple producer applications.</li> <li>Track or monitor messages.</li> <li>Modify or sanitize data before it\u2019s sent.</li> <li>Collect metrics or logs for auditing and debugging.</li> <li>You might not have access to the original application source code.</li> </ul>"},{"location":"streaming/kafka/17-Kafka_Interceptors/#producerinterceptor-interface","title":"ProducerInterceptor interface","text":"<p>Kafka provides the <code>ProducerInterceptor</code> interface with two main methods:</p>"},{"location":"streaming/kafka/17-Kafka_Interceptors/#1-producerrecordk-v-onsendproducerrecordk-v-record","title":"1. <code>ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record)</code>","text":"<ul> <li>This method is called before a record is serialized and sent to Kafka.</li> <li> <p>You can:</p> </li> <li> <p>Inspect or log the message.</p> </li> <li>Add headers or metadata (for example, tracking IDs or timestamps).</li> <li>Mask or redact sensitive data.</li> <li>Even modify the record itself.</li> <li>The method must return a valid <code>ProducerRecord</code>, because that record is what will actually be serialized and sent.</li> </ul> <p>Example use:</p> <pre><code>@Override\npublic ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {\n    // Add a custom header before sending\n    record.headers().add(\"source-app\", \"payment-service\".getBytes());\n    return record;\n}\n</code></pre>"},{"location":"streaming/kafka/17-Kafka_Interceptors/#2-void-onacknowledgementrecordmetadata-metadata-exception-exception","title":"2. <code>void onAcknowledgement(RecordMetadata metadata, Exception exception)</code>","text":"<ul> <li>This method runs after Kafka acknowledges a message (i.e., once the broker confirms receipt).</li> <li>You cannot change the acknowledgment itself.</li> <li> <p>Typical uses:</p> </li> <li> <p>Record success/failure metrics.</p> </li> <li>Log message delivery status.</li> <li>Send tracing or monitoring data.</li> </ul> <p>Example use:</p> <pre><code>@Override\npublic void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n    if (exception == null) {\n        System.out.println(\"Message sent successfully to \" + metadata.topic());\n    } else {\n        System.err.println(\"Send failed: \" + exception.getMessage());\n    }\n}\n</code></pre>"},{"location":"streaming/kafka/17-Kafka_Interceptors/#common-use-cases","title":"Common use cases","text":"<ul> <li>Monitoring and tracing \u2014 Collect metrics about message latency or failures.</li> <li>Data enrichment \u2014 Add standard headers (for lineage or auditing).</li> <li>Redaction \u2014 Remove or mask sensitive data before sending.</li> <li>Consistent policies \u2014 Apply organization-wide behaviors without altering each app\u2019s code.</li> </ul> <p>In short, producer interceptors give you a flexible way to observe, modify, and log message flow in Kafka producers before and after communication with the Kafka cluster \u2014 all without touching the core application logic.</p> <p>Check pg 129 and 130 of Kafka Def Guide for the code examples</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/","title":"Kafka Quotas and Throttling","text":""},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#quotas-and-throttling-in-kafka","title":"Quotas and Throttling in Kafka","text":"<p>Kafka\u2019s quota mechanism is designed to control and balance how much data producers and consumers can send and receive, as well as how much time brokers spend serving client requests. It ensures that no single client or user overwhelms the cluster, protecting performance and fairness across all users.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#1-what-quotas-do","title":"1. What quotas do","text":"<p>Kafka quotas help manage throughput and resource usage by defining limits on:</p> <ol> <li>Produce rate \u2014 How fast producers can send data to Kafka (in bytes per second).</li> <li>Consume rate \u2014 How fast consumers can fetch data from Kafka (in bytes per second).</li> <li>Request rate \u2014 How much broker processing time clients can consume (as a percentage of total broker time).</li> </ol> <p>In other words:</p> <ul> <li>Produce quota limits outgoing traffic from producers.</li> <li>Consume quota limits incoming traffic to consumers.</li> <li>Request quota limits the CPU or processing effort a broker dedicates to a client.</li> </ul> <p>If clients exceed these quotas, Kafka throttles them \u2014 it delays requests to slow them down to the configured rate.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#2-types-of-quotas","title":"2. Types of quotas","text":""},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#a-produce-quota","title":"a. Produce quota","text":"<ul> <li>Controls the rate at which producers can send data to Kafka.</li> <li>Measured in bytes per second.</li> <li>Prevents aggressive producers from saturating network bandwidth or broker disk I/O.</li> </ul> <p>Example:</p> <pre><code>quota.producer.default=2M\n</code></pre> <p>This means every producer can send up to 2 megabytes per second by default.</p> <p>If a producer exceeds this rate, the broker will throttle it \u2014 delaying further sends so the average rate stays within 2 MB/s.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#b-consume-quota","title":"b. Consume quota","text":"<ul> <li>Controls the rate at which consumers fetch data from brokers.</li> <li>Also measured in bytes per second.</li> <li>Prevents a single consumer from monopolizing broker resources or network capacity.</li> </ul> <p>Example:</p> <pre><code>quota.consumer.default=5M\n</code></pre> <p>This limits all consumers to fetching up to 5 MB/s from the broker.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#c-request-quota","title":"c. Request quota","text":"<ul> <li>Controls how much broker processing time each client can consume.</li> <li>Expressed as a percentage of broker time.</li> <li>Ensures that one client cannot consume too much of the broker\u2019s CPU cycles by sending too many small or complex requests.</li> </ul> <p>Example:</p> <pre><code>quota.request.default=25\n</code></pre> <p>This would mean each client can consume up to 25% of the broker\u2019s request-handling time before being throttled.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#3-scope-of-quotas","title":"3. Scope of quotas","text":"<p>Kafka allows you to define quotas at different levels:</p> Level Description Default Applies to all clients that don\u2019t have custom settings. User-specific Applies to an authenticated Kafka user. Works only when security (authentication) is enabled. Client ID-specific Applies to specific client applications identified by their <code>client.id</code> property. User + Client ID combination Most specific level \u2014 applies to a particular client under a particular user identity. <p>Kafka uses the most specific matching rule when deciding which quota to apply.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#4-static-vs-dynamic-quotas","title":"4. Static vs Dynamic quotas","text":""},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#a-static-quotas","title":"a. Static quotas","text":"<ul> <li>Configured in the Kafka broker configuration file (e.g., <code>server.properties</code>).</li> <li>Example:</li> </ul> <p><pre><code>quota.producer.default=2M\nquota.producer.override=\"clientA:4M,clientB:10M\"\n</code></pre> * These settings are static, meaning:</p> <ul> <li>You must edit the config file.</li> <li>You must restart all brokers for changes to take effect.</li> <li>Static quotas are suitable only for small, predictable environments.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#b-dynamic-quotas","title":"b. Dynamic quotas","text":"<ul> <li>Preferred in production environments.</li> <li> <p>Configured without restarting brokers, using:</p> </li> <li> <p>The <code>kafka-configs.sh</code> command-line tool, or</p> </li> <li>The AdminClient API.</li> <li>Dynamic configuration is stored in ZooKeeper (pre-KIP-500) or Kafka\u2019s internal configuration topics (in KIP-500 and later versions).</li> </ul> <p>Example command:</p> <pre><code>kafka-configs.sh --alter --add-config 'producer_byte_rate=4194304' \\\n--entity-type clients --entity-name clientA --bootstrap-server broker1:9092\n</code></pre> <p>This sets a dynamic produce quota of 4 MB/s for <code>clientA</code> (4194304 bytes).</p> <p>Dynamic quotas take effect immediately and don\u2019t require restarts.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#5-enforcement-mechanism","title":"5. Enforcement mechanism","text":"<p>Kafka brokers continuously measure the data throughput for each client and apply throttling when limits are exceeded.</p> <ul> <li>Throttling happens by delaying responses to producers or consumers.</li> <li>The broker maintains a moving average of data rate per client.</li> <li>If the rate exceeds the allowed quota, the broker inserts artificial delay before sending the acknowledgment (for producers) or response (for consumers).</li> </ul> <p>This ensures the effective throughput remains below the configured limit while maintaining fairness.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#6-why-quotas-matter","title":"6. Why quotas matter","text":"<p>Quotas are critical in large, shared Kafka clusters for:</p> <ul> <li>Preventing resource starvation \u2014 one misbehaving client can\u2019t take down brokers.</li> <li>Maintaining fair usage across multiple teams or tenants.</li> <li>Protecting system stability under high load.</li> <li>Avoiding network congestion and I/O bottlenecks.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#summary","title":"Summary","text":"Quota Type Controls Measured In Throttling Trigger Typical Use Produce Data sent by producers Bytes/sec Producer sends data too fast Limit producer throughput Consume Data fetched by consumers Bytes/sec Consumer reads too fast Balance consumer usage Request Broker processing time % of broker time Broker overloaded Prevent CPU overuse <p>In essence, Kafka quotas are a governance and stability mechanism. They provide administrators with precise control over how clients interact with the cluster \u2014 ensuring performance isolation, fair resource distribution, and predictable system behavior in multi-tenant or high-load environments.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#some-code-examples","title":"Some Code Examples","text":"<p>Limiting clientC (identified by client-id) to produce only 1024 bytes per second.</p> <pre><code>bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config\n'producer_byte_rate=1024' --entity-name clientC --entity-type clients\n</code></pre> <p>Limiting user1 (identified by authenticated principal) to produce only 1024 bytes per second and consume only 2048 bytes per second.</p> <pre><code>bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config\n'producer_byte_rate=1024,consumer_byte_rate=2048' --entity-name user1 --\nentity-type users\n</code></pre> <pre><code>bin/kafka-configs --bootstrap-server localhost:9092 --alter --add-config\n'consumer_byte_rate=2048' --entity-type users\n</code></pre> <p>This section explains how Kafka enforces quotas once a client (producer or consumer) exceeds its configured rate limit.</p> <p>Kafka doesn\u2019t simply reject requests when a quota is exceeded \u2014 instead, it throttles the client to bring its data rate back within the allowed limits.</p> <p>Let\u2019s go step by step.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#1-what-happens-when-a-client-exceeds-its-quota","title":"1. What happens when a client exceeds its quota","text":"<p>Each Kafka broker continuously tracks how much data each client is:</p> <ul> <li>Producing (sending to Kafka), or</li> <li>Consuming (fetching from Kafka).</li> </ul> <p>If a client\u2019s data rate (in bytes per second) goes above its allowed quota, the broker intervenes.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#2-throttling-mechanism","title":"2. Throttling mechanism","text":"<p>When the broker detects that a client is over its quota, it does not immediately block or disconnect the client. Instead, it slows it down by introducing delays in responses.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#example","title":"Example:","text":"<ul> <li>Suppose a producer is allowed to send 2 MB/s, but it starts sending 4 MB/s.</li> <li>The broker calculates that it must delay the next response enough to bring the producer\u2019s average rate back to 2 MB/s.</li> <li>So, if a producer sends a batch too quickly, the broker will wait (throttle) before sending back the acknowledgment.</li> </ul> <p>This delay forces the producer to slow down, because most producer clients have:</p> <ul> <li>A limited number of in-flight requests (i.e., unacknowledged messages).</li> <li>When acknowledgments are delayed, new sends are paused or queued.</li> <li>As a result, the client\u2019s effective throughput automatically drops to within the quota.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#3-mute-mechanism-protecting-the-broker","title":"3. Mute mechanism (protecting the broker)","text":"<p>Kafka must also defend itself against misbehaving clients \u2014 ones that ignore backpressure or continue flooding the broker even while throttled.</p> <p>To handle this, the broker can mute the client\u2019s network channel temporarily.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#what-this-means","title":"What this means:","text":"<ul> <li>The broker stops reading requests from the client\u2019s socket for a short time.</li> <li>The client can still try to send data, but the broker won\u2019t process or acknowledge it until the mute period ends.</li> <li>This enforces compliance by physically blocking the client from overwhelming the broker.</li> </ul> <p>When the delay (throttle duration) expires, the broker unmutes the client and resumes communication normally.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#4-automatic-self-regulation","title":"4. Automatic self-regulation","text":"<p>This throttling process works smoothly because:</p> <ul> <li>Kafka producers and consumers are designed to handle backpressure naturally.</li> <li>The delay in acknowledgments (throttling) limits how fast they can send or fetch messages.</li> <li>No manual intervention is needed; the client\u2019s behavior automatically adjusts to stay within quota limits.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#5-key-points-summarized","title":"5. Key points summarized","text":"Step What Happens Purpose 1. Client exceeds quota Broker detects higher-than-allowed data rate. Identify overload. 2. Broker delays responses Acknowledgments or fetch responses are held back. Slow down the client. 3. Client self-adjusts Limited in-flight requests cause rate to drop automatically. Maintain compliance. 4. Broker mutes connection (if needed) Temporarily stops reading from the client\u2019s socket. Protect broker from misbehaving clients. 5. Resume normal flow Once average rate drops below quota. Continue at allowed rate."},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#6-why-this-design-matters","title":"6. Why this design matters","text":"<p>This throttling system gives Kafka several advantages:</p> <ul> <li>Graceful control \u2014 Clients are slowed down, not disconnected.</li> <li>Fairness \u2014 No single producer or consumer can dominate broker resources.</li> <li>Protection \u2014 Brokers are shielded from overloading or denial-of-service\u2013like behavior.</li> <li>Transparency \u2014 Clients need not be explicitly aware of throttling; it happens naturally through delayed responses.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#in-essence","title":"In essence:","text":"<p>When a client exceeds its quota:</p> <ul> <li>Kafka brokers throttle it by delaying responses.</li> <li>The client\u2019s own request flow slows down automatically because of limited in-flight requests.</li> <li>If the client keeps misbehaving, the broker mutes its network channel temporarily.</li> <li>Once the quota balance is restored, normal communication resumes.</li> </ul> <p>This ensures stability, fairness, and protection in multi-client Kafka environments without breaking client connections.</p> <p>This section explains how Kafka exposes throttling information to clients through built-in metrics. These metrics let you monitor when and how much Kafka is throttling producer or consumer requests \u2014 an important indicator of whether clients are hitting their quota limits.</p> <p>Let\u2019s break this down in depth.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#1-purpose-of-throttling-metrics","title":"1. Purpose of throttling metrics","text":"<p>Kafka doesn\u2019t just throttle clients silently \u2014 it also provides metrics so you can see:</p> <ul> <li>Whether throttling is happening.</li> <li>How severe it is (how long clients are being delayed).</li> <li>Which part of the system (produce or fetch) is affected.</li> </ul> <p>These metrics are part of the client\u2019s JMX (Java Management Extensions) or producer/consumer metrics API.</p> <p>They help developers and administrators detect performance bottlenecks or verify that quotas are correctly enforced.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#2-the-key-throttling-metrics","title":"2. The key throttling metrics","text":"<p>Kafka exposes four main metrics related to quota throttling:</p> Metric Name Applies To Description <code>produce-throttle-time-avg</code> Producer The average amount of time (in milliseconds) that produce requests were delayed due to throttling. <code>produce-throttle-time-max</code> Producer The maximum time that any single produce request was delayed due to throttling. <code>fetch-throttle-time-avg</code> Consumer The average time that fetch requests (data pulls) were delayed due to throttling. <code>fetch-throttle-time-max</code> Consumer The maximum delay applied to any single fetch request. <p>These metrics represent how much the broker delayed responses \u2014 the higher the numbers, the more the client was throttled.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#3-what-causes-these-throttles","title":"3. What causes these throttles","text":"<p>There are two main reasons a Kafka broker will throttle a client, and both are reflected in these metrics:</p> <ol> <li> <p>Throughput quotas</p> </li> <li> <p>When a producer or consumer exceeds its configured data rate limit (bytes per second).</p> </li> <li>The broker delays responses to slow the client down.</li> <li> <p>These throttles show up in the <code>produce-throttle-time-*</code> and <code>fetch-throttle-time-*</code> metrics.</p> </li> <li> <p>Request-time quotas</p> </li> <li> <p>When a client uses too much broker CPU or I/O time.</p> </li> <li>The broker throttles the client to ensure fair resource sharing.</li> <li>These throttles also appear in the same metrics, since they affect request processing time.</li> </ol> <p>So, the reported throttling time could reflect either throughput-based throttling, request-time throttling, or both.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#4-example-interpretation","title":"4. Example interpretation","text":"<p>Suppose you\u2019re monitoring a producer client and see:</p> <pre><code>produce-throttle-time-avg = 300 ms\nproduce-throttle-time-max = 1200 ms\n</code></pre> <p>This means:</p> <ul> <li>On average, the producer\u2019s requests were delayed by 300 milliseconds before being acknowledged.</li> <li>At some point, one request was delayed by as much as 1.2 seconds.</li> </ul> <p>This strongly suggests the producer is exceeding its produce quota (e.g., sending too many bytes per second).</p> <p>Similarly, for consumers:</p> <pre><code>fetch-throttle-time-avg = 500 ms\nfetch-throttle-time-max = 2000 ms\n</code></pre> <p>indicates the consumer is reading data faster than allowed.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#5-other-client-requests","title":"5. Other client requests","text":"<p>The metrics above specifically measure throttling of produce and fetch requests. However, Kafka also throttles other types of requests \u2014 such as metadata requests, offset fetches, or administrative operations \u2014 but only under request-time quotas.</p> <p>For those, similar metrics exist, usually exposed under:</p> <pre><code>request-throttle-time-avg\nrequest-throttle-time-max\n</code></pre> <p>These help monitor general throttling behavior across all client interactions with the broker.</p>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#6-how-to-use-these-metrics","title":"6. How to use these metrics","text":"<p>Monitoring these metrics is essential for:</p> <ul> <li>Performance tuning: Detecting if producers or consumers are frequently throttled (indicating quota limits are too low).</li> <li>Quota enforcement validation: Confirming that quotas are actively controlling data rates as intended.</li> <li>Cluster health checks: Identifying overloaded brokers or imbalanced client workloads.</li> <li>Application optimization: Adjusting client configurations (e.g., batch size, linger time, fetch size) to avoid hitting throttling limits.</li> </ul> <p>In most cases, Kafka clients expose these metrics through:</p> <ul> <li>JMX exporters (for Prometheus or Grafana dashboards).</li> <li>The Kafka AdminClient metrics API.</li> <li>Tools like Confluent Control Center or Kafka Manager.</li> </ul>"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#7-summary","title":"7. Summary","text":"Metric Applies To Meaning Indicates <code>produce-throttle-time-avg</code> Producer Average time produce requests were delayed Ongoing throttling <code>produce-throttle-time-max</code> Producer Longest delay of any produce request Severity of throttling <code>fetch-throttle-time-avg</code> Consumer Average delay in fetch responses Consumer throttled <code>fetch-throttle-time-max</code> Consumer Maximum delay for any fetch request Peak throttling event <code>request-throttle-time-*</code> All clients Delay for non-produce/fetch requests Request-time quota enforcement"},{"location":"streaming/kafka/18-Kafka_Quotas_And_Throttling/#in-summary","title":"In summary","text":"<p>Kafka provides throttling metrics to make quota enforcement transparent. They measure how long client requests are being delayed by brokers, helping identify when clients exceed configured throughput or request-time quotas.</p> <p>By observing these metrics, administrators can:</p> <ul> <li>Confirm that throttling is happening as expected,</li> <li>Diagnose overactive clients,</li> <li>And fine-tune quotas for fair, balanced, and efficient Kafka cluster performance.</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/","title":"Kafka Consumers Eager and Coorperative Rebalancing","text":""},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#kafka-consumers-eager-and-cooperative-rebalancing","title":"Kafka Consumers : Eager and Cooperative Rebalancing","text":"<p>Here\u2019s a deep technical explanation of Kafka Consumer Group Rebalancing, including the difference between Eager Rebalancing and Cooperative (Incremental) Rebalancing, and how both interact with Static Membership (KIP-345).</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#1-what-is-consumer-group-rebalancing","title":"1. What is Consumer Group Rebalancing","text":"<p>In Apache Kafka, consumer groups allow multiple consumers to share the work of reading messages from one or more topics. Each partition of a topic is consumed by exactly one consumer in a group at a time.</p> <p>Rebalancing is the process where Kafka redistributes partitions among consumers when:</p> <ul> <li>A new consumer joins the group.</li> <li>A consumer leaves (crash, shutdown, or timeout).</li> <li>Topic partitions are added or removed.</li> <li>Consumer subscription changes (e.g., new topics are subscribed).</li> </ul> <p>Rebalancing ensures load balancing and fault tolerance, but it also causes a temporary pause in message consumption \u2014 this is what Kafka aims to minimize with newer protocols.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#2-how-rebalancing-works-at-a-high-level","title":"2. How Rebalancing Works (at a high level)","text":"<p>Rebalancing is coordinated by the Group Coordinator (a broker responsible for managing a specific consumer group).</p> <p>The process involves two key phases:</p> <ol> <li>JoinGroup \u2014 Consumers send a request to join the group.</li> <li>SyncGroup \u2014 The leader (one of the consumers) assigns partitions and shares the plan with others.</li> </ol>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#typical-sequence","title":"Typical sequence:","text":"<ol> <li>A consumer joins or leaves \u2192 triggers rebalance.</li> <li>All consumers stop fetching messages.</li> <li>All consumers send <code>JoinGroup</code> requests.</li> <li>Group coordinator elects a leader.</li> <li>Leader calculates partition assignment.</li> <li>Assignment is distributed via <code>SyncGroup</code>.</li> <li>Consumers resume consumption with new assignments.</li> </ol> <p>During this time, no messages are processed \u2014 this pause is the main drawback of rebalancing.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#3-eager-rebalancing-default-traditional","title":"3. Eager Rebalancing (Default / Traditional)","text":"<p>Eager Rebalancing (used before Kafka 2.4) is the original protocol.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#behavior","title":"Behavior:","text":"<ul> <li>All consumers stop consuming and revoke all partitions immediately when a rebalance starts.</li> <li>The entire group must rejoin, even if most members and assignments didn\u2019t change.</li> <li>After reassignment, consumers receive their new partitions and resume processing.</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#drawbacks","title":"Drawbacks:","text":"<ol> <li>Full stop of consumption \u2014 all consumers pause during rebalance.</li> <li>Unnecessary disruption \u2014 even unaffected consumers lose partitions.</li> <li>High latency in large groups or frequent joins/leaves.</li> <li>Impact on availability \u2014 the system is effectively idle during the rebalance.</li> </ol> <p>Example scenario: If a single new consumer joins a group of 10, all 10 must revoke partitions and wait for reassignment, even though only a few partitions need redistribution.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#4-cooperative-incremental-rebalancing-kip-429-kip-441","title":"4. Cooperative (Incremental) Rebalancing (KIP-429, KIP-441)","text":"<p>To overcome these issues, Kafka introduced Incremental Cooperative Rebalancing in KIP-429 (Kafka 2.4+).</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#key-idea","title":"Key idea:","text":"<p>Instead of revoking all partitions during every rebalance, only the affected partitions are revoked and reassigned.</p> <p>This makes rebalancing incremental and non-disruptive.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#how-it-works","title":"How it works:","text":"<ol> <li> <p>Phase 1: Detect change</p> </li> <li> <p>A new consumer joins or one leaves.</p> </li> <li> <p>Group coordinator initiates rebalance.</p> </li> <li> <p>Phase 2: Cooperative assignment</p> </li> <li> <p>Only partitions that need to move are revoked.</p> </li> <li>Unaffected consumers keep their existing partitions and continue processing.</li> <li> <p>The leader proposes a partial reassignment.</p> </li> <li> <p>Phase 3: Gradual synchronization</p> </li> <li> <p>The reassignment happens in steps (incrementally).</p> </li> <li> <p>Once all members confirm readiness, partitions move to the new consumers.</p> </li> <li> <p>Result:</p> </li> <li> <p>The group stabilizes without full disruption.</p> </li> <li>Latency and downtime drop drastically.</li> </ol>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#example","title":"Example:","text":"Event Eager Rebalancing Cooperative Rebalancing New consumer joins All 10 consumers revoke all partitions Only a few partitions are reallocated to new consumer Consumer leaves All remaining revoke and rebalance Only partitions of the leaving consumer are reassigned Duration Long, seconds to tens of seconds Very short, often milliseconds Consumption pause Entire group stops Only small subset pauses"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#5-cooperativestickyassignor","title":"5. CooperativeStickyAssignor","text":"<p>Kafka provides a partition assignment strategy called <code>CooperativeStickyAssignor</code>.</p> <ul> <li>It ensures stickiness \u2014 tries to keep existing assignments stable.</li> <li>It performs incremental (cooperative) changes during rebalance.</li> <li>It uses two-phase partition revocation to achieve smooth transition.</li> </ul> <p>Configuration example:</p> <pre><code>partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor\n</code></pre> <p>If some consumers in a group use <code>CooperativeStickyAssignor</code> and others use <code>RangeAssignor</code> or <code>RoundRobinAssignor</code>, they will be incompatible \u2014 all members of the group must use cooperative-compatible assignors.</p> <p>Awesome \u2014 this is one of those Kafka consumer group topics that sounds complicated (\ud83d\ude35\u200d\ud83d\udcab \"cooperative sticky assignor\") but actually makes perfect sense once you visualize what\u2019s happening.</p> <p>Let\u2019s explain it step by step \u2014 like you\u2019re 10 years old, using a fun analogy.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#1-imagine-a-group-of-friends-sharing-juice-boxes","title":"\ud83e\uddc3 1. Imagine a group of friends sharing juice boxes \ud83c\udf79","text":"<p>Let\u2019s say you and your friends (Kafka consumers) are sharing a bunch of juice boxes (Kafka partitions).</p> <p>Each friend gets a few juice boxes to drink from. Kafka\u2019s job is to decide who gets which juice boxes \u2014 that\u2019s called the partition assignment.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#2-rebalancing-when-friends-reshuffle-the-juice-boxes","title":"\u2699\ufe0f 2. Rebalancing = when friends reshuffle the juice boxes","text":"<p>Sometimes new friends join, or someone leaves:</p> <ul> <li>Maybe you get a new friend joining the group.</li> <li>Or someone leaves early.</li> </ul> <p>When that happens, the group needs to rebalance \u2014 meaning everyone has to share the juice boxes again so everyone gets their fair share.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#3-the-old-way-eager-rebalancing","title":"\ud83d\ude23 3. The old way \u2014 \u201ceager rebalancing\u201d","text":"<p>In the old system, Kafka used what\u2019s called the RangeAssignor or RoundRobinAssignor. These use a process called eager rebalancing.</p> <p>Here\u2019s what happens there:</p> <ol> <li>When something changes (someone joins or leaves), everyone must put down all their juice boxes.</li> <li>Then Kafka redistributes all of them from scratch.</li> <li>Every friend gets new boxes again \u2014 maybe some old, maybe new.</li> </ol> <p>This means:</p> <ul> <li>Everyone stops drinking (processing messages).</li> <li>Even people who could\u2019ve kept the same boxes must stop.</li> <li>There\u2019s a brief \u201cpause\u201d where the group does no work.</li> </ul> <p>That\u2019s wasteful, right?</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#4-the-new-way-cooperative-sticky-assignor","title":"\ud83e\udd1d 4. The new way \u2014 \u201cCooperative Sticky Assignor\u201d","text":"<p>Kafka 2.4+ introduced the Cooperative Sticky Assignor, which works much smarter.</p> <p>Think of it like this: When a new friend joins, Kafka says:</p> <p>\u201cOkay, nobody panic! You can keep drinking your current juice boxes. I\u2019ll just move around a few so the new friend gets some too.\u201d</p> <p>\ud83c\udfaf Key idea: Instead of everyone dropping all their boxes, Kafka changes only what\u2019s necessary \u2014 step by step.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#5-what-cooperative-and-sticky-mean","title":"\ud83d\udca1 5. What \u201ccooperative\u201d and \u201csticky\u201d mean","text":"Word Meaning Example Cooperative Everyone works together smoothly instead of stopping everything. \u201cKeep what you have until we need to move it.\u201d Sticky Try to keep the same assignments whenever possible. \u201cIf you already have a juice box, keep it unless I really have to take it away.\u201d <p>So, the Cooperative Sticky Assignor:</p> <ul> <li>Moves only what\u2019s needed during rebalancing.</li> <li>Keeps each consumer\u2019s existing partitions \u201csticky\u201d (unchanged) if possible.</li> <li>Allows incremental rebalancing (partial reshuffles).</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#6-how-it-changes-rebalancing-step-by-step","title":"\ud83d\udd01 6. How it changes rebalancing (step-by-step)","text":"<p>Let\u2019s walk through an example:</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#before","title":"Before","text":"<p>You have 3 friends (C1, C2, C3) and 6 juice boxes (P0\u2013P5).</p> Consumer Juice boxes C1 P0, P1 C2 P2, P3 C3 P4, P5 <p>Now a new friend (C4) joins.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#old-way-eager-rebalancing","title":"Old way (eager rebalancing)","text":"<ol> <li>Everyone puts down all juice boxes.</li> <li>Kafka redistributes from scratch.</li> <li>Everyone gets new boxes (like musical chairs).</li> <li>During that time, no one is drinking (processing paused).</li> </ol>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#new-way-cooperative-sticky","title":"New way (cooperative sticky)","text":"<ol> <li> <p>Kafka says:</p> </li> <li> <p>\u201cOkay, most of you can keep what you already have.\u201d</p> </li> <li>\u201cC4, I\u2019ll take one box from each of you and give them to you.\u201d</li> <li>C1, C2, and C3 keep one box each.</li> <li>C4 gets a few boxes gradually.</li> <li>Everyone else keeps working while this happens.</li> </ol> <p>\u2192 Nobody drops everything. Only small adjustments happen.</p> <p>That\u2019s why it\u2019s called cooperative \u2014 everyone works together instead of stopping.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#7-why-this-matters-in-real-kafka-use","title":"\u26a1 7. Why this matters in real Kafka use","text":""},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#with-old-rebalancing","title":"With old rebalancing:","text":"<ul> <li>Consumers stop reading messages.</li> <li>Offsets may need to be re-synced.</li> <li>Big pause in data flow.</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#with-cooperative-sticky-assignor","title":"With cooperative sticky assignor:","text":"<ul> <li>Rebalancing is incremental and much faster.</li> <li>Minimal interruption.</li> <li>Less network traffic.</li> <li>More predictable processing.</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#8-behind-the-scenes-what-kafka-actually-does","title":"\ud83e\udde9 8. Behind the scenes (what Kafka actually does)","text":"<p>When cooperative sticky is used, rebalancing happens in two phases:</p> <ol> <li>Revoke phase:    Kafka tells only some consumers to give up certain partitions.</li> <li>Assign phase:    Those partitions are reassigned to the new (or remaining) consumers.</li> </ol> <p>Everyone else continues processing without interruption.</p> <p>Contrast that with eager rebalancing, which revokes everything from everyone before reassigning.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#9-how-to-enable-it","title":"\ud83e\uddf0 9. How to enable it","text":"<p>In the consumer config:</p> <pre><code>partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor\n</code></pre> <p>Kafka supports multiple assignors \u2014 this one is the modern default for smooth, incremental rebalances.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#10-tldr-like-youre-explaining-to-a-kid","title":"\ud83e\udde0 10. TL;DR \u2014 Like you\u2019re explaining to a kid","text":"<p>Imagine a classroom where kids share crayons.</p> <ul> <li> <p>Eager rebalancing:   Every time a new kid comes in, the teacher makes everyone put their crayons back, then redistributes them all.   \u2192 Lots of waiting.</p> </li> <li> <p>Cooperative Sticky Assignor:   The teacher says, \u201cKeep your crayons! I\u2019ll just ask one or two of you to share a few with the new kid.\u201d   \u2192 Minimal disruption, no chaos.</p> </li> </ul> <p>\u2705 In short:</p> Term Meaning Why it\u2019s good Cooperative Change assignments gradually Consumers keep working during rebalancing Sticky Keep partitions with the same consumer as long as possible Fewer moves, faster recovery Result Incremental rebalancing Faster, smoother, minimal downtime"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#6-static-membership-kip-345","title":"6. Static Membership (KIP-345)","text":"<p>Even with cooperative rebalancing, frequent consumer restarts (for example, due to container redeployments) can still cause unnecessary rebalances, because Kafka sees a new consumer as a new member.</p> <p>Static Membership (KIP-345) reduces this.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#how-it-works_1","title":"How it works:","text":"<ul> <li>Each consumer has a stable identity (<code>group.instance.id</code>).</li> <li>As long as the same ID rejoins before <code>session.timeout.ms</code> expires, Kafka treats it as the same member.</li> <li>Therefore, no rebalance is triggered.</li> </ul>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#example_1","title":"Example:","text":"<p>Without static membership:</p> <ul> <li>Consumer restarts \u2192 considered a new member \u2192 triggers rebalance.</li> </ul> <p>With static membership:</p> <ul> <li>Consumer restarts with same <code>group.instance.id</code> \u2192 no rebalance \u2192 resumes same partitions instantly.</li> </ul> <p>Configuration example:</p> <pre><code>group.instance.id=consumer-app-01\n</code></pre> <p>Result: Minimal churn and faster recovery during rolling deployments or container restarts.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#7-combining-cooperative-rebalancing-static-membership","title":"7. Combining Cooperative Rebalancing + Static Membership","text":"<p>These two features together provide near-continuous availability:</p> Feature Role Cooperative Rebalancing (KIP-429) Minimizes partition movement; incremental changes only. Static Membership (KIP-345) Prevents rebalances during temporary restarts. <p>Together, they drastically reduce downtime and consumer lag, particularly in large consumer groups or microservice-based deployments.</p>"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#8-summary-eager-vs-cooperative-rebalancing","title":"8. Summary: Eager vs Cooperative Rebalancing","text":"Aspect Eager Rebalancing Cooperative Rebalancing Revoke all partitions Yes No (only affected) Pause during rebalance Entire group Partial Rebalance duration Long Short Impact on availability High Minimal Protocol version Default (pre-2.4) KIP-429 (2.4+) Assignor type RangeAssignor, StickyAssignor CooperativeStickyAssignor Supports incremental changes No Yes Works with static membership Yes, but less useful Very effective"},{"location":"streaming/kafka/19-Kafka_Consumer_Eager_And_Cooperative_Rebalance/#9-in-summary","title":"9. In summary","text":"<ul> <li>Rebalancing is how Kafka redistributes partitions in a consumer group.</li> <li>Eager rebalancing stops everyone, causing latency and disruption.</li> <li>Cooperative (Incremental) rebalancing (KIP-429) performs non-disruptive, stepwise reassignments.</li> <li>Static Membership (KIP-345) prevents unnecessary rebalances during client restarts.</li> <li>The CooperativeStickyAssignor ensures smooth partition handovers.</li> </ul> <p>Together, these improvements make modern Kafka consumer groups highly stable, low-latency, and resilient even in dynamic, cloud-native environments.</p> <p>Consumers maintain membership in a consumer group and ownership of partitions assigned to them by sending heartbeats to a Kafka broker designated as a group coordinator (differs for different consumer groups)</p> <p>The heartbeats are sent by a background thread and as long as it keeps sending within session timeout duration, the consumer is considered to be alive.</p> <p>If consumer stops sending heartbeats, the consumer is considered dead and a rebalance is triggered. When closing a consumer cleanly, the consumer will notify group coordinator that its leaving.</p>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/","title":"Kafka Consumer Static Partitioning","text":""},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#static-consumer-partitioning-in-kafka","title":"Static Consumer Partitioning in Kafka","text":"<p>When a consumer leaves and rejoins a consumer group, its assigned a new member id and a new set of partitions via rebalance protocol.</p> <p>If we configure a <code>group.instance.id</code> then the consumer becomes static member of a group. When a consumer first joins a group its assigned partitions using the partition assignment strategy.</p> <p>However if the consumer shuts down and restarts, it remains part of the same group until session times out. Its assigned the same partitions without triggering a rebalance.</p> <p>Below is a clear, step-by-step explanation of the behavior you described, with examples, consequences, and practical guidance \u2014 no emojis.</p>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#what-static-membership-does-in-plain-terms","title":"What static membership does (in plain terms)","text":"<p>When a consumer in a group is configured with a stable identity (<code>group.instance.id</code>), the group coordinator can treat that consumer as the same member across restarts. Instead of triggering a full rebalance when that consumer rejoins, the coordinator can simply return the previously cached partition assignment to the rejoining consumer. That avoids the usual revoke/assign cycle and the associated pause in processing.</p>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#step-by-step-example","title":"Step-by-step example","text":"<ol> <li> <p>Initial state</p> </li> <li> <p>Consumer A starts with <code>group.instance.id = app-1</code>.</p> </li> <li> <p>Coordinator assigns partitions P0 and P1 to consumer A and caches that assignment.</p> </li> <li> <p>Short restart (within the timeout window)</p> </li> <li> <p>Consumer A shuts down and restarts quickly (e.g., a few seconds).</p> </li> <li>Because the coordinator still has A\u2019s cached assignment, it does not trigger a rebalance; it sends the cached assignment to the rejoining consumer A.</li> <li> <p>Result: other consumers in the group continue processing their partitions without interruption.</p> </li> <li> <p>Restart longer than the configured timeout</p> </li> <li> <p>If consumer A does not rejoin before the session timeout expires, the coordinator treats the cached member as gone and reassigns its partitions to other consumers. When A eventually returns, it will be treated as a new member and receive whatever assignment is current.</p> </li> <li> <p>Two consumers using the same static id</p> </li> <li> <p>If Consumer B tries to join the same group using <code>group.instance.id = app-1</code> while Consumer A is already registered, the join will fail \u2014 the coordinator returns an error indicating that an instance with that ID already exists. Only one active member may use a given <code>group.instance.id</code>.</p> </li> </ol>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#key-consequences-and-trade-offs","title":"Key consequences and trade-offs","text":"<p>Pros</p> <ul> <li>Avoids needless rebalances on transient restarts, which keeps consumption continuous and avoids expensive local state rebuilds.</li> <li>Stability for stateful consumers \u2014 useful when a consumer maintains local caches or state tied to the partitions it owns.</li> </ul> <p>Cons / Risks</p> <ul> <li>Partitions may sit idle while the static member is down: because the coordinator does not reassign those partitions immediately, no other consumer will consume them during the static member\u2019s downtime. That causes message backlog on those partitions.</li> <li>Lag on restart: when the static member eventually restarts and regains ownership, it must process the backlog and may lag behind the head of the partition.</li> <li>Duplicate-id collisions: accidental simultaneous starts with the same <code>group.instance.id</code> will cause one of the joins to fail.</li> <li>Detection lag: the broker only considers a static member \u201cgone\u201d when the <code>session.timeout.ms</code> (or equivalent mechanism) expires. Until then, no automatic reassignment occurs.</li> </ul>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#important-configuration-considerations","title":"Important configuration considerations","text":"<ul> <li> <p><code>session.timeout.ms</code> (or the equivalent setting on your client/broker): this determines how long the coordinator waits before declaring a member dead.</p> </li> <li> <p>Set it high enough so simple, expected restarts (graceful restarts, rolling deployments) don\u2019t cause rebalances.</p> </li> <li>Set it low enough so that if a consumer really fails or is down for an extended period, its partitions will be reassigned and processing will continue under other consumers.</li> <li> <p>In short: there is a trade-off between minimizing spurious rebalances and minimizing time-to-failover for truly failed consumers.</p> </li> <li> <p>Heartbeats and polling: make sure <code>heartbeat.interval.ms</code> and <code>max.poll.interval.ms</code> are tuned so the consumer sends heartbeats frequently enough and can process records without being timed out unintentionally.</p> </li> <li> <p>Capacity for catch-up: if a static member can be down for some time, ensure it (or your system) can handle catching up \u2014 both in terms of throughput and memory/state required to rebuild caches.</p> </li> </ul>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#when-to-use-static-membership","title":"When to use static membership","text":"<ul> <li>Use static membership when your consumers maintain expensive local state or caches that are costly to recreate on every restart (examples: local in-memory caches, RocksDB state stores used with stream processing).</li> <li>Avoid static membership when high availability through immediate reassignment is more important than preserving a specific consumer\u2019s state (for example, where backlog or lag must be minimized at all costs).</li> </ul>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#practical-recommendations","title":"Practical recommendations","text":"<ol> <li>Test restart behavior in a staging environment to find good <code>session.timeout.ms</code> and heartbeat settings that fit your deployment patterns (container restarts, rolling updates, etc.).</li> <li>Monitor backlog and lag for partitions owned by static members so you can detect problematic downtime and tune timeouts accordingly.</li> <li>Ensure uniqueness of <code>group.instance.id</code> in your deployment automation to avoid accidental duplicate usage.</li> <li>Combine features when helpful \u2014 e.g., using static membership together with cooperative rebalancing and a sticky assignor often yields the best balance of low disruption and controlled reassignments.</li> </ol>"},{"location":"streaming/kafka/20-Kafka_Consumer_Static_Partitioning/#summary","title":"Summary","text":"<p>Static membership lets the group coordinator hand cached assignments to a rejoining consumer so you avoid a full rebalance on short restarts. That\u2019s excellent for stateful consumers, but it also means partitions can remain unprocessed while the static member is down. The critical tuning point is the session timeout: make it long enough to survive expected restarts but short enough to allow timely reassignment when a consumer is truly gone.</p>"},{"location":"streaming/kafka/21-Kafka_Poll_Loop/","title":"Kafka Poll Loop","text":""},{"location":"streaming/kafka/21-Kafka_Poll_Loop/#the-kafka-poll-loop","title":"The Kafka Poll Loop","text":"<pre><code>Duration timeout = Duration.ofMillis(100);\n\nwhile (true) {\n    ConsumerRecords&lt;String, String&gt; records = consumer.poll(timeout);\n    for (ConsumerRecord&lt;String, String&gt; record : records) \n    {\n        System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n        \"customer = %s, country = %s\\n\",\n        record.topic(), record.partition(), record.offset(),\n        record.key(), record.value());\n\n        int updatedCount = 1;\n\n        if (custCountryMap.containsKey(record.value())) \n            {\n            updatedCount = custCountryMap.get(record.value()) + 1;\n            }\n        custCountryMap.put(record.value(), updatedCount);\n\n        JSONObject json = new JSONObject(custCountryMap);\n\n        System.out.println(json.toString());\n        }\n}\n</code></pre> <p>The same way that sharks need to keep moving or they will die, consumers must continuously poll or they will be considered dead.</p> <p>The parameter <code>timeout</code> passed to poll() controls how long poll() will block data if data is not available in consumer buffer.</p> <p>Above code just keeps running sum of # of customers and their details are printed out.</p>"},{"location":"streaming/kafka/21-Kafka_Poll_Loop/#addn-functions-of-poll-loop","title":"Addn Functions of Poll Loop","text":"<p>The first time poll() is called with consumer, its responsible for finding the <code>GroupCoordinator</code>, joining a consumer group and reveiving a partition assignment.</p> <p>If a rebalance is triggered it will be handled in the poll loop including callbacks. Anything that can go wrong in consumer or listeners is likely to show up as an exception while polling.</p> <p>If poll is not invoked for more than <code>max.poll.interval.ms</code> consumer will be considered dead and removed from consumer group.</p>"},{"location":"streaming/kafka/21-Kafka_Poll_Loop/#thread-safety","title":"Thread Safety","text":"<p>We cant have multiple consumers that belong to same group in one thread, we cant have multiple threads using same consumer.</p> <p>To run multiple consumers in same group each one needs to run in own thread.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/","title":"Kafka Consumer Properties Part I","text":""},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#configuring-consumers-in-kafka-part-1","title":"Configuring Consumers in Kafka - Part 1","text":""},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#1-fetchminbytes","title":"1. <code>fetch.min.bytes</code>","text":"<p>This property allows a consumer to specify the minimum amount of data that it wants to receive from the broker. If the batch size is less than 1KB(default) then the consumer waits.</p> <p>This reduces the load on both broker and consumer as they dont have to handle back and forth requests.</p> <p>If the consumer is using too much CPU then we need to set this param higher but if there are lot of consumers then we need to set the value lower so that the other consumers  dont wait for a very long time for their request to be considered by broker.</p> <p>Tuning this property could introduce higher latency than usual.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#2-fetchmaxwaitms","title":"2. <code>fetch.max.wait.ms</code>","text":"<p>We are telling Kafka to wait before it has enough data to sendto consumer. By default Kafka waits for 500 ms. This introduce a latency of 500ms if kafka doesnt have enough data to send to consumer. </p> <p>If we want to control latency it can be set to 10ms and fetch.min.bytes to 1MB.</p> <p>Kafka broker will respond when the data is either 1MB or 100ms has passed.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#3-fetchmaxbytes","title":"3. <code>fetch.max.bytes</code>","text":"<p>The <code>fetch.max.bytes</code> property in a Kafka consumer controls the maximum amount of data (in bytes) the consumer can receive from the broker in a single fetch request.</p> <p>Here\u2019s what it means in detail:</p> <ul> <li>Purpose: It limits how much data the consumer will store in memory per poll request, regardless of how many partitions or messages are included in that data.</li> <li>Default Value: 50 MB.</li> <li> <p>Behavior:</p> </li> <li> <p>When a consumer calls <code>poll()</code>, the broker sends data up to this size limit.</p> </li> <li>However, if the first batch of messages available for a partition is larger than this limit, the broker will still send that entire batch \u2014 ignoring the limit temporarily \u2014 to ensure the consumer keeps making progress and does not get stuck.</li> <li> <p>Broker Side Control:</p> </li> <li> <p>The broker also has its own limit through the <code>message.max.bytes</code> (for producer messages) and <code>max.message.bytes</code> or <code>max.partition.fetch.bytes</code> (for consumers) configurations.</p> </li> <li>These ensure that a single fetch request or message doesn\u2019t overload the broker or network.</li> <li> <p>Performance Consideration:</p> </li> <li> <p>Setting this value too high can cause large memory usage and long network transfers.</p> </li> <li>Setting it too low can cause the consumer to make frequent fetch requests, reducing efficiency.</li> </ul> <p>In short, <code>fetch.max.bytes</code> helps balance memory usage, network load, and fetch frequency on the consumer side.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#4-maxpollrecords","title":"4. <code>max.poll.records</code>","text":"<p>Controls max number of records that a single poll of broker can fetch.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#5-maxpartitionfetchbytes","title":"5. <code>max.partition.fetch.bytes</code>","text":""},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#definition","title":"Definition","text":"<p><code>max.partition.fetch.bytes</code> specifies the maximum number of bytes the broker will return for each partition in a single fetch request. The default value is 1 MB.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#how-it-works","title":"How It Works","text":"<ul> <li>When the consumer sends a fetch request, the broker looks at each partition the consumer is subscribed to.</li> <li>The broker sends up to <code>max.partition.fetch.bytes</code> of data per partition in that request.</li> <li>So if a consumer is reading from multiple partitions, the total data returned can be up to:</li> </ul> <pre><code>total bytes \u2248 number of partitions \u00d7 max.partition.fetch.bytes\n</code></pre>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#example","title":"Example","text":"<p>If a consumer reads from 5 partitions and <code>max.partition.fetch.bytes = 1MB</code>, the broker could return up to 5 MB of data in one poll.</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#important-note","title":"Important Note","text":"<ul> <li>If the first record batch in a partition is larger than this value, Kafka will still send it (ignoring the limit temporarily) to ensure progress.</li> <li>Managing consumer memory through this property can be tricky because you can\u2019t easily control how many partitions the broker will include in a single response.</li> </ul>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#recommendation","title":"Recommendation","text":"<p>Because it\u2019s difficult to predict and control total memory use across multiple partitions, it\u2019s usually better to tune <code>fetch.max.bytes</code> instead. That property sets a global upper limit on how much data the consumer fetches in total per request, making it easier to manage memory and network load.</p> <p>In short:</p> <ul> <li><code>max.partition.fetch.bytes</code> \u2192 limits data per partition (default 1 MB)</li> <li><code>fetch.max.bytes</code> \u2192 limits total data per fetch request (default 50 MB)</li> <li>Use <code>fetch.max.bytes</code> for general memory control; use <code>max.partition.fetch.bytes</code> only when you need fine-grained control per partition.</li> </ul>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#6-sessiontimeoutms-and-heartbeatintervalms","title":"6. <code>session.timeout.ms</code> and <code>heartbeat.interval.ms</code>","text":""},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#1-what-it-does","title":"1. What It Does","text":"<p><code>session.timeout.ms</code> specifies the maximum time the Kafka group coordinator will wait for a consumer to send a heartbeat before deciding that the consumer has failed.</p> <ul> <li>Default: 10 seconds</li> <li>If the consumer does not send a heartbeat within this period, the coordinator assumes the consumer is dead and triggers a rebalance.</li> <li>During a rebalance, the coordinator reassigns the partitions owned by that consumer to the remaining active consumers in the group.</li> </ul>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#2-related-property-heartbeatintervalms","title":"2. Related Property \u2014 <code>heartbeat.interval.ms</code>","text":"<p>This property defines how often the consumer sends heartbeats to the group coordinator.</p> <ul> <li><code>heartbeat.interval.ms</code> must always be less than <code>session.timeout.ms</code>.</li> <li>Common practice is to set:</li> </ul> <pre><code>heartbeat.interval.ms = session.timeout.ms / 3\n</code></pre> <p>Example: If <code>session.timeout.ms = 9000</code> (9 seconds), then <code>heartbeat.interval.ms</code> should be around <code>3000</code> (3 seconds).</p>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#3-balancing-failure-detection-vs-stability","title":"3. Balancing Failure Detection vs. Stability","text":"<p>Tuning <code>session.timeout.ms</code> involves a tradeoff:</p> <ul> <li> <p>Lower value (e.g., 3\u20135 seconds):</p> </li> <li> <p>Detects consumer failures faster</p> </li> <li> <p>But may cause frequent rebalances if network delays or short pauses occur</p> </li> <li> <p>Higher value (e.g., 20\u201330 seconds):</p> </li> <li> <p>Fewer accidental rebalances</p> </li> <li>But slower to detect actual consumer failures</li> </ul>"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#4-summary","title":"4. Summary","text":"Property Purpose Default Notes <code>session.timeout.ms</code> Max time consumer can go without heartbeats before being considered dead 10,000 ms Shorter = faster failure detection, but risk of instability <code>heartbeat.interval.ms</code> How often heartbeats are sent 3,000 ms Must be less than session timeout, usually \u2153 of it"},{"location":"streaming/kafka/22-Kafka_Configuring_Consumers_Pt1/#in-short","title":"In short","text":"<ul> <li><code>session.timeout.ms</code> = how long a consumer can be silent before being declared dead.</li> <li><code>heartbeat.interval.ms</code> = how frequently the consumer checks in.</li> <li>Keep heartbeat interval lower than session timeout (about one-third).</li> <li>Adjust both together based on your system\u2019s network stability and desired failure recovery speed.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/","title":"Kafka Consumer Properties Part II","text":""},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#kafka-configuring-consumers-part-ii","title":"Kafka Configuring Consumers - Part II","text":""},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#1-maxpollintervalms","title":"1. <code>max.poll.interval.ms</code>","text":"<p>This explanation describes the <code>max.poll.interval.ms</code> property in a Kafka consumer \u2014 an important setting that determines how long a consumer can go without calling <code>poll()</code> before Kafka considers it dead.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#1-purpose","title":"1. Purpose","text":"<p><code>max.poll.interval.ms</code> defines the maximum delay allowed between two consecutive <code>poll()</code> calls made by a consumer.</p> <ul> <li>If the consumer does not call <code>poll()</code> within this time window, it is considered stuck or dead.</li> <li>Kafka will then remove it from the consumer group and trigger a rebalance to redistribute its partitions.</li> </ul> <p>Default: 5 minutes (300,000 ms)</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#2-why-its-needed","title":"2. Why It\u2019s Needed","text":"<p>Kafka consumers have two main \u201cliveness\u201d mechanisms:</p> <ul> <li>Heartbeats (<code>session.timeout.ms</code> + <code>heartbeat.interval.ms</code>) \u2014 ensure the consumer process is alive.</li> <li>Poll interval (<code>max.poll.interval.ms</code>) \u2014 ensures the consumer is actively processing records, not just alive but doing work.</li> </ul> <p>Without <code>max.poll.interval.ms</code>, it\u2019s possible for:</p> <ul> <li>The consumer\u2019s main thread to be stuck or deadlocked,</li> <li>While the background heartbeat thread keeps sending heartbeats,</li> <li>Making Kafka think the consumer is still healthy even though it\u2019s no longer processing messages.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#3-how-it-works","title":"3. How It Works","text":"<ul> <li>The consumer\u2019s main thread must call <code>poll()</code> periodically to fetch new records.</li> <li> <p>If the consumer does not call <code>poll()</code> again within the <code>max.poll.interval.ms</code> time limit:</p> </li> <li> <p>The consumer is considered unresponsive.</p> </li> <li>The broker removes it from the group.</li> <li>A rebalance occurs so other consumers can take over its partitions.</li> <li>The background heartbeat thread stops sending heartbeats after signaling a \u201cleave group\u201d request.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#4-interaction-with-other-properties","title":"4. Interaction with Other Properties","text":"<ul> <li> <p><code>max.poll.records</code>:   Limits how many records <code>poll()</code> returns in one call.   Smaller values mean the consumer calls <code>poll()</code> more frequently.   Larger values may increase the processing time before the next <code>poll()</code>.   Together with <code>max.poll.interval.ms</code>, it helps balance throughput and stability.</p> </li> <li> <p><code>session.timeout.ms</code> vs. <code>max.poll.interval.ms</code>:</p> </li> <li> <p><code>session.timeout.ms</code> detects dead consumers (no heartbeats).</p> </li> <li><code>max.poll.interval.ms</code> detects stuck consumers (not polling).</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#5-tuning-guidance","title":"5. Tuning Guidance","text":"Scenario Recommended Setting Fast message processing Keep default (5 minutes) or lower if frequent polls are guaranteed Heavy or slow processing per record Increase to allow enough time for processing before next <code>poll()</code> To avoid false rebalances due to long processing times Adjust both <code>max.poll.records</code> and <code>max.poll.interval.ms</code> accordingly"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#6-example","title":"6. Example","text":"<p>If a consumer takes around 2 minutes to process one batch of messages, set:</p> <pre><code>max.poll.interval.ms = 300000   # (5 minutes) \u2013 still safe\nmax.poll.records = 100          # process fewer messages per poll\n</code></pre> <p>This ensures the consumer won\u2019t be removed unless it truly stops processing for more than 5 minutes.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#in-short","title":"In short","text":"<ul> <li><code>max.poll.interval.ms</code> controls how long a consumer can go without polling.</li> <li>It prevents \u201czombie\u201d consumers that appear alive but aren\u2019t processing data.</li> <li>Default is 5 minutes.</li> <li>Tune it alongside <code>max.poll.records</code> based on how long your consumer takes to process each batch.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#2-defaultapitimeoutms","title":"2. <code>default.api.timeout.ms</code>","text":"<p>This is the timeout that will apply to almost all API calls for timeout and will include a retry when needed.</p> <p>Exception is poll method that requires a default explicit timeout.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#3requesttimoutms","title":"3.<code>request.timout.ms</code>","text":"<p>Max amount of time that consumer will wait for broker to respond.</p> <p>If the broker doesnt respond by this time, consumer may think broker is busy and will try reconnecting after some time. We shouldnt keep it too low because if broker is already overloaded then it makes no sense to add more overhead by sending api calls ever 1/2 seconds.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#4-autooffsetreset","title":"4. <code>auto.offset.reset</code>","text":"<p>This property controls the behavious when a consumer starts reading from a partition without a committed offset, or if it has an invalid id.</p> <p>The default is <code>latest</code>, basically indicating that lacking a valid offset, consumer will start reading from the latest records, alternate is <code>earliest</code>.</p> <p>If we set it to <code>none</code> then consumer will fail when trying to restart from invalid offset.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#5-enableautocommit","title":"5. <code>enable.auto.commit</code>","text":"<p>By default its <code>true</code> but if we want to control when offsets are committed to minimize duplicates and avoid duplicating data.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#6-offsetsretentionminutes","title":"6. <code>offsets.retention.minutes</code>","text":"<p>This paragraph describes how Kafka manages committed offsets for consumer groups, and how a specific broker configuration controls how long those offsets are kept once the group becomes inactive.</p> <p>Let\u2019s break it down step by step.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#1-context-consumer-groups-and-committed-offsets","title":"1. Context: Consumer Groups and Committed Offsets","text":"<p>In Kafka, a consumer group keeps track of what messages its consumers have already read using committed offsets.</p> <ul> <li>A committed offset tells Kafka, \u201cThis group has successfully processed messages up to this point in the partition.\u201d</li> <li>Kafka stores these offsets internally in a special topic called <code>__consumer_offsets</code>.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#2-when-consumers-are-active","title":"2. When Consumers Are Active","text":"<p>As long as the consumer group is active (meaning at least one consumer is running and sending heartbeats to the group coordinator):</p> <ul> <li>Kafka retains the group\u2019s committed offsets.</li> <li>This ensures that if a consumer crashes or a rebalance happens, Kafka can resume consumption from the last committed offset \u2014 no data is lost or reprocessed unnecessarily.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#3-when-the-group-becomes-inactive-empty","title":"3. When the Group Becomes Inactive (Empty)","text":"<p>If all consumers in a group stop running (the group becomes empty):</p> <ul> <li>Kafka starts a retention timer for that group\u2019s offsets.</li> <li>The duration of this timer is controlled by the broker configuration parameter:</li> </ul> <pre><code>offsets.retention.minutes\n</code></pre> <p>(By default, 7 days \u2014 equivalent to 10080 minutes.)</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#4-offset-expiration","title":"4. Offset Expiration","text":"<p>If the consumer group remains inactive beyond that retention period, Kafka deletes its stored offsets.</p> <ul> <li>After this happens, Kafka forgets that the group ever existed.</li> <li>When the same group restarts later, it will behave like a new consumer group \u2014 starting from the position defined by its <code>auto.offset.reset</code> setting (usually <code>latest</code> or <code>earliest</code>).</li> </ul> <p>In other words:</p> <p>Once the offsets expire, Kafka cannot resume consumption from where the group left off.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#5-version-differences","title":"5. Version Differences","text":"<p>Kafka\u2019s behavior around offset retention changed several times in older versions.</p> <ul> <li>Before version 2.1.0, Kafka\u2019s logic for when offsets were deleted was slightly different, so older clusters may not behave exactly the same way.</li> <li>In modern Kafka versions (2.1.0 and later), the behavior described above is the standard.</li> </ul>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#6-example_1","title":"6. Example","text":"<p>Let\u2019s say:</p> <ul> <li>You have a consumer group named <code>trade-settlement-group</code>.</li> <li>It processes messages daily but is inactive on weekends.</li> <li>The broker\u2019s offset retention is set to 7 days.</li> </ul> <p>If your consumers stop on Friday and don\u2019t restart until the next Monday, everything is fine \u2014 offsets are still retained.</p> <p>But if the consumers remain idle for more than 7 days, Kafka deletes their committed offsets. When the group restarts after that, it won\u2019t remember its last position and will consume messages starting from the offset defined by <code>auto.offset.reset</code>.</p>"},{"location":"streaming/kafka/23-Kafka_Configuring_Consumers_Pt2/#7-summary-table","title":"7. Summary Table","text":"Situation Kafka Behavior Consumers in group are active Offsets retained indefinitely Group becomes empty Retention countdown starts Group inactive longer than <code>offsets.retention.minutes</code> Offsets deleted Group restarts after deletion Treated as a new group (starts from <code>auto.offset.reset</code>) <p>In summary: Kafka keeps consumer offsets only while the group is active. Once all consumers stop and the retention period passes, those offsets are deleted. When the group returns, it starts over \u2014 as if it never existed before.</p>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/","title":"Kafka Partition Assignment Strategies","text":""},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#partition-assignment-strategies-in-kafka-consumers","title":"Partition Assignment Strategies in Kafka Consumers","text":""},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#range","title":"Range","text":"<p>In Kafka, the Range assignment strategy determines how partitions are distributed among consumers in a consumer group.</p> <p>When using Range assignment:</p> <ul> <li>Each consumer is assigned a consecutive range of partitions from each topic it subscribes to.</li> <li>The assignment is done separately for each topic.</li> </ul> <p>For example:</p> <ul> <li>Suppose there are two consumers: <code>C1</code> and <code>C2</code></li> <li>And two topics: <code>T1</code> and <code>T2</code></li> <li>Each topic has three partitions: <code>0</code>, <code>1</code>, and <code>2</code></li> </ul> <p>Under Range assignment:</p> <ul> <li> <p>For topic <code>T1</code>:</p> </li> <li> <p><code>C1</code> gets partitions <code>0</code> and <code>1</code></p> </li> <li><code>C2</code> gets partition <code>2</code></li> <li> <p>For topic <code>T2</code>:</p> </li> <li> <p><code>C1</code> again gets partitions <code>0</code> and <code>1</code></p> </li> <li><code>C2</code> gets partition <code>2</code></li> </ul> <p>So, <code>C1</code> ends up with four partitions in total (two from each topic), while <code>C2</code> gets only two partitions.</p> <p>This imbalance happens because the Range assignor works topic by topic, and if the number of partitions is not evenly divisible by the number of consumers, the earlier consumers in the list get slightly more partitions.</p> <p>In summary:</p> <ul> <li>Range assignment divides partitions in order and independently per topic.</li> <li>It can lead to uneven load distribution if partitions do not divide evenly among consumers.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#round-robin","title":"Round Robin","text":"<p>In Kafka, the RoundRobin assignment strategy distributes partitions more evenly across consumers in a consumer group.</p> <p>Here\u2019s how it works:</p> <ul> <li>It takes all partitions from all subscribed topics and then assigns them to consumers one by one in order.</li> <li>This process continues in a round-robin fashion until all partitions are assigned.</li> </ul> <p>For example, suppose you have:</p> <ul> <li>Two consumers: <code>C1</code> and <code>C2</code></li> <li>Two topics: <code>T1</code> and <code>T2</code></li> <li>Each topic has three partitions: <code>0</code>, <code>1</code>, and <code>2</code></li> </ul> <p>Under RoundRobin assignment:</p> <ul> <li> <p><code>C1</code> gets:</p> </li> <li> <p>Partitions <code>T1-0</code>, <code>T1-2</code>, and <code>T2-1</code></p> </li> <li> <p><code>C2</code> gets:</p> </li> <li> <p>Partitions <code>T1-1</code>, <code>T2-0</code>, and <code>T2-2</code></p> </li> </ul> <p>Because partitions are assigned one at a time across all topics, the distribution is more balanced.</p> <p>In general:</p> <ul> <li>If all consumers subscribe to the same set of topics (which is common), RoundRobin ensures that each consumer gets nearly the same number of partitions, with at most a one-partition difference.</li> <li>This makes it a good choice for maintaining load balance among consumers.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#sticky","title":"Sticky","text":"<p>The Sticky Assignor in Kafka is a partition assignment strategy designed to achieve two main objectives:</p> <ol> <li> <p>Balanced partition distribution:    It tries to assign partitions across consumers as evenly as possible, ensuring that each consumer handles roughly the same amount of data and workload.</p> </li> <li> <p>Minimal partition movement during rebalances:    When a rebalance occurs (for example, when a consumer joins or leaves the group), the Sticky Assignor tries to keep existing partition assignments unchanged as much as possible. This minimizes the need to move partitions between consumers, which reduces processing interruptions and improves stability.</p> </li> </ol>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#how-it-works","title":"How It Works","text":"<ul> <li>In the initial assignment, when consumers first join the group, the Sticky Assignor distributes partitions almost exactly like the RoundRobin assignor \u2014 evenly across consumers.</li> <li>On subsequent rebalances, instead of completely reshuffling all partitions, it keeps as many existing assignments as possible and only moves the partitions that need to be reassigned to maintain balance.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#example","title":"Example","text":"<p>If two consumers <code>C1</code> and <code>C2</code> are assigned partitions from a topic and <code>C3</code> joins the group:</p> <ul> <li>The Sticky Assignor will not reassign all partitions.</li> <li>It will only move a few partitions from <code>C1</code> and <code>C2</code> to <code>C3</code> so that the new distribution is still balanced but with minimal movement.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#when-consumers-subscribe-to-different-topics","title":"When Consumers Subscribe to Different Topics","text":"<p>If consumers in the same group subscribe to different topics, the Sticky Assignor performs even better than the RoundRobin Assignor. It achieves a more balanced assignment across topics while still minimizing unnecessary partition shuffling.</p>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#summary","title":"Summary","text":"Property Sticky Assignor Behavior Partition balance As balanced as RoundRobin Rebalance behavior Minimizes partition movement Efficiency Reduces overhead during rebalances Handling of different topic subscriptions More balanced than RoundRobin <p>In short, the Sticky Assignor provides both load balance and stability by keeping partition assignments steady across rebalances while ensuring even distribution.</p>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#cooperative-sticky-rebalancer","title":"Cooperative Sticky Rebalancer","text":"<p>Good question \u2014 the Sticky Assignor and the Cooperative Sticky Assignor are closely related, but the main difference lies in how they handle rebalancing and the level of disruption caused when consumers join or leave a group.</p> <p>Let\u2019s break it down clearly.</p>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#1-sticky-assignor-also-known-as-eager-sticky-assignor","title":"1. Sticky Assignor (also known as Eager Sticky Assignor)","text":"<p>Rebalance behavior: Eager (full) rebalance</p> <ul> <li>When a rebalance occurs (for example, a consumer joins or leaves the group), all consumers in the group stop consuming.</li> <li>All current partition assignments are revoked, and the group coordinator reassigns partitions to all consumers from scratch.</li> <li>Even though the assignor tries to keep the new assignment as \u201csticky\u201d as possible (minimizing partition movements), all consumers must pause until the rebalance completes.</li> <li>This can cause short processing interruptions and temporary underutilization of resources.</li> </ul> <p>Analogy: Think of it as reshuffling all cards in a deck every time a new player joins, even if most cards could have stayed where they were.</p> <p>Advantages:</p> <ul> <li>Simple to reason about.</li> <li>Ensures fully consistent assignments.</li> </ul> <p>Disadvantages:</p> <ul> <li>Causes temporary downtime during rebalances.</li> <li>Can lead to throughput drops when consumer group membership changes frequently.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#2-cooperative-sticky-assignor","title":"2. Cooperative Sticky Assignor","text":"<p>Rebalance behavior: Incremental (cooperative) rebalance</p> <ul> <li>The Cooperative Sticky Assignor builds on the Sticky Assignor but adds incremental rebalancing.</li> <li>Instead of revoking all assignments, it only revokes and reassigns the partitions that need to move.</li> <li>Other consumers keep consuming from their current partitions while the reassignment takes place.</li> <li>The rebalance process is incremental and non-disruptive \u2014 only affected consumers pause briefly.</li> </ul> <p>Analogy: If a new player joins, you only take a few cards from others to give them, without stopping the whole game.</p> <p>Advantages:</p> <ul> <li>Much faster and smoother rebalances.</li> <li>Reduces consumer downtime.</li> <li>More efficient in dynamic environments where consumers frequently join or leave.</li> </ul> <p>Disadvantages:</p> <ul> <li>Slightly more complex coordination process between the consumers and the group coordinator.</li> </ul>"},{"location":"streaming/kafka/24-Kafka_Partition_Assignment_Strategies/#3-summary-of-differences","title":"3. Summary of Differences","text":"Feature Sticky Assignor (Eager) Cooperative Sticky Assignor Rebalance Type Full (Eager) Incremental (Cooperative) Partition Revocation All partitions are revoked and reassigned Only necessary partitions are revoked Consumer Downtime All consumers stop during rebalance Only affected consumers pause briefly Rebalance Speed Slower Faster Stability Maintains balance and stickiness Maintains balance, stickiness, and minimizes disruption Kafka Version Introduced earlier (pre\u2013Kafka 2.4) Introduced in Kafka 2.4+ <p>In short:</p> <ul> <li>Sticky Assignor = Balanced + Minimal movement, but full pause on rebalance.</li> <li>Cooperative Sticky Assignor = Balanced + Minimal movement + No full pause (incremental and smoother rebalancing).</li> </ul>"},{"location":"streaming/kafka/25-Kafka_Commits_Offsets_Intro/","title":"Kafka Commits and Offsets","text":""},{"location":"streaming/kafka/25-Kafka_Commits_Offsets_Intro/#kafka-commits-and-offsets","title":"Kafka Commits and Offsets","text":"<p>When we call <code>poll</code> method, it returns all records from producer that have not yet been consumed. So we have a method of tracking which messages arent consumed yet.</p> <p>Kafka does not track acknowledgements from consumers but rather allows consumers to track their positions per partition of data using offsets. This is called an offset commit.</p> <p>Consumers commit the last message they have successfully processed and assume that every message before this has been processed.</p>"},{"location":"streaming/kafka/25-Kafka_Commits_Offsets_Intro/#how-does-a-consumer-commit-an-offset","title":"How does a consumer commit an offset?","text":"<p>It sends a message to Kafka that will update a special topic <code>_consumer_offsets</code> for each partition.</p> <p>When a consumer crashes, a rebalance will be triggered and inorder for consumer to know where to pick up from when it starts, it will read data from latest committed offset.</p> <p>If the committed offset is smaller than offset of latest message in partition, then we will end up reading messages twice (duplicated)</p> <p>If the committed offset is larger than the offset of the latest message in partition, then we will end up missing out on lot of data.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/","title":"Kafka Types of Commits","text":""},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#types-of-commits-in-consumers","title":"Types of Commits in Consumers","text":""},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#automatic-commits","title":"Automatic Commits","text":"<p>By default, Kafka automatically commits offsets every five seconds (controlled by <code>auto.commit.interval.ms</code>). This means that every time the consumer calls <code>poll()</code>, Kafka checks whether it\u2019s time to commit. If it is, it commits the latest offset that was returned in the last <code>poll()</code> call.</p> <p>Here\u2019s what this means in practice:</p> <ul> <li>When auto-commit is enabled, Kafka automatically keeps track of which records your consumer has read.</li> <li>If the consumer crashes three seconds after the last commit, when it restarts or another consumer takes over, it will begin reading from the last committed offset\u2014three seconds old.</li> <li>As a result, all messages read during those three seconds will be processed again (duplicate processing).</li> <li>You can reduce this duplication window by committing more frequently, but you can\u2019t eliminate duplicates entirely.</li> </ul> <p>A key point is that Kafka commits offsets for messages returned by the last <code>poll()</code>, not for messages that were actually processed. So if you call <code>poll()</code> again before processing all previous messages, or if an exception causes you to exit before processing them, some offsets may be committed even though the messages were not handled \u2014 leading to data loss.</p> <p>Also note:</p> <ul> <li>When <code>close()</code> is called, it automatically commits offsets as well.</li> <li>Automatic commits are convenient for simple use cases but don\u2019t give enough control for systems that must avoid duplicates or data loss.</li> </ul> <p>In summary:</p> <ul> <li>Auto-commit = convenient, but risk of duplicates or unprocessed messages.</li> <li>Manual commit = more control, better for reliable or exactly-once processing.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#commit-current-offset","title":"Commit Current Offset","text":""},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#1-why-manual-offset-control-is-needed","title":"1. Why Manual Offset Control Is Needed","text":"<p>By default, Kafka commits offsets automatically at regular intervals:</p> <pre><code>enable.auto.commit=true\nauto.commit.interval.ms=5000\n</code></pre> <p>This means the consumer commits offsets every 5 seconds, regardless of whether your application has finished processing those messages. This behavior can cause two major problems:</p> <ul> <li>Duplicate processing: If the consumer crashes before the next commit, it reprocesses the same messages after restart.</li> <li>Data loss: If offsets are committed before processing completes, some messages may never be processed.</li> </ul> <p>To gain control over when offsets are committed and to avoid these issues, developers usually disable automatic commits.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#2-disabling-automatic-commits","title":"2. Disabling Automatic Commits","text":"<p>To switch to manual commits, you set:</p> <pre><code>enable.auto.commit=false\n</code></pre> <p>This means Kafka will only commit offsets when your application explicitly tells it to. You can then commit offsets using one of two APIs:</p> <ol> <li><code>commitSync()</code> \u2014 Synchronous, reliable, blocking commit.</li> <li><code>commitAsync()</code> \u2014 Asynchronous, faster, but less reliable.</li> </ol>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#3-how-commitsync-works","title":"3. How <code>commitSync()</code> Works","text":"<p><code>commitSync()</code> is the simplest and most reliable method for manually committing offsets.</p> <p>Example:</p> <pre><code>while (true) {\n    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n\n    for (ConsumerRecord&lt;String, String&gt; record : records) {\n        process(record); // your message processing logic\n    }\n\n    consumer.commitSync(); // commits after processing\n}\n</code></pre>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#internal-behavior","title":"Internal Behavior:","text":"<ul> <li>Kafka keeps track of the latest offset processed for each partition.</li> <li>When you call <code>commitSync()</code>, the consumer sends a <code>CommitOffsetRequest</code> to the broker.</li> <li>The broker writes the committed offsets to a special internal topic called <code>__consumer_offsets</code>.</li> <li>The API blocks until Kafka confirms that the offsets have been successfully stored.</li> </ul> <p>If the commit fails due to a network issue or coordinator error, <code>commitSync()</code> throws an exception, allowing you to handle it or retry.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#4-where-offsets-are-stored","title":"4. Where Offsets Are Stored","text":"<p>Committed offsets are stored in Kafka\u2019s internal topic named:</p> <pre><code>__consumer_offsets\n</code></pre> <p>This topic holds records that map:</p> <ul> <li>Consumer group ID</li> <li>Topic-partition</li> <li>Committed offset</li> <li>Commit timestamp</li> </ul> <p>When a consumer restarts or a rebalance occurs, it reads its committed offsets from this topic and resumes consumption from the last committed position.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#5-important-semantics-of-commitsync","title":"5. Important Semantics of <code>commitSync()</code>","text":"<p><code>commitSync()</code> always commits the latest offsets returned by the most recent <code>poll()</code>.</p> <p>Therefore:</p> <ul> <li>If you call <code>commitSync()</code> before finishing processing the batch, you risk losing messages (Kafka assumes they\u2019re processed even if they aren\u2019t).</li> <li>If you call <code>commitSync()</code> after processing all messages, you may reprocess some in the event of a crash before commit, but no messages will be lost.</li> </ul> <p>This leads to an important trade-off:</p> Strategy Behavior Risk Commit before processing May lose messages Unsafe Commit after processing May duplicate messages Safe and preferred"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#6-trade-offs-in-commit-timing","title":"6. Trade-offs in Commit Timing","text":""},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#option-1-commit-after-each-batch","title":"Option 1: Commit after each batch","text":"<ul> <li>Reliable, easy to implement</li> <li>Slight performance overhead due to blocking commit call</li> <li>Recommended for most use cases</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#option-2-commit-after-each-message","title":"Option 2: Commit after each message","text":"<ul> <li>Maximum control, minimal data loss</li> <li>Very slow because every commit is a network operation</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#option-3-commit-periodically-or-based-on-count","title":"Option 3: Commit periodically or based on count","text":"<ul> <li>Balanced approach between performance and accuracy</li> </ul> <p>Example:</p> <pre><code>int processedCount = 0;\n\nwhile (true) {\n    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n    for (ConsumerRecord&lt;String, String&gt; record : records) {\n        process(record);\n        processedCount++;\n    }\n\n    if (processedCount &gt;= 1000) {\n        consumer.commitSync();\n        processedCount = 0;\n    }\n}\n</code></pre>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#7-failure-scenarios","title":"7. Failure Scenarios","text":"Scenario Result Impact Consumer crashes before commit Reprocesses the last batch Duplicates Consumer crashes after commit but before processing finishes Messages skipped Data loss Network failure during commit Commit fails and throws exception Handle or retry Rebalance during processing Uncommitted data reprocessed Duplicates"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#8-handling-rebalances-safely","title":"8. Handling Rebalances Safely","text":"<p>During a rebalance, Kafka reassigns partitions between consumers. If you lose ownership of a partition without committing, another consumer may reprocess uncommitted records.</p> <p>To handle this safely, use a <code>ConsumerRebalanceListener</code> and commit offsets in <code>onPartitionsRevoked()</code>:</p> <pre><code>consumer.subscribe(Collections.singletonList(\"topic\"), new ConsumerRebalanceListener() {\n    @Override\n    public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {\n        consumer.commitSync(currentOffsets); // commit before losing partitions\n    }\n\n    @Override\n    public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {\n        // initialization logic if needed\n    }\n});\n</code></pre> <p>This ensures all processed offsets are committed before partition ownership changes.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#9-best-practices-for-using-commitsync","title":"9. Best Practices for Using <code>commitSync()</code>","text":"<ol> <li>Disable auto-commit</li> </ol> <p><pre><code>enable.auto.commit=false\n</code></pre> 2. Commit only after successful processing    This ensures no data loss. 3. Use try-catch around commits    Handle transient errors gracefully. 4. Commit on rebalance    Prevents duplication after group reassignments. 5. Combine with <code>commitAsync()</code> for performance    Use async commits during normal operation and a final sync commit during shutdown. 6. Make message processing idempotent    Ensure reprocessing the same message does not cause incorrect results (for example, using upserts or deduplication keys).</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#10-combined-commit-strategy-example","title":"10. Combined Commit Strategy Example","text":"<p>A common production pattern is to combine <code>commitAsync()</code> and <code>commitSync()</code>:</p> <pre><code>try {\n    while (running) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n        for (ConsumerRecord&lt;String, String&gt; record : records) {\n            process(record);\n        }\n\n        consumer.commitAsync(); // fast, non-blocking commit\n    }\n} catch (Exception e) {\n    // handle error or retry logic\n} finally {\n    try {\n        consumer.commitSync(); // final blocking commit for safety\n    } finally {\n        consumer.close();\n    }\n}\n</code></pre> <p>This pattern achieves a balance of:</p> <ul> <li>High performance during normal operation (asynchronous commits)</li> <li>Reliability during shutdown or rebalance (synchronous commit)</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#11-summary","title":"11. Summary","text":"Feature Auto Commit Manual Commit (<code>commitSync()</code>) Control Low High Data Safety Medium High Duplicates Possible Possible but manageable Data Loss Possible Avoidable Commit Type Timed Developer-controlled Suitable For Simple consumers Production-grade consumers <p>In summary, disabling auto-commit and using <code>commitSync()</code> after processing each batch gives you strong guarantees against data loss and predictable behavior. While it introduces a bit more complexity and latency, it is the most reliable way to manage consumer offsets in production systems.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#asynchronous-commit","title":"Asynchronous Commit","text":"<p>Below is a detailed, emoji-free explanation of the asynchronous commit API (<code>commitAsync()</code>), its behavior, trade-offs, error-handling patterns, and practical recommendations for production use.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#what-commitasync-does","title":"What <code>commitAsync()</code> does","text":"<p><code>commitAsync()</code> sends the commit request to the broker and does not block waiting for a response. This improves throughput because the consumer thread can continue processing records while offset commits are in flight.</p> <p>There are two common forms:</p> <ol> <li> <p><code>consumer.commitAsync();</code>    Sends the offsets the consumer is tracking (the latest offsets returned by the last <code>poll()</code>).</p> </li> <li> <p><code>consumer.commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback);</code>    Sends a specific set of offsets and supplies a callback that Kafka will call when the broker responds (success or error).</p> </li> </ol>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#benefits","title":"Benefits","text":"<ul> <li>Higher throughput because commits are non-blocking.</li> <li>Lower latency in the consumer loop; the consumer does not wait for commit round-trip.</li> <li>Useful for high-volume consumers where commit latency would otherwise throttle processing.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#drawbacks-and-important-caveats","title":"Drawbacks and important caveats","text":"<ol> <li> <p>No built-in retry or blocking guarantee    If a commit fails (network error, coordinator error), <code>commitAsync()</code> will report the error in the callback but it will not retry automatically. If you need guaranteed persistence, you must handle retries yourself or use <code>commitSync()</code> at critical points.</p> </li> <li> <p>Out-of-order callback completion    If you call <code>commitAsync()</code> multiple times in rapid succession, callbacks can complete out-of-order. That means a later commit might succeed first and an earlier commit might fail afterward; if you react to failures by retrying, be careful not to regress to an earlier offset.</p> </li> <li> <p>Potential for lost commit acknowledgement    Because you don't block, the application may crash before the broker processes the commit. That is why a final <code>commitSync()</code> on shutdown is recommended.</p> </li> <li> <p>Rebalance handling    Commits in-flight during a rebalance may be lost or applied after partition reassignment. Use a rebalance listener to commit offsets synchronously in <code>onPartitionsRevoked()</code> to make sure processed offsets are stored before losing partition ownership.</p> </li> </ol>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#typical-usage-patterns","title":"Typical usage patterns","text":""},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#1-fast-path-with-async-commits-safe-shutdown-with-sync-commit","title":"1) Fast path with async commits, safe shutdown with sync commit","text":"<p>This is a common production pattern: use <code>commitAsync()</code> for normal throughput and call <code>commitSync()</code> in <code>finally</code> to guarantee the last offsets are committed.</p> <pre><code>try {\n    while (running) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n        for (ConsumerRecord&lt;String, String&gt; record : records) {\n            process(record);\n        }\n        consumer.commitAsync(); // non-blocking\n    }\n} catch (Exception e) {\n    // log or handle processing error\n} finally {\n    try {\n        consumer.commitSync(); // final, blocking commit to ensure offsets are stored\n    } finally {\n        consumer.close();\n    }\n}\n</code></pre> <p>Rationale: <code>commitAsync()</code> gives throughput; the final <code>commitSync()</code> ensures the offsets you last processed are persisted before exit.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#2-async-commit-with-callback-to-logfallback-retry","title":"2) Async commit with callback to log/fallback retry","text":"<p>Use a callback to detect commit failures and optionally retry or record telemetry. Avoid blind retries that might create ordering problems.</p> <pre><code>consumer.commitAsync((offsets, exception) -&gt; {\n    if (exception != null) {\n        // handle failure: log, increment metric, or schedule a retry\n        System.err.printf(\"Commit failed for offsets %s: %s%n\", offsets, exception.getMessage());\n        // Optionally: retry once using commitSync() or a bounded retry mechanism\n    }\n});\n</code></pre> <p>If you attempt retries, prefer <code>commitSync()</code> for the retry attempt (or a bounded number of async retries with careful ordering control).</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#3-commit-specific-offsets-application-managed-offsets","title":"3) Commit specific offsets (application-managed offsets)","text":"<p>If your application uses its own tracking (for example, when using manual per-message acknowledgements or when integrating with external storage), pass explicit offsets:</p> <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();\noffsets.put(partition, new OffsetAndMetadata(offsetToCommit, \"metadata\"));\n\nconsumer.commitAsync(offsets, (map, ex) -&gt; {\n    if (ex != null) {\n        // handle failure\n    }\n});\n</code></pre> <p>Note: commit the offset of the next message to process (i.e., last-processed-offset + 1) to avoid reprocessing the same message on restart.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#rebalance-integration","title":"Rebalance integration","text":"<p>Always commit offsets in <code>onPartitionsRevoked()</code> using a synchronous commit. This prevents losing processed offsets when partitions are taken away.</p> <pre><code>consumer.subscribe(topics, new ConsumerRebalanceListener() {\n    @Override\n    public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {\n        // commit the offsets you have recorded for these partitions\n        consumer.commitSync(currentOffsets);\n    }\n\n    @Override\n    public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {\n        // restore offsets if needed\n    }\n});\n</code></pre> <p>Rationale: <code>onPartitionsRevoked()</code> is called before the consumer loses ownership. Synchronous commit here guarantees offsets are recorded before the rebalance completes.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#error-handling-recommendations","title":"Error handling recommendations","text":"<ul> <li>Wrap commits in try/catch when using <code>commitSync()</code> and react to <code>CommitFailedException</code> or transient errors.</li> <li> <p>For <code>commitAsync()</code> callbacks:</p> </li> <li> <p>Log failures and metrics.</p> </li> <li>Consider a bounded retry using <code>commitSync()</code> when appropriate (for example, in the callback of a failed commit you might attempt one <code>commitSync()</code> to ensure persistence).</li> <li>Avoid infinite retry loops or retries that block the main processing thread, which would defeat the purpose of async commits.</li> <li>On shutdown, always call <code>commitSync()</code> in a <code>finally</code> block to ensure the last offsets are committed.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#throughput-vs-duplicate-window-trade-off","title":"Throughput vs duplicate-window trade-off","text":"<ul> <li>Committing less frequently increases throughput but widens the window for potential duplicate processing after a rebalance or crash.</li> <li>Committing more frequently reduces duplicates but increases commit overhead and may reduce throughput.</li> <li> <p>Use application-specific metrics and load tests to determine the right commit frequency. Common rules:</p> </li> <li> <p>Commit every N messages (e.g., 500\u201310,000) for high-throughput consumers.</p> </li> <li>Or commit every T seconds (e.g., 5\u201360s) depending on acceptable duplicate window and latency requirements.</li> <li>Combine async commits for performance with periodic sync commits for safety if you require stronger guarantees.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#idempotence-and-external-side-effects","title":"Idempotence and external side effects","text":"<p>Because duplicates are possible even with careful commits, design your processing to be idempotent or to tolerate retries:</p> <ul> <li>Use upserts instead of inserts where possible.</li> <li>Deduplicate using a unique message ID stored in a database/cache.</li> <li>Make downstream systems tolerant to repeated messages.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#when-to-prefer-commitasync-vs-commitsync","title":"When to prefer <code>commitAsync()</code> vs <code>commitSync()</code>","text":"<ul> <li> <p><code>commitAsync()</code>:</p> </li> <li> <p>Preferred for steady-state, high-throughput processing.</p> </li> <li>Use with robust monitoring and a final <code>commitSync()</code> on shutdown.</li> <li> <p>Use callbacks to observe failures and increment metrics.</p> </li> <li> <p><code>commitSync()</code>:</p> </li> <li> <p>Preferred when you need a strong guarantee that offsets were committed at a specific point (e.g., before releasing partitions, during controlled shutdown, or after critical state updates).</p> </li> <li>Use in <code>onPartitionsRevoked()</code> and at application exit.</li> </ul> <p>Combining both (<code>commitAsync()</code> normally, <code>commitSync()</code> on rebalance/shutdown) yields a practical and commonly used balance between performance and safety.</p>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#other-considerations","title":"Other considerations","text":"<ul> <li>Offset metadata: You can attach small metadata strings to commits (for debugging or bookkeeping).</li> <li>Consumer group scale: With many consumers, commit frequency affects the <code>__consumer_offsets</code> topic size and broker load. Monitor cluster health.</li> <li>Transactions / exactly-once: If you need strong exactly-once semantics across producers and consumers, consider Kafka transactions (producer-side) or higher-level frameworks (Kafka Streams). Those are separate mechanisms beyond simple consumer offset commits.</li> </ul>"},{"location":"streaming/kafka/26-Kafka_Types_Of_Commits/#short-checklist-for-production-consumers","title":"Short checklist for production consumers","text":"<ol> <li>Disable auto-commit: <code>enable.auto.commit=false</code>.</li> <li>Use <code>commitAsync()</code> during normal processing for throughput.</li> <li>Use a callback to observe commit failures and record metrics.</li> <li>Commit synchronously in <code>onPartitionsRevoked()</code> and in <code>finally</code> on shutdown.</li> <li>Keep processing idempotent or implement deduplication.</li> <li>Tune commit frequency based on acceptable duplicate window and throughput goals.</li> <li>Monitor <code>__consumer_offsets</code> topic, commit latency, and commit failure rates.</li> </ol>"},{"location":"streaming/kafka/27-Kafka_Rebalance_Listeners/","title":"Kafka Rebalance Listeners","text":""},{"location":"streaming/kafka/27-Kafka_Rebalance_Listeners/#rebalance-listeners-in-kafka","title":"Rebalance Listeners in Kafka","text":"<p>If we know that our consumer is about to lose ownership of a partition, we want to commit offsets of the last event you've processed.</p> <p>The consumer API allows you to run your own code when partitions are added or removed from the consumer. We need to do this by passing <code>ConsumerRebalanceListener</code> when calling the <code>subscribe</code> method.</p> <p>It has three methods to implement:</p> <p><code>public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions)</code></p> <p>Called after partitions have been reassigned to the consumer but before the consumer starts consuming messages. This is where you prepare or load any state that you want to use with the partition.</p> <p>Any steps taken here should be guaranteed to return within <code>max.poll.timeout.ms</code></p> <p><code>public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions)</code></p> <p>Called when the consumer has to give up partitions that it previously owned - either when a rebalance happens or consumer closes.</p> <p>When an eager rebalancing algorithm is used, this method is invoked before rebalancing starts and after consumer has consumed last batch of messages. This is because in eager algorithm, we need to reassign all partitions and hence before it happens offsets must be committed.</p> <p>If a cooperative rebalancing algorithm is used, this method is invoked at the end of the rebalance with just the subset of partitions that the consumer has to give up.</p> <p><code>public void onPartitionsLost(Collection&lt;TopicPartition&gt; partitions)</code></p> <p>Called in exceptional cases where the partitions were assigned to other consumers without first being revoked by the rebalance algorithm.</p> <p>This is where we clean up any state or resources that are used with these partitions. Note that this has to be done with caution because the new owner of the partitions may have already saved its own state.</p> <p></p>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/","title":"Kafka Consuming Records with Specified Offset","text":""},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#kafka-consuming-records-with-specific-offset","title":"Kafka : Consuming Records with Specific Offset","text":"<p>Sure \u2014 let\u2019s go through this step by step and unpack the meaning of that paragraph in detail.</p>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#1-default-consumer-behavior","title":"1. Default Consumer Behavior","text":"<p>When you use the Kafka Consumer API, you usually call:</p> <pre><code>consumer.poll(Duration.ofMillis(100));\n</code></pre> <p>Each time you call <code>poll()</code>, Kafka starts reading messages from the last committed offset for each partition that your consumer is assigned to.</p> <ul> <li>The offset is a numerical position within a Kafka partition.</li> <li>The committed offset is the last message position your consumer group marked as \u201cprocessed.\u201d</li> </ul> <p>So, by default, Kafka resumes consumption exactly where it left off \u2014 ensuring no messages are skipped or reprocessed.</p>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#2-starting-from-a-different-offset","title":"2. Starting from a Different Offset","text":"<p>Sometimes, you might not want to start reading from the last committed position. You may want to:</p> <ul> <li>Reprocess all messages from the beginning.</li> <li>Skip to the most recent messages only.</li> <li>Or start at an exact offset you specify.</li> </ul> <p>Kafka provides APIs that allow you to manually control where the consumer begins reading.</p>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#3-seektobeginning","title":"3. <code>seekToBeginning()</code>","text":"<p>If you want to read all messages in a partition \u2014 from the very first offset \u2014 you can use:</p> <pre><code>consumer.seekToBeginning(Collection&lt;TopicPartition&gt; partitions);\n</code></pre> <p>What happens:</p> <ul> <li>This tells Kafka to move the consumer\u2019s position to the earliest offset available in the log for each partition in the list.</li> <li>On the next <code>poll()</code>, Kafka will start returning messages from the oldest record onward.</li> </ul> <p>Use case example:</p> <ul> <li>You\u2019re debugging or replaying historical data.</li> <li>You want to rebuild a downstream database or reprocess all records.</li> </ul>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#4-seektoend","title":"4. <code>seekToEnd()</code>","text":"<p>If you only care about new messages that arrive after you start consuming, use:</p> <pre><code>consumer.seekToEnd(Collection&lt;TopicPartition&gt; partitions);\n</code></pre> <p>What happens:</p> <ul> <li>This moves the consumer\u2019s position to the latest offset in each partition.</li> <li>On the next <code>poll()</code>, Kafka will start returning only new messages published after this point.</li> </ul> <p>Use case example:</p> <ul> <li>You\u2019re monitoring live events and only want real-time updates, not historical data.</li> </ul>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#5-seek-moving-to-a-specific-offset","title":"5. <code>seek()</code> \u2014 Moving to a Specific Offset","text":"<p>For even more control, you can use:</p> <pre><code>consumer.seek(TopicPartition partition, long offset);\n</code></pre> <p>This allows you to position the consumer exactly at a specific offset.</p> <p>What happens:</p> <ul> <li>You manually tell Kafka which record position to start from.</li> <li>On the next <code>poll()</code>, consumption begins from that offset.</li> </ul> <p>Use case examples:</p> <ol> <li>Recovery scenario \u2014 if your application crashed while writing data to a file, you can restart it and <code>seek()</code> back to the last successfully written offset.</li> <li>Skipping stale data \u2014 if you detect that you\u2019re falling behind, you can <code>seek()</code> ahead to a newer offset and catch up faster.</li> <li>Time-based replay \u2014 with the help of <code>offsetsForTimes()</code>, you can find the offset corresponding to a timestamp, then use <code>seek()</code> to start from that point.</li> </ol>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#6-when-these-apis-are-useful","title":"6. When These APIs Are Useful","text":"<p>These APIs are particularly useful when:</p> <ul> <li>You manage offsets manually instead of relying on Kafka\u2019s automatic commits.</li> <li>You need custom recovery or replay logic.</li> <li>You are building tools for data reprocessing, debugging, or time-travel (reading old events).</li> </ul>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#7-important-note","title":"7. Important Note","text":"<p>After you use <code>seekToBeginning()</code>, <code>seekToEnd()</code>, or <code>seek()</code>, the new position takes effect only for the next call to <code>poll()</code>. Kafka will not commit this new position automatically \u2014 you must commit offsets yourself (using <code>commitSync()</code> or <code>commitAsync()</code>) if you want Kafka to remember your new starting point for the next session.</p>"},{"location":"streaming/kafka/28-Kafka_Consuming_Records_With_Spec_Offset/#summary","title":"Summary","text":"Method What It Does When to Use <code>seekToBeginning()</code> Start reading from the earliest offset Replay all data <code>seekToEnd()</code> Skip to the latest offset Only process new messages <code>seek()</code> Jump to a specific offset Custom replay/recovery logic"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/","title":"Kafka Exiting Consumers and Poll Loop","text":""},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#kafka-exiting-consumer-poll-loop-safely","title":"Kafka : Exiting Consumer Poll Loop Safely","text":""},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#1-the-problem-infinite-poll-loop","title":"1. The Problem: Infinite poll() Loop","text":"<p>A typical Kafka consumer continuously runs in an infinite loop like this:</p> <pre><code>try {\n    while (true) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n        for (ConsumerRecord&lt;String, String&gt; record : records) {\n            process(record);\n        }\n    }\n} finally {\n    consumer.close();\n}\n</code></pre> <p>The consumer must poll() continuously to:</p> <ul> <li>Fetch new records,</li> <li>Send heartbeats to Kafka (so Kafka knows the consumer is alive), and</li> <li>Maintain its membership in the consumer group.</li> </ul> <p>But this raises a question: How do you stop the consumer safely when you want to shut down your application?</p> <p>If you just break the loop or kill the thread abruptly:</p> <ul> <li>You might lose messages that were fetched but not yet processed or committed.</li> <li>Kafka won\u2019t immediately know this consumer is gone, so it will take a session timeout (typically 10 seconds or more) before Kafka rebalances partitions to another consumer.</li> </ul>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#2-the-challenge-poll-may-be-waiting","title":"2. The Challenge: poll() May Be Waiting","text":"<p>The <code>poll()</code> call can block for a certain duration (for example, up to the timeout specified in <code>poll(Duration.ofMillis(x))</code>).</p> <p>So if you just try to stop the thread directly, it might still be waiting for <code>poll()</code> to return \u2014 meaning your shutdown could hang for several seconds.</p> <p>You need a way to interrupt the poll safely.</p>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#3-the-solution-consumerwakeup","title":"3. The Solution: <code>consumer.wakeup()</code>","text":"<p>Kafka provides a special mechanism for this:</p> <pre><code>consumer.wakeup();\n</code></pre> <p>This is the only thread-safe method in the Kafka Consumer API. You can safely call it from another thread, even while the main thread is blocked in <code>poll()</code>.</p>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#what-happens-when-wakeup-is-called","title":"What happens when <code>wakeup()</code> is called","text":"<ul> <li>If the consumer is currently waiting inside <code>poll()</code>, the method immediately causes <code>poll()</code> to exit and throw a <code>WakeupException</code>.</li> <li>If the consumer is not inside poll() at that moment, the next time <code>poll()</code> is called, it will throw <code>WakeupException</code>.</li> </ul> <p>This gives you a clean, predictable way to break out of the loop.</p>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#4-using-a-shutdown-hook","title":"4. Using a Shutdown Hook","text":"<p>If your consumer runs in the main thread, you can use a Shutdown Hook (a special JVM mechanism that runs code before the process terminates).</p> <p>Example:</p> <pre><code>final Thread mainThread = Thread.currentThread();\n\nRuntime.getRuntime().addShutdownHook(new Thread() {\n    public void run() {\n        System.out.println(\"Detected shutdown, calling consumer.wakeup()...\");\n        consumer.wakeup();\n        try {\n            mainThread.join();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n});\n</code></pre> <p>This means:</p> <ul> <li>When you stop your application (for example, by pressing <code>Ctrl+C</code>),   the shutdown hook runs in a separate thread.</li> <li>That thread calls <code>consumer.wakeup()</code> to interrupt the main thread\u2019s poll loop.</li> </ul>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#5-handling-the-wakeupexception","title":"5. Handling the <code>WakeupException</code>","text":"<p>Once <code>wakeup()</code> is called, the consumer\u2019s next <code>poll()</code> will throw a <code>WakeupException</code>. You typically use this pattern:</p> <pre><code>try {\n    while (true) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n        for (ConsumerRecord&lt;String, String&gt; record : records) {\n            process(record);\n        }\n    }\n} catch (WakeupException e) {\n    // This is expected during shutdown, so no action needed.\n} finally {\n    consumer.close();\n    System.out.println(\"Consumer closed.\");\n}\n</code></pre> <p>Key points:</p> <ul> <li>The <code>WakeupException</code> is not an error \u2014 it\u2019s just Kafka\u2019s way of telling you that <code>poll()</code> was interrupted.</li> <li>You don\u2019t need to log or rethrow it.</li> <li>But before you exit, you must call <code>consumer.close()</code>.</li> </ul>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#6-why-consumerclose-is-important","title":"6. Why <code>consumer.close()</code> Is Important","text":"<p>Calling <code>close()</code> ensures:</p> <ol> <li>Offsets are committed if <code>enable.auto.commit=true</code> or if you have uncommitted offsets and auto-commit on close is enabled.</li> <li>The consumer sends a \u201cleave group\u201d message to the group coordinator.</li> <li>Kafka immediately triggers a rebalance, redistributing this consumer\u2019s partitions to others in the same group.</li> </ol> <p>If you skip <code>close()</code>:</p> <ul> <li>Kafka will think the consumer might still be alive.</li> <li>It will wait for the session timeout to expire (default 10 seconds or longer) before rebalancing.</li> <li>This delays recovery and causes processing downtime for other consumers.</li> </ul>"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#7-summary","title":"7. Summary","text":"Step Action Purpose 1 Consumer runs an infinite <code>poll()</code> loop To fetch data and send heartbeats 2 Add a Shutdown Hook To catch application termination 3 Call <code>consumer.wakeup()</code> from another thread Safely interrupt <code>poll()</code> 4 Catch <code>WakeupException</code> Exit the loop gracefully 5 Call <code>consumer.close()</code> Commit offsets and trigger rebalance"},{"location":"streaming/kafka/29-Kafka_Exiting_Consumer_Poll_Loop/#8-why-this-matters","title":"8. Why This Matters","text":"<p>A clean shutdown is essential in Kafka because:</p> <ul> <li>It prevents data duplication or loss.</li> <li>It avoids unnecessary delays in rebalancing.</li> <li>It keeps your consumer group\u2019s state consistent.</li> </ul> <p>In production systems, this shutdown pattern is standard \u2014 you\u2019ll find it in nearly all well-designed Kafka consumer implementations.</p>"},{"location":"streaming/kafka/30-Kafka_Deserialisers/","title":"Kafka Deserializers","text":""},{"location":"streaming/kafka/30-Kafka_Deserialisers/#deserializers-in-kafka","title":"Deserializers in Kafka","text":"<p>This passage explains the importance of using matching serializers and deserializers in Kafka.</p> <p>When a producer sends data to Kafka, it must first serialize that data \u2014 convert it from an object (like a number or string) into a sequence of bytes that Kafka can store and transmit. On the consumer side, the same data must be deserialized \u2014 converted back from bytes into a usable form.</p> <p>If the serializer and deserializer don\u2019t match, the consumer will not be able to correctly interpret the data. For example, if you use <code>IntSerializer</code> to send integers but the consumer uses <code>StringDeserializer</code>, the consumer will misread the byte data and likely fail with an error.</p> <p>As a result, developers must know which serializers were used for each Kafka topic and ensure that all consumers of that topic use the corresponding deserializers.</p> <p>Using a Schema Registry with Avro serialization helps enforce this consistency. The Avro serializer and deserializer rely on a defined schema that ensures all data in a topic follows the same structure. If a producer tries to write incompatible data or if a consumer uses the wrong schema, Kafka will throw clear errors.</p> <p>This makes debugging much easier and prevents data corruption or misinterpretation.</p>"},{"location":"streaming/kafka/30-Kafka_Deserialisers/#custom-deserializers-in-kafka","title":"Custom Deserializers in Kafka","text":"<p>Check pg 184 for more details.</p>"},{"location":"streaming/kafka/30-Kafka_Deserialisers/#using-avro-deserialization-with-consumer","title":"Using Avro Deserialization with Consumer","text":"<p>1 - Use KafkaAvroDeserializer to deserialize Avro messages 2 - To store schemas 3 - Define type for the record value   4 - Instance of customer</p>"},{"location":"streaming/kafka/31-Kafka_Standalone_Consumers/","title":"Standalone Consumers : Consumer without a Group","text":""},{"location":"streaming/kafka/31-Kafka_Standalone_Consumers/#core-concepts","title":"Core concepts","text":"<p>Serializer/deserializer compatibility (you already know this): consumers must be able to interpret the bytes written by producers. That means matching serializers/deserializers and\u2014if you use a schema system like Avro + Schema Registry\u2014matching schemas.</p> <p>Consumer groups and subscribe():</p> <ul> <li>When consumers call <code>subscribe(topics, on_assign=...)</code>, they request membership in a consumer group identified by <code>group.id</code>.</li> <li>Kafka\u2019s group coordinator assigns partitions of the subscribed topics among active group members. This assignment is automatic and rebalanced whenever group membership or subscription metadata changes.</li> <li>Rebalancing is useful when you want the cluster to adapt automatically to consumer joins/leaves or topic partition changes (scaling).</li> <li>With <code>subscribe</code>, consumers participate in heartbeating and session management. If a consumer fails or is slow, partitions are reassigned to other group members.</li> <li>You typically commit offsets (either automatically with <code>enable.auto.commit</code> or manually) and Kafka stores those offsets under the consumer group. On restart, a consumer in the same group resumes from committed offsets.</li> </ul> <p>Manual assignment and assign():</p> <ul> <li>With <code>assign([TopicPartition(...)])</code> you bypass the group coordinator for assignment decisions \u2014 the consumer will not join the consumer group for partition assignment purposes. You explicitly tell the consumer which partitions to read.</li> <li>This is appropriate when you need deterministic, static reads: e.g., a single consumer that must read all partitions, a tool that reads a specific partition for replay, or a consumer that must concurrently read certain partitions for specialized processing.</li> <li>Assigning partitions gives you more control but removes automatic failover and rebalancing. If another process needs to take over, you must implement that orchestration yourself.</li> <li>Important: if you want to commit offsets to Kafka (so offsets are persisted in <code>__consumer_offsets</code>), you still need a <code>group.id</code> configured. Committing offsets without a group id is either impossible or meaningless for group-managed offsets. (Storing offsets externally is an alternative.)</li> </ul> <p>Key trade-offs</p> <ul> <li>Subscribe (group-managed): +automatic scaling and failover, easier maintenance; \u2212possible rebalances which add pause time and complexity (rebalance listeners, offset commit timing).</li> <li>Assign (manual): +deterministic control, no rebalances; \u2212no automatic recovery, you must coordinate failover and partition ownership yourself.</li> </ul> <p>Common operational concerns</p> <ul> <li>Rebalance latency: large processing during on-revoke/on-assign handlers can prolong rebalances. Keep handlers quick.</li> <li>Heartbeats and session timeouts: tune <code>session.timeout.ms</code>, <code>heartbeat.interval.ms</code> to avoid false consumer group failures or too slow failure detection.</li> <li>Offset commit semantics: commit after processing a message (or batch) to ensure at-least-once semantics; track transactions if you need exactly-once across systems.</li> <li>If you use manual assignment and commit offsets, ensure <code>group.id</code> is set and coordinate offset ownership (avoid two processes committing offsets for the same group/partition pair).</li> </ul>"},{"location":"streaming/kafka/31-Kafka_Standalone_Consumers/#example-1-consumer-group-subscribe-with-manual-offset-commits","title":"Example 1 \u2014 Consumer group (subscribe) with manual offset commits","text":"<p>This consumer joins a group, is assigned partitions automatically, processes records, and commits offsets manually after successful processing.</p> <pre><code># consumer_subscribe.py\nfrom confluent_kafka import Consumer, KafkaError, KafkaException\nimport signal\nimport sys\nimport time\n\nTOPIC = 'my_topic'\nGROUP_ID = 'my_consumer_group'\nBOOTSTRAP_SERVERS = 'localhost:9092'\n\nrunning = True\n\ndef shutdown(signum, frame):\n    global running\n    running = False\n\nsignal.signal(signal.SIGINT, shutdown)\nsignal.signal(signal.SIGTERM, shutdown)\n\nconf = {\n    'bootstrap.servers': BOOTSTRAP_SERVERS,\n    'group.id': GROUP_ID,\n    'auto.offset.reset': 'earliest',   # or 'latest'\n    'enable.auto.commit': False,       # we'll commit manually\n    'session.timeout.ms': 10000,\n}\n\nconsumer = Consumer(conf)\n\ndef on_assign(consumer, partitions):\n    print(\"Assigned partitions:\", partitions)\n    # Optionally seek to specific offsets here\n    # for p in partitions:\n    #     p.offset = OFFSET_TO_START\n    # consumer.assign(partitions)\n\ndef on_revoke(consumer, partitions):\n    print(\"Revoked partitions:\", partitions)\n    # Called before rebalance; flush state if needed\n\nconsumer.subscribe([TOPIC], on_assign=on_assign, on_revoke=on_revoke)\n\ntry:\n    while running:\n        msg = consumer.poll(timeout=1.0)\n        if msg is None:\n            continue\n        if msg.error():\n            # handle errors; ignore EOF for older clients if necessary\n            print(\"Error:\", msg.error())\n            continue\n\n        # Process message\n        try:\n            key = msg.key()\n            value = msg.value()\n            partition = msg.partition()\n            offset = msg.offset()\n            # business logic here\n            print(f\"Message at {partition}:{offset}, key={key}, len(value)={len(value) if value else 0}\")\n\n            # After successful processing, commit the offset for this message\n            consumer.commit(message=msg, asynchronous=False)\n            # For batch commits, you could gather and call consumer.commit()\n        except Exception as e:\n            # Handle processing exception; you can choose not to commit so message will be reprocessed\n            print(\"Processing failed:\", e)\nfinally:\n    # Close will also try to commit offsets (if enabled) and leave the group cleanly.\n    consumer.close()\n    print(\"Consumer closed.\")\n</code></pre> <p>Notes:</p> <ul> <li>Using <code>subscribe</code> means automatic assignment and rebalances. The callbacks <code>on_assign</code>/<code>on_revoke</code> let you respond to rebalances.</li> <li><code>enable.auto.commit=False</code> gives you full control over when offsets are stored. This is common for at-least-once processing.</li> <li>In Example 1 (the consumer-group version), we didn\u2019t explicitly define partitions \u2014 and that\u2019s actually by design.</li> <li>\u201cI want to consume messages from all partitions of this topic, but I don\u2019t care which specific partitions I get \u2014 please assign them automatically as part of my consumer group.\u201d</li> </ul> <p>Contacts the group coordinator for the specified group.id.</p> <p>The coordinator collects all active consumers in that group and all their subscribed topics.</p> <p>It distributes (assigns) partitions among those consumers \u2014 this is called rebalancing.</p> <p>The assignment result (which partitions this consumer should read) is passed to your on_assign() callback.</p>"},{"location":"streaming/kafka/31-Kafka_Standalone_Consumers/#example-2-manual-assignment-assign-reading-specific-partitions","title":"Example 2 \u2014 Manual assignment (assign) reading specific partitions","text":"<p>This consumer explicitly assigns itself to partitions. It still has <code>group.id</code> so it can commit offsets to Kafka if desired.</p> <pre><code># consumer_assign.py\nfrom confluent_kafka import Consumer, TopicPartition, KafkaError\nimport signal\nimport sys\n\nTOPIC = 'my_topic'\nPARTITIONS_TO_CONSUME = [0, 1]   # explicit partition numbers\nBOOTSTRAP_SERVERS = 'localhost:9092'\nGROUP_ID = 'manual_assign_group'  # still set if you want to commit offsets\n\nrunning = True\ndef shutdown(signum, frame):\n    global running\n    running = False\n\nsignal.signal(signal.SIGINT, shutdown)\nsignal.signal(signal.SIGTERM, shutdown)\n\nconf = {\n    'bootstrap.servers': BOOTSTRAP_SERVERS,\n    'group.id': GROUP_ID,\n    'auto.offset.reset': 'earliest',\n    'enable.auto.commit': False,    # commit manually\n}\n\nconsumer = Consumer(conf)\n\n# Build TopicPartition list and optionally set starting offsets\ntps = [TopicPartition(TOPIC, p) for p in PARTITIONS_TO_CONSUME]\n\n# Optionally set explicit offsets: e.g., start at offset 0 for partition 0\n# tps = [TopicPartition(TOPIC, 0, 0), TopicPartition(TOPIC, 1, 10)]\n\nconsumer.assign(tps)\nprint(f\"Manually assigned to partitions: {tps}\")\n\ntry:\n    while running:\n        msg = consumer.poll(timeout=1.0)\n        if msg is None:\n            continue\n        if msg.error():\n            print(\"Error:\", msg.error())\n            continue\n\n        try:\n            print(f\"Got message from partition {msg.partition()} offset {msg.offset()}\")\n            # process msg.value()\n\n            # commit for the partition after processing\n            consumer.commit(message=msg, asynchronous=False)\n        except Exception as exc:\n            print(\"Processing error:\", exc)\nfinally:\n    consumer.close()\n    print(\"Consumer closed (manual assigned).\")\n</code></pre> <p>Notes:</p> <ul> <li><code>assign()</code> does not contact the group coordinator for assignment; the consumer will not be part of the group for rebalance purposes.</li> <li>Because you explicitly choose partitions, other consumers must avoid reading the same partitions simultaneously (unless you intentionally want multiple readers).</li> <li>If another client subscribes with the same <code>group.id</code>, Kafka will not automatically move your assigned partitions to that client. Coordination must be external.</li> </ul>"},{"location":"streaming/kafka/31-Kafka_Standalone_Consumers/#practical-recommendations","title":"Practical recommendations","text":"<ol> <li>Use <code>subscribe</code> (consumer groups) in normal consumer applications where you want automatic scaling and resilience. Handle rebalances gracefully.</li> <li>Use <code>assign</code> for special cases \u2014 replay tools, administrative utilities, or single-process readers that must own particular partitions.</li> <li>Set <code>group.id</code> if you intend to commit offsets to Kafka. If you don\u2019t need Kafka-managed commits, you can persist offsets elsewhere (a database, Redis, etc.) and omit <code>group.id</code>.</li> <li>Keep rebalance handlers quick. Do not perform heavy processing in <code>on_revoke</code>/<code>on_assign</code>; instead flush metadata state quickly, then continue processing.</li> <li>Be deliberate about offset commit strategy (after processing each message, after a batch, or using transactions). This determines your delivery semantics (at-most-once, at-least-once, or exactly-once with transactions).</li> <li>Monitor consumer liveness and lag. Tools like consumer group lag metrics help detect slow consumers or sticky partitions.</li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/","title":"Kafka Internals of Zookeeper","text":""},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#cluster-membership-in-kafka","title":"Cluster Membership in Kafka","text":"<p>Every broker has a unique identifier which is in broker configuration or automatically granted.</p> <p>It registers itself with its ID in Zookeeper by creating ephermal node and some of the ecosystem tools subscribe to /broker/ids path in Zookeeper to get notified when brokers are added or removed. </p> <p>If we try to create another broker with same Id we get error.</p> <p>When a broker loses connectivity, the ephehermal node automatically gets removed from Zookeeper.</p> <p>If the broker is part of the replicas, the new one immediately takes its place and partitions depending on the rebalancing technique used.</p>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#the-controller","title":"The Controller","text":"<p>The controller node is resposible for electing partition leaders.</p> <p>The first broker that joins the cluster becomes the controller by creating /controller ephemeral node.</p> <p>When other brokers try to become controller, they get \"node already exists exception\".</p> <p>When the controller stops sending heartbeats, the other brokers get this notification through Zookeeper and will attempt to create the controller themselves.</p> <p>The first node that's successful becomes new controller, each time a new one is elected it receives a higher controller epoch number through Zookeeper conditional increment operation.</p> <p>The brokers know the current highest epoch number and if they recieve pings from a controller with lower number they ignore it.</p> <p>This is important because the controller broker can disconnect from ZooKeeper due to a long garbage collection pause\u2014during this pause a new controller will be elected. When the previous leader resumes operations after the pause, it can continue sending messages to brokers without knowing that there is a new controller\u2014in this case, the old controller is considered a zombie. The controller epoch in the message, which allows brokers to ignore messages from old controllers, is a form of zombie fencing.</p>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#kafka-controller-startup-leader-election-and-replica-state-transitions-detailed-explanation","title":"Kafka controller startup, leader election, and replica state transitions \u2014 detailed explanation","text":"<p>When a Kafka cluster boots or a controller process starts, it must first learn the current cluster state before it can safely manage metadata and perform leader elections. That initial bootstrapping and the subsequent response to broker failures are what keep the cluster available and consistent. Below is a step-by-step, in-depth description of what happens, why it matters, and the key implementation and operational details.</p>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#controller-bootstrap-loading-the-replica-state-map","title":"Controller bootstrap: loading the replica state map","text":"<ol> <li> <p>What the controller needs    The controller is the broker responsible for cluster-wide metadata decisions (which partition has which leader, triggering reassignments, orchestrating topic creations/deletions, etc.). To make correct decisions it needs the most recent view of:</p> </li> <li> <p>the list of brokers,</p> </li> <li>the replica assignment for every partition (which replicas exist and their order),</li> <li>the last known leader and in-sync replica (ISR) sets,</li> <li> <p>ongoing partition state (e.g., under-replicated partitions, preferred leader info).</p> </li> <li> <p>Reading from ZooKeeper    Historically Kafka stored this metadata in ZooKeeper. On startup the controller reads the replica state map and related znodes. Because many partitions exist in production clusters, this read is not a single small call but many metadata reads. To avoid being dominated by per-request latency, the controller issues pipelined asynchronous read requests to ZooKeeper. Pipelining means it fires many async reads in parallel and handles responses as they arrive, rather than waiting for each one sequentially. This hides round-trip latency and reduces total load time.</p> </li> <li> <p>Why it can take seconds    Even with pipelining, large clusters (hundreds of brokers / tens or hundreds of thousands of partitions) can require reading many znodes and reconstructing in-memory maps. The controller must validate and merge states, detect inconsistencies, and often rehydrate caches. Kafka performance tests have shown this can take several seconds in large clusters \u2014 a meaningful window during which the cluster is not making leader changes.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#detecting-broker-departure","title":"Detecting broker departure","text":"<ol> <li> <p>How the controller notices a broker is gone    The controller watches ZooKeeper paths that reflect broker liveness (ephemeral znodes written by brokers) or receives an explicit <code>ControlledShutdownRequest</code> from a broker that is shutting down gracefully. An ephemeral znode disappears if the broker process or network session dies; ZooKeeper notifies the controller.</p> </li> <li> <p>Scope of work when a broker dies    Any partition for which the departed broker was the leader now needs a new leader. The controller enumerates those partitions and determines the new leader for each.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#choosing-the-new-leader","title":"Choosing the new leader","text":"<ol> <li> <p>Replica list order    Each partition has a configured replica list \u2014 an ordered list of broker IDs that hold replicas. The controller typically picks the next eligible replica in that list as the new leader. Eligibility depends on whether the replica is in the partition\u2019s ISR (in-sync replicas). If the first replica in the list is unavailable, the controller picks the next ISR replica, and so on. If no ISR replica is available, the controller may elect a non-ISR (depending on configuration), but that risks data loss.</p> </li> <li> <p>Leadership selection rules    Kafka respects the invariant that a new leader should be as up-to-date as possible (prefer ISR). Configuration flags control whether the controller may choose out-of-sync replicas in exceptional cases (for availability).</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#persisting-state-and-broadcasting-changes","title":"Persisting state and broadcasting changes","text":"<ol> <li> <p>Persisting to ZooKeeper    After deciding new leaders and updated ISR sets, the controller persists the updated partition state back to ZooKeeper. Like reads, these writes are issued via pipelined asynchronous requests to reduce latency and increase throughput. Persisting ensures the authoritative state is durably stored and visible to any other controller or admin tooling that reads ZooKeeper.</p> </li> <li> <p>LeaderAndISR requests    Once the new state is persisted, the controller sends <code>LeaderAndIsr</code> requests to every broker that holds replicas for the affected partitions. These are broker-to-broker RPCs that tell each replica who the new leader is, what the updated ISR set is, and other metadata required for replication coordination. The controller batches these updates: rather than sending a separate request for each partition, it groups multiple partition updates that affect the same broker into a single <code>LeaderAndIsr</code> RPC to reduce overhead.</p> </li> <li> <p>UpdateMetadata broadcast    Kafka brokers cache cluster metadata (topics, partitions, leaders, replicas). To keep those caches fresh, the controller sends <code>UpdateMetadata</code> requests to all brokers, which contain the new leadership information for changed partitions. After receiving this, brokers update their <code>MetadataCache</code> so that producers and consumers hitting them later will discover the new leaders.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#broker-roles-after-a-leadership-change","title":"Broker roles after a leadership change","text":"<ol> <li> <p>New leader behavior    A broker that becomes the leader for a partition begins serving client requests immediately for that partition (subject to any internal checks). It accepts producer writes and responds to consumer fetches for the partition\u2019s offsets.</p> </li> <li> <p>Follower behavior    Followers learn their new status from <code>LeaderAndIsr</code> and start replicating messages from the leader. They must pull messages and apply them to their local log to catch up.</p> </li> <li> <p>Catch-up and ISR maintenance    A replica rejoins the ISR only after it has caught up to the leader\u2019s high watermark (or otherwise met the configured replication criteria). The controller tracks ISR membership; when followers fall behind they may be removed from ISR, which affects future leader election safety.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#broker-rejoin-and-recovery","title":"Broker rejoin and recovery","text":"<ol> <li> <p>Broker starts back up    When a previously dead broker returns, it registers with ZooKeeper and reports the replicas it hosts. Initially, those replicas are treated as followers and must replicate from the partition leaders to catch up.</p> </li> <li> <p>Eligibility for leadership    Only after catching up and rejoining the ISR will those replicas be eligible to become leaders in later elections. This prevents electing a lagging replica that could cause data loss.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#efficiency-and-operational-considerations","title":"Efficiency and operational considerations","text":"<ol> <li> <p>Batching and pipelining    The controller batches <code>LeaderAndIsr</code> and <code>UpdateMetadata</code> requests by destination broker, reducing RPC overhead. Pipelining ZooKeeper reads/writes hides latency but requires careful handling of ordering and error cases.</p> </li> <li> <p>Latency vs safety trade-offs    Sometimes administrators prefer faster availability (elect any replica) and accept potential data loss; other times the cluster prioritizes durability and waits for ISR replicas. Kafka exposes configuration knobs to tune this behavior (e.g., unclean leader election settings).</p> </li> <li> <p>Scale limits    Large partition counts slow initial controller load and can lengthen the time to converge after failures. Operators reduce impact by judicious partition counts, careful controller placement, and monitoring controller load and ZooKeeper throughput.</p> </li> <li> <p>Future changes    Later Kafka architectures replace ZooKeeper with an internal quorum (KRaft); the high-level flow (bootstrapping metadata, leader election, persisting state, notifying brokers) remains conceptually similar though implemented without external ZooKeeper.</p> </li> </ol>"},{"location":"streaming/kafka/32-Kafka_Internals_Zookeeper/#summary","title":"Summary","text":"<p>The controller\u2019s work \u2014 reading the current replica state, deciding leaders on broker failure, persisting changes, and notifying brokers \u2014 is the choreography that keeps Kafka available and consistent. Pipelined async I/O and request batching optimize latency at scale, but large partition counts still impose measurable startup and failover delays. The design balances availability, durability, and scalability through ISR semantics, batching, and configurable election policies.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/","title":"Kafka Raft Consensus Protocol","text":""},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#kraft-kafkas-new-raft-based-controller","title":"KRaft : Kafka's new Raft Based Controller","text":"<p>Below is a complete, detailed explanation of why the Kafka community decided to replace the controller, covering every practical and technical motivation in depth and how those issues drove the design of the replacement (KRaft / controller quorum). I do not skip any of the important details described in your passage and I expand on them to make the consequences and alternatives clear.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#high-level-motivation","title":"High-level motivation","text":"<p>At a conceptual level, the controller is the component that owns cluster metadata: broker membership, topic and partition configuration, leader assignments, ISR membership, and other cluster-scoped state. The historical controller implementation relied on ZooKeeper as the authoritative metadata store and used asynchronous interactions with brokers and ZooKeeper. Over time, that architectural coupling exposed hard limits and a set of correctness and operational problems that became increasingly painful as Kafka was pushed to support many more partitions and larger clusters. The decision to replace the controller arose from the need for:</p> <ol> <li>Better scalability (support far larger partition counts with predictable performance).</li> <li>Stronger metadata correctness and determinism (fewer subtle, hard-to-detect inconsistency windows).</li> <li>Faster recovery and bootstrap (reduce time-to-availability after controller restart).</li> <li>Simpler operational model (do not require operators to run and tune an independent distributed system, ZooKeeper).</li> <li>Cleaner ownership and simpler architecture (eliminate split responsibilities and duplication of control paths).</li> </ol> <p>Each of these is discussed below in technical detail.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#1-metadata-writeread-asymmetry-inconsistency-windows","title":"1) Metadata write/read asymmetry \u2192 inconsistency windows","text":""},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#the-symptom","title":"The symptom","text":"<p>In the ZooKeeper-backed model, the controller writes metadata updates to ZooKeeper synchronously (so they are durably recorded), but then sends metadata updates to brokers asynchronously. In parallel, brokers receive metadata updates from ZooKeeper asynchronously (they watch znodes and react when changes happen). Because writing to ZooKeeper and the propagation of those writes to brokers are decoupled and asynchronous, there exists a window of time when ZooKeeper, the controller, and brokers have different views of the cluster state.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#why-this-matters","title":"Why this matters","text":"<ul> <li>When metadata (leader assignments, ISR, topic config) is only durably stored in ZooKeeper but not yet known to all brokers, producers or consumers hitting different brokers may get inconsistent responses. A producer might be told a leader is X by one broker, while another broker still believes leader is Y.</li> <li>These divergence windows create hard-to-detect edge cases: race conditions where the controller believes a transition is complete while some brokers still operate with stale metadata, potentially causing failed writes, misrouted requests, or even split-brain-like behaviors for leadership decisions.</li> <li>Detecting, reproducing, and reasoning about these races is difficult because the events are asynchronous and orderings can differ per broker and per network partition.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#root-cause","title":"Root cause","text":"<p>The root cause is the split control plane\u2014ZooKeeper is the persistent canonical store, the controller writes there synchronously, but brokers also watch and act asynchronously. That split increases complexity and leaves correctness to careful orchestration of asynchronous events.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#2-controller-restart-cost-reading-entire-metadata-set-from-zookeeper","title":"2) Controller restart cost \u2014 reading entire metadata set from ZooKeeper","text":""},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#the-symptom_1","title":"The symptom","text":"<p>Whenever the controller process restarts or a new controller is elected, it must reconstruct the global cluster metadata by reading znodes for all topics, partitions, replicas, ISRs, broker registrations, and more. For large clusters with many partitions, that can be a lot of data and many ZooKeeper requests.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#why-this-matters_1","title":"Why this matters","text":"<ul> <li>Reading and validating all that state is I/O-heavy and, even with pipelined async ZooKeeper calls, can take seconds or longer. That delay prolongs the time the cluster is without a fully functioning controller.</li> <li>During that time the cluster cannot perform new metadata changes or reliably complete leader elections, reducing the cluster\u2019s ability to respond to broker failures and increasing downtime or unavailability windows.</li> <li>The restart cost scales with partition count; therefore growth in partitions causes longer outages or longer convergence times.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#root-cause_1","title":"Root cause","text":"<p>ZooKeeper was not designed as a high-volume metadata log with very large numbers of tiny znodes accessed frequently for reconstruction. The controller\u2019s need to rebuild full in-memory maps on each restart exposed this scalability limit.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#3-fragmented-metadata-ownership-and-inconsistent-paths-for-operations","title":"3) Fragmented metadata ownership and inconsistent paths for operations","text":""},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#the-symptom_2","title":"The symptom","text":"<p>As features were added over the years, Kafka\u2019s metadata ownership got split: some operations were mediated by the controller, some were handled by any broker, and some were manipulated directly in ZooKeeper by tools or admin APIs. This produced inconsistent semantics and tight coupling between components.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#why-this-matters_2","title":"Why this matters","text":"<ul> <li>Different code paths handling metadata increase the cognitive and operational burden on both developers and operators. Bugs and race conditions can arise when different components make assumptions about who \u201cowns\u201d the truth for a particular operation.</li> <li>The mix of direct ZooKeeper writes and broker/controller-mediated writes complicated reasoning about atomicity and ordering of metadata updates.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#root-cause_2","title":"Root cause","text":"<p>The architecture evolved organically; ZooKeeper was used both for coordination and as the single source of truth. Over time that created multiple interaction patterns and divergence in responsibility.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#4-operational-burden-of-running-zookeeper-as-an-additional-distributed-system","title":"4) Operational burden of running ZooKeeper as an additional distributed system","text":""},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#the-symptom_3","title":"The symptom","text":"<p>ZooKeeper is a separate distributed system with its own deployment, scaling, and tuning requirements: ensemble sizing, disk and network tuning, monitoring, and handling of session timeouts and ephemeral znodes. Kafka users now had to operate two distributed systems.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#why-this-matters_3","title":"Why this matters","text":"<ul> <li>Running and debugging ZooKeeper failures adds complexity for teams deploying Kafka in production. Knowledge of ZooKeeper internals (for tuning session timeouts, understanding znode behavior, diagnosing quorum issues) became part of running Kafka reliably.</li> <li>It increases the total operational surface area and failure modes operators must handle.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#root-cause_3","title":"Root cause","text":"<p>The early design choice to use ZooKeeper as the canonical metadata store meant Kafka could not avoid the operational responsibilities imposed by ZooKeeper.</p>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#how-these-problems-motivated-replacing-the-controller","title":"How these problems motivated replacing the controller","text":"<p>Taken together, the above problems pointed to a fundamental architectural mismatch between the level of scale and dynamism Kafka wanted to support and the existing controller + ZooKeeper model. The key goals for a replacement were to:</p> <ul> <li>Co-locate metadata management with Kafka itself, removing the separate ZooKeeper dependency for metadata.</li> <li>Introduce a log-based, consensus-backed metadata store within Kafka that provides a single, strongly-consistent source of truth with linearizable semantics for metadata changes.</li> <li>Enable a controller quorum (multiple controller nodes cooperating via a replicated metadata log) so controller failures don\u2019t require full metadata re-read and can be handled with predictable leader election among controllers.</li> <li>Allow atomic metadata changes via durable, append-only metadata logs, which makes ordering and replication deterministic and easier to reason about.</li> <li>Reduce restart time and bootstrap cost by persisting metadata in an internal replicated log; a newly elected controller can pick up the tail of the log and resume without re-reading thousands of znodes.</li> <li>Simplify operations by removing the need to manage a separate ZooKeeper cluster for metadata; operators manage only Kafka and its internal quorum.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#concrete-design-directions-that-followed","title":"Concrete design directions that followed","text":"<p>The replacement work (often discussed as KRaft and the controller quorum design) therefore implemented:</p> <ul> <li>An internal metadata log (append-only) replicated by a controller quorum using an internal consensus protocol.</li> <li>Controller leaders and followers behave like other replicated services: leaders append metadata changes; followers replicate; commit is decided by quorum.</li> <li>Atomic, ordered, and durable metadata modifications that are applied in the same order everywhere, removing the ZooKeeper async propagation window and eliminating the split control plane.</li> <li>Faster failover since new controllers do not need to rehydrate state by reading many small znodes; they replay/consume the metadata log and are already in sync or can catch up quickly.</li> <li>Unified ownership of metadata, removing the split between controller-managed and broker-managed updates and offering a single authoritative path for all cluster metadata changes.</li> </ul>"},{"location":"streaming/kafka/33-Kafka_Raft_Consensus_Protocol/#summary","title":"Summary","text":"<p>In short, the decision to replace the controller came from a confluence of practical and architectural problems: asynchronous gaps between ZooKeeper and brokers that produced inconsistent metadata windows, slow and expensive controller bootstraps that scaled poorly, fragmented metadata ownership, and the extra burden of operating ZooKeeper as a separate system. Replacing the controller with a quorum-backed, log-based metadata system internal to Kafka addresses these issues by providing a single, durable, ordered metadata log, faster and more deterministic controller failover, and a simplified operational model for users. These changes were necessary to scale Kafka\u2019s metadata plane to the partition counts and operational expectations of modern, large-scale deployments.</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/","title":"Kafka Controller Quorum Concepts","text":""},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#kafka-controller-architecture","title":"Kafka Controller Architecture","text":""},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#1-background-zookeepers-role-in-the-existing-architecture","title":"1. Background: ZooKeeper\u2019s role in the existing architecture","text":"<p>In the original Kafka architecture, ZooKeeper played two key roles:</p> <ol> <li> <p>Controller election \u2013 ZooKeeper was used to elect which Kafka broker would act as the controller.</p> </li> <li> <p>Each broker tried to create an ephemeral znode (say, <code>/controller</code>).</p> </li> <li>The broker that successfully created it became the controller.</li> <li> <p>If the controller failed, ZooKeeper automatically deleted its ephemeral znode, triggering a new election.</p> </li> <li> <p>Metadata storage \u2013 ZooKeeper stored all cluster-level metadata, including:</p> </li> <li> <p>Registered brokers and their endpoints.</p> </li> <li>Topics and their configurations.</li> <li>Partition assignments (which broker holds which replica).</li> <li>ISR (in-sync replicas) sets.</li> <li>Topic-level and broker-level configuration overrides.</li> </ol> <p>The controller, once elected, used ZooKeeper as the backing store while managing:</p> <ul> <li>Leader elections for partitions.</li> <li>Topic creation and deletion.</li> <li>Replica reassignment.</li> <li>Partition rebalancing.</li> </ul> <p>This setup meant the controller and ZooKeeper were deeply intertwined \u2014 the controller executed cluster logic, but the durable state lived in ZooKeeper.</p> <p>However, as Kafka clusters scaled, this model hit limitations: inconsistent state propagation, ZooKeeper bottlenecks, and long failover times (as explained in your previous questions).</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#2-the-vision-for-the-new-controller-architecture","title":"2. The vision for the new controller architecture","text":"<p>The new controller design (introduced as part of the KRaft mode, short for Kafka Raft Metadata mode) aims to completely remove ZooKeeper from Kafka and bring metadata management inside Kafka itself.</p> <p>The core idea behind the redesign is:</p> <p>Kafka already uses a log-based architecture for user data. Why not represent cluster metadata the same way?</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#the-log-based-principle","title":"The log-based principle","text":"<p>In Kafka, a log is a sequence of ordered events. Each new event appends to the end, defining a single, total order of state changes. Consumers read from the log, replaying events to reconstruct state.</p> <p>By applying this model to metadata:</p> <ul> <li>The cluster\u2019s metadata becomes a stream of events (e.g., \u201cTopic created,\u201d \u201cPartition 3 leader changed,\u201d \u201cBroker 7 joined\u201d).</li> <li>Every controller and broker can replay this log to reach the exact same metadata state.</li> <li>This guarantees a consistent and ordered view of cluster metadata across all nodes.</li> </ul> <p>This log-based metadata design aligns Kafka\u2019s internal control plane with its existing data plane philosophy \u2014 state as an event stream.</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#3-raft-based-controller-quorum-the-foundation","title":"3. Raft-based controller quorum: the foundation","text":"<p>In the new architecture, controller nodes form a Raft quorum. Raft is a consensus algorithm designed for managing replicated logs safely across distributed nodes.</p> <p>Here\u2019s how it works conceptually in Kafka\u2019s controller quorum:</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#a-metadata-log","title":"a. Metadata log","text":"<ul> <li> <p>The metadata log is a Kafka-internal, Raft-replicated log that records every metadata change event:</p> </li> <li> <p>Topic creation/deletion</p> </li> <li>Partition addition</li> <li>ISR changes</li> <li>Broker registrations</li> <li>Configuration changes</li> <li>Leader elections</li> <li>Every change that was once written to ZooKeeper is now appended as an event to this metadata log.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#b-raft-roles","title":"b. Raft roles","text":"<p>Within the Raft quorum, controller nodes can play three roles:</p> <ol> <li>Leader (active controller) \u2013 the single node that handles all write operations and coordinates the cluster.</li> <li>Followers (standby controllers) \u2013 replicate the metadata log from the leader and maintain an up-to-date copy.</li> <li>Candidate \u2013 a transient role during elections when a leader fails or steps down.</li> </ol>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#c-leader-election-without-zookeeper","title":"c. Leader election without ZooKeeper","text":"<ul> <li>Raft provides internal leader election.</li> <li>Controllers elect one among themselves as the active controller (the Raft leader).</li> <li>No external system (like ZooKeeper) is needed \u2014 Raft\u2019s consensus mechanism ensures only one leader is active at a time.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#d-replication-and-durability","title":"d. Replication and durability","text":"<ul> <li>Every metadata change is appended to the metadata log on the leader.</li> <li>The leader replicates it to the follower controllers.</li> <li>Once a majority (quorum) of controllers acknowledge the entry, it\u2019s committed.</li> <li> <p>This guarantees:</p> </li> <li> <p>Consistency (all controllers eventually converge on the same log).</p> </li> <li>Durability (metadata changes survive crashes as they are replicated).</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#4-the-active-controller","title":"4. The \u201cactive controller\u201d","text":"<p>The Raft leader \u2014 called the active controller \u2014 performs the same logical duties the old ZooKeeper-based controller did, but now:</p> <ul> <li>It writes all changes to the metadata log.</li> <li> <p>It handles RPC requests from brokers for:</p> </li> <li> <p>Topic creation/deletion.</p> </li> <li>Partition reassignment.</li> <li>Leadership updates.</li> <li>Configuration updates.</li> </ul> <p>Because every metadata event is replicated across the Raft quorum, followers always stay hot \u2014 they continuously replay and apply updates from the leader.</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#5-fast-failover-and-recovery","title":"5. Fast failover and recovery","text":"<p>In the old architecture:</p> <ul> <li>When a controller failed, a new one had to read all metadata from ZooKeeper \u2014 a slow process for large clusters.</li> <li>Then it had to propagate this state to all brokers, which also took time.</li> </ul> <p>In the new Raft-based design:</p> <ul> <li>All follower controllers already have the latest metadata log replicated locally.</li> <li> <p>When the active controller fails:</p> </li> <li> <p>Raft automatically elects a new leader (within milliseconds).</p> </li> <li>The new leader already has the entire metadata state (no full reload needed).</li> <li>It immediately resumes handling cluster operations.</li> </ul> <p>This eliminates the lengthy reloading period and drastically improves cluster availability.</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#6-metadata-distribution-to-brokers-metadatafetch-api","title":"6. Metadata distribution to brokers (MetadataFetch API)","text":""},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#old-method-push-based-updates","title":"Old method: Push-based updates","text":"<ul> <li>The old controller \u201cpushed\u201d metadata updates to brokers via ZooKeeper watches or direct RPCs.</li> <li>This push model was asynchronous and could result in inconsistent or delayed state across brokers.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#new-method-pull-based-updates","title":"New method: Pull-based updates","text":"<ul> <li>In the new design, brokers fetch metadata from the active controller.</li> <li>Kafka introduces a MetadataFetch API, similar to how brokers fetch messages from topic partitions.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#how-it-works","title":"How it works:","text":"<ol> <li>Each broker maintains a metadata offset \u2014 the position in the metadata log it has processed.</li> <li>Brokers periodically issue a MetadataFetchRequest to the active controller, asking:</li> </ol> <p>\u201cGive me any metadata updates after offset X.\u201d 3. The controller replies with the delta \u2014 only the new metadata events since offset X. 4. The broker applies these updates and advances its local metadata offset.</p> <p>This mechanism:</p> <ul> <li>Guarantees brokers stay in sync with the controller.</li> <li>Reduces unnecessary data transfer (only incremental updates are sent).</li> <li>Simplifies consistency \u2014 every broker\u2019s metadata state corresponds to a specific log offset.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#7-broker-persistence-and-faster-startup","title":"7. Broker persistence and faster startup","text":"<p>In the new design:</p> <ul> <li>Brokers persist metadata to their local disk (not just hold it in memory).</li> <li> <p>This means that if a broker restarts:</p> </li> <li> <p>It already has most of the metadata up to its last fetched offset.</p> </li> <li>It simply asks the active controller for new updates since that offset.</li> </ul> <p>This enables very fast broker startup, even in clusters with millions of partitions \u2014 a major improvement over the previous need to reload and rebuild all metadata state from ZooKeeper or the controller each time.</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#8-summary-of-improvements","title":"8. Summary of improvements","text":"Area Old ZooKeeper-Based Controller New Raft-Based Controller (KRaft) Metadata storage In ZooKeeper (znodes) In Kafka\u2019s internal Raft metadata log Controller election ZooKeeper ephemeral znode Raft consensus within controller quorum Controller failover Slow \u2014 full reload from ZooKeeper Fast \u2014 followers already replicated metadata State consistency Async writes/reads via ZooKeeper and brokers Strong consistency via replicated log Metadata propagation Controller pushes updates Brokers fetch updates via MetadataFetch API Broker startup Requires metadata reload Quick \u2014 brokers replay local metadata log Operational complexity Kafka + ZooKeeper to manage Kafka only (ZooKeeper removed) Scalability Limited by ZooKeeper\u2019s structure Scales with Kafka\u2019s native log architecture"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#9-architectural-elegance-and-conceptual-unification","title":"9. Architectural elegance and conceptual unification","text":"<p>The new controller design has a conceptual elegance: Kafka now uses the same fundamental abstraction \u2014 an ordered, replicated log \u2014 for everything.</p> <ul> <li>User data is stored as event logs in Kafka topics.</li> <li>System metadata is stored as an event log in the Raft-based metadata log.</li> </ul> <p>Both are:</p> <ul> <li>Append-only</li> <li>Ordered</li> <li>Replayable</li> <li>Consistent</li> <li>Replicated across multiple nodes</li> </ul> <p>This unification simplifies reasoning, improves fault tolerance, and leverages Kafka\u2019s own proven architectural strengths (log replication, offset tracking, incremental consumption).</p>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#10-operational-impact","title":"10. Operational impact","text":"<p>For cluster operators:</p> <ul> <li>There\u2019s no ZooKeeper to manage anymore \u2014 simplifying deployments and reducing moving parts.</li> <li>Controller failovers are now sub-second and do not block metadata operations.</li> <li>Kafka becomes a self-contained distributed system, which simplifies monitoring, tuning, and upgrades.</li> </ul> <p>For developers and clients:</p> <ul> <li>Metadata is always consistent across brokers.</li> <li>Leadership changes propagate predictably and quickly.</li> <li>Administrative operations (topic creation, reconfiguration) are more reliable and faster.</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#11-the-big-picture-kafkas-evolution-to-a-single-self-governing-distributed-system","title":"11. The big picture \u2014 Kafka\u2019s evolution to a single self-governing distributed system","text":"<p>The shift to a Raft-based controller quorum represents the final step in Kafka\u2019s long-term plan to:</p> <ul> <li>Eliminate external dependencies (ZooKeeper).</li> <li>Achieve linear scalability in both data and metadata planes.</li> <li>Provide predictable performance for massive clusters (millions of partitions).</li> <li>Simplify the mental and operational model for users.</li> </ul> <p>In short, the new controller makes Kafka a fully self-contained distributed system with:</p> <ul> <li>Internal consensus (Raft)</li> <li>Unified log abstraction</li> <li>Fast failover</li> <li>Deterministic metadata propagation</li> <li>Simpler, more reliable operation at scale</li> </ul>"},{"location":"streaming/kafka/34-Kafka_Controller_Quorum/#tldr","title":"TL;DR","text":"<p>The Kafka community replaced the old ZooKeeper-based controller with a new Raft-based controller quorum (KRaft) because it allows Kafka to:</p> <ul> <li>Manage metadata internally in a replicated log.</li> <li>Elect leaders using Raft instead of ZooKeeper.</li> <li>Guarantee consistent metadata across all brokers.</li> <li>Eliminate slow controller restarts and reloading phases.</li> <li>Scale to millions of partitions with fast failover and simple operations.</li> </ul> <p>Essentially, Kafka now manages both data and metadata the same way \u2014 as ordered, replicated event logs \u2014 giving it the reliability, scalability, and simplicity that ZooKeeper could never provide.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/","title":"Kafka Replication Concepts","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#replication-in-kafka","title":"Replication in Kafka","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#kafkas-replication-architecture-deep-dive","title":"Kafka\u2019s Replication Architecture: Deep Dive","text":"<p>Apache Kafka\u2019s replication model underpins its durability, availability, and fault tolerance guarantees. At its core, replication ensures that partition data is copied across multiple brokers, so that even if a broker fails, Kafka can continue to serve reads and writes without losing data.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#1-topics-partitions-and-replicas","title":"1. Topics, Partitions, and Replicas","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#a-topic","title":"a) Topic","text":"<p>A topic is a named stream of records (e.g., <code>\"payments\"</code>, <code>\"orders\"</code>, <code>\"user-activity\"</code>). It is a logical category for messages.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#b-partition","title":"b) Partition","text":"<p>Each topic is split into partitions \u2014 append-only, ordered logs that Kafka distributes across brokers. Each partition:</p> <ul> <li>Is stored on multiple brokers (for redundancy).</li> <li>Has an offset sequence (0, 1, 2, \u2026) identifying message order.</li> <li>Is replicated n times (<code>replication.factor = n</code>).</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#c-replicas","title":"c) Replicas","text":"<p>Each replica is a copy of a partition\u2019s data. Replicas are spread across brokers for fault isolation.</p> <p>Example: Topic <code>payments</code> has 3 partitions and a replication factor of 3.</p> Partition Leader Broker Followers payments-0 Broker 1 Broker 2, 3 payments-1 Broker 2 Broker 1, 3 payments-2 Broker 3 Broker 1, 2 <p>This ensures that every partition has multiple identical copies on different brokers.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#2-types-of-replicas","title":"2. Types of Replicas","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#a-leader-replica","title":"a) Leader Replica","text":"<ul> <li>Each partition has exactly one leader replica at a given time.</li> <li>All produce requests (writes) are handled by the leader.</li> <li>The leader appends new messages to its local log and coordinates replication to followers.</li> <li>Clients can consume from the leader (by default).</li> </ul> <p>Kafka enforces strong ordering and consistency by ensuring that all writes flow through the leader.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#b-follower-replicas","title":"b) Follower Replicas","text":"<ul> <li>All other replicas are followers.</li> <li>Followers replicate data asynchronously from the leader.</li> <li>They continuously fetch new messages from the leader\u2019s log and append them to their own local copy.</li> <li>Followers stay in sync with the leader as closely as possible.</li> </ul> <p>If the leader fails, a follower can be promoted to leader, preserving availability.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#3-the-isr-in-sync-replica-set","title":"3. The ISR (In-Sync Replica) Set","text":"<p>The ISR (in-sync replica set) is a dynamic list of replicas that are fully caught up with the leader.</p> <p>A replica is considered in-sync if:</p> <ol> <li>It is alive (not crashed).</li> <li>It has replicated the leader\u2019s data up to the latest committed offset.</li> <li>It responds to FetchRequests from the leader within a configured timeout (<code>replica.lag.time.max.ms</code>).</li> </ol>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#a-leader-election-and-isr","title":"a) Leader election and ISR","text":"<p>When the leader fails, only replicas in the ISR are eligible to become the new leader \u2014 unless the broker is configured to allow unclean leader elections (<code>unclean.leader.election.enable=true</code>), which can cause data loss.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#b-high-water-mark-hwm","title":"b) High-Water Mark (HWM)","text":"<ul> <li>The HWM is the highest offset that is committed, i.e., replicated to all replicas in the ISR.</li> <li>Consumers can only read messages below or equal to the HWM.</li> <li>The leader periodically shares the current HWM with followers.</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#4-the-replication-protocol-leader-follower","title":"4. The Replication Protocol (Leader \u2192 Follower)","text":"<p>The replication protocol governs how followers fetch data from leaders.</p> <ol> <li>Each follower sends periodic FetchRequests to its leader, requesting data starting from its last known offset.</li> <li>The leader responds with a batch of messages and the current high-water mark (HWM).</li> <li>The follower appends the data to its local log and updates its offset tracking.</li> <li>The follower acknowledges receipt.</li> <li>The leader tracks follower progress and updates the ISR accordingly.</li> </ol> <p>This design ensures:</p> <ul> <li>Asynchronous replication (low latency for producers).</li> <li>High throughput (batch replication, pipelined fetches).</li> <li>Consistency at read time (consumers read only committed messages).</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#5-fault-tolerance-and-leader-election","title":"5. Fault Tolerance and Leader Election","text":"<p>When a broker (leader) fails:</p> <ol> <li>The controller detects broker failure (via heartbeats or ZooKeeper/KRaft metadata).</li> <li>It determines which partitions lost their leaders.</li> <li> <p>For each affected partition:</p> </li> <li> <p>The controller selects a new leader from the ISR list.</p> </li> <li>Updates the cluster metadata and informs all brokers.</li> <li>Followers learn the new leader and resume replication.</li> </ol> <p>This ensures fast recovery \u2014 typically sub-second in modern Kafka versions.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#reading-from-follower-replicas-kip-392","title":"Reading from Follower Replicas (KIP-392)","text":"<p>Originally, Kafka clients could only consume from leaders. KIP-392 (\u201cAllow consumers to fetch from follower replicas\u201d) changed that, adding rack-aware replica selection and enabling cross-data-center efficiency.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#1-motivation","title":"1. Motivation","text":"<p>Large, geo-distributed Kafka deployments face a problem:</p> <ul> <li>Producers and consumers in multiple data centers (or racks).</li> <li>All consumers reading from the leader causes cross-network traffic if the leader is remote.</li> </ul> <p>For example, a topic\u2019s leader may reside in Data Center A, but many consumers live in Data Center B. Every read requires cross-DC data transfer, inflating bandwidth costs and latency.</p> <p>The solution: Allow consumers to read from nearest followers that are in-sync replicas.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#2-core-mechanism","title":"2. Core Mechanism","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#a-the-clientrack-setting-consumer-side","title":"a) The <code>client.rack</code> setting (consumer side)","text":"<ul> <li>Each consumer identifies its physical or logical location:</li> </ul> <p><pre><code>client.rack=us-east-2\n</code></pre> * This informs brokers where the client is located.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#b-the-replicaselectorclass-setting-broker-side","title":"b) The <code>replica.selector.class</code> setting (broker side)","text":"<ul> <li>Defines how brokers choose which replica to serve read requests from.</li> <li>Default:</li> </ul> <p><pre><code>replica.selector.class=org.apache.kafka.common.replica.RackAwareReplicaSelector\n</code></pre> * Other options:</p> <ul> <li><code>LeaderSelector</code> \u2013 Always read from leader (default before KIP-392).</li> <li><code>RackAwareReplicaSelector</code> \u2013 Prefer replicas whose <code>rack.id</code> matches <code>client.rack</code>.</li> <li>Custom implementation \u2013 You can plug in your own logic by implementing the <code>ReplicaSelector</code> interface (e.g., based on latency, load, or region).</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#c-rack-awareness-in-brokers","title":"c) Rack-awareness in brokers","text":"<p>Each broker\u2019s <code>server.properties</code> must include:</p> <pre><code>broker.rack=us-east-2\n</code></pre> <p>This allows Kafka to match client and broker locations.</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#3-how-read-from-follower-works-internally","title":"3. How \u201cread from follower\u201d works internally","text":"<ol> <li>The consumer sends a FetchRequest to the cluster.</li> <li> <p>The broker receiving the request examines:</p> </li> <li> <p>The consumer\u2019s <code>client.rack</code>.</p> </li> <li>The available replicas (leader + followers) and their <code>rack.id</code>s.</li> <li>Based on the configured <code>replica.selector.class</code>, the broker chooses which replica to serve the read from.</li> <li>The chosen replica (often a follower) serves data only up to its high-water mark, ensuring it does not return uncommitted messages.</li> <li>If the follower is slightly behind, the consumer sees data a few milliseconds later than it would from the leader.</li> </ol>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#4-consistency-and-safety-guarantees","title":"4. Consistency and Safety Guarantees","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#a-commit-visibility","title":"a) Commit visibility","text":"<ul> <li>Only committed messages (those below the HWM) are visible on followers.</li> <li>Followers fetch the leader\u2019s HWM along with each data batch.</li> <li>Consumers therefore never see uncommitted messages, maintaining exactly-once and at-least-once semantics.</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#b-potential-delay","title":"b) Potential delay","text":"<ul> <li>There\u2019s a small propagation delay between when data is committed on the leader and when the follower becomes aware of the new HWM.</li> <li>Thus, reading from followers may slightly increase consumer lag compared to the leader.</li> <li>This is a trade-off: lower cross-network cost at the expense of slightly higher latency.</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#c-failure-scenarios","title":"c) Failure scenarios","text":"<ul> <li>If the leader fails, one of the followers (already serving reads) can quickly be promoted to leader with minimal data loss (since it\u2019s in-sync).</li> <li>Consumers automatically re-route based on updated metadata.</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#5-benefits-of-read-from-follower-rff","title":"5. Benefits of Read-from-Follower (RFF)","text":"Benefit Description Reduced network cost Local consumers can fetch from local replicas instead of remote leaders. Improved read scalability Load is distributed across replicas rather than all reads hitting the leader. Faster geo-local access Latency improves when consumers fetch from nearby brokers. Better resource utilization Followers now share read load, balancing CPU and disk usage."},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#6-trade-offs-and-considerations","title":"6. Trade-offs and Considerations","text":"Consideration Description Increased replication delay Followers may lag the leader slightly; consumers see updates later. Consistency vs latency Followers serve only committed data, so low-latency uncommitted reads aren\u2019t possible. Complex client routing Consumers need rack info; misconfiguration can cause suboptimal routing. Operational complexity Monitoring ISR size and replica lag becomes more important. Only works for fetch (read) Produce (write) requests still go to the leader."},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#7-practical-example-configuration","title":"7. Practical Example Configuration","text":""},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#broker-side","title":"Broker side:","text":"<pre><code>broker.id=1\nbroker.rack=us-east-1\nreplica.selector.class=org.apache.kafka.common.replica.RackAwareReplicaSelector\n</code></pre>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#consumer-side","title":"Consumer side:","text":"<pre><code>client.rack=us-east-1\nfetch.max.bytes=1048576\nenable.auto.commit=true\n</code></pre> <p>The consumer will now fetch from the nearest in-sync replica on the same rack (<code>us-east-1</code>).</p>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#the-bigger-picture-why-this-matters","title":"The Bigger Picture: Why This Matters","text":"<ol> <li>Scalability \u2014 As Kafka clusters reach thousands of brokers, distributing read load becomes essential.</li> <li>Geo-awareness \u2014 Multi-region Kafka deployments (via MirrorMaker 2 or Confluent Cluster Linking) benefit from rack-aware fetching.</li> <li>Performance \u2014 Decreased inter-rack traffic reduces latency spikes caused by cross-zone routing.</li> <li>Reliability \u2014 Because followers can now serve reads safely, Kafka becomes more resilient to temporary leader overloads or network partitions.</li> </ol>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#future-directions-beyond-kip-392","title":"Future Directions (beyond KIP-392)","text":"<ul> <li>Adaptive Replica Selection: Ongoing discussions explore dynamic selection based on network latency and broker load (not just rack IDs).</li> <li>Tiered Storage Integration: With Kafka\u2019s tiered storage (KIP-405), replicas may fetch historical data from cloud/object storage, and follower reads might integrate with tiered logs.</li> <li>Cross-cluster follower reads: Future designs could allow follower fetches from remote clusters in multi-region architectures.</li> </ul>"},{"location":"streaming/kafka/35-Kafka_Replication_Concepts/#summary","title":"Summary","text":"Concept Description Leader Replica Handles all writes; default source for reads. Follower Replica Copies data from leader; eligible for leadership during failover. ISR Set Replicas fully caught up with leader; only ISR members can become new leaders. High-Water Mark Offset of last committed record replicated to all ISR members. KIP-392 Adds support for reading from follower replicas based on rack awareness. client.rack Consumer config indicating client location. replica.selector.class Broker config determining how replicas are chosen for read requests. Guarantee Only committed messages (\u2264 HWM) are served, preserving reliability. Trade-off Slight extra delay compared to reading from the leader. <p>In essence, Kafka\u2019s replication system ensures that each partition\u2019s data is durable, consistent, and available across brokers. And with KIP-392\u2019s \u201cread-from-follower\u201d, Kafka\u2019s architecture evolves beyond fault tolerance \u2014 towards network efficiency, geo-awareness, and horizontal scalability for global-scale deployments.</p>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/","title":"In-depth explanation of leader/follower replication, ISR, and related behavior","text":""},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#basic-mechanics-fetch-requests-offsets-and-how-the-leader-tracks-progress","title":"Basic mechanics: Fetch requests, offsets, and how the leader tracks progress","text":"<ul> <li>Each partition has one leader replica and zero or more follower replicas. Producers write only to the leader; consumers usually read from the leader (though replication uses the same Fetch RPC).</li> <li>Followers maintain background fetcher threads that send Fetch requests to the leader. A Fetch request asks for messages starting at a particular offset (the next offset the follower needs).</li> <li>Fetch requests are always for a contiguous sequence starting at that offset. Because requests are ordered and offsets are monotonic, the leader can infer exactly which messages each follower has already received: if a follower requests offset <code>N</code>, the follower has successfully fetched (and therefore stored) all messages up to offset <code>N-1</code>.</li> <li> <p>Two useful offset concepts:</p> </li> <li> <p>Log End Offset (LEO) or Log End: the next offset a replica would assign to a newly appended message (i.e., last offset + 1) \u2014 indicates how many messages are present at that replica.</p> </li> <li>Follower\u2019s Fetch Offset: the offset the follower requests; the leader uses this to know how far the follower is behind.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#in-sync-replicas-isr","title":"In-Sync Replicas (ISR)","text":"<ul> <li>The ISR is the set of replicas that are considered sufficiently up-to-date with the leader to be eligible for leader election.</li> <li>A replica is in the ISR if it has been actively fetching and has not lagged behind the leader by more than configured thresholds (see below).</li> <li>Only replicas in the ISR are candidates for becoming leader if the current leader fails. This prevents electing a replica that lacks recent writes (which would cause data loss relative to acknowledged writes).</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#how-out-of-sync-is-determined","title":"How \u201cout of sync\u201d is determined","text":"<ul> <li> <p>Kafka uses timing + progress to decide whether a follower is out-of-sync:</p> </li> <li> <p>If a follower stops sending Fetch requests for longer than a configured time window, or</p> </li> <li>If a follower continues fetching but fails to catch up to the leader (i.e., its fetched offset remains behind leader's LEO) for too long,     then the controller will remove the follower from the ISR.</li> <li> <p>The primary configuration that controls this timeout is:</p> </li> <li> <p><code>replica.lag.time.max.ms</code> \u2014 if a follower\u2019s fetched offset hasn\u2019t advanced to the leader\u2019s latest offset within this window, it can be removed from the ISR.</p> </li> <li>Other related timeouts and settings include replica fetcher timeouts and request timeouts, but <code>replica.lag.time.max.ms</code> is the main one controlling ISR membership.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#configuration-parameters-and-safety-related-flags","title":"Configuration parameters and safety-related flags","text":"<ul> <li> <p><code>replica.lag.time.max.ms</code></p> </li> <li> <p>Controls how long a follower can lag (or be inactive) before being removed from the ISR. Shorter values make the ISR strict (followers must stay very current); longer values are more tolerant but increase risk of electing a stale replica.</p> </li> <li> <p><code>min.insync.replicas</code> (topic-level or broker default)</p> </li> <li> <p>When producers use <code>acks=all</code> (i.e., require acknowledgement from all in-sync replicas), the broker enforces that at least <code>min.insync.replicas</code> replicas are in the ISR; otherwise the broker rejects writes. This prevents acknowledged writes from having too few replicas.</p> </li> <li> <p><code>acks</code> (producer setting)</p> </li> <li> <p><code>acks=0</code>, <code>acks=1</code>, <code>acks=all</code> control client durability semantics. <code>acks=all</code> combined with <code>min.insync.replicas</code> gives the strongest durability guarantee.</p> </li> <li> <p><code>unclean.leader.election.enable</code></p> </li> <li> <p>If <code>false</code> (recommended for durability), Kafka will not allow a follower that is not in the ISR to be promoted to leader. If <code>true</code>, a non-ISR replica can become leader to improve availability at the cost of possible data loss (because the new leader might lack some acknowledged messages).</p> </li> <li> <p><code>replica.fetch.max.bytes</code>, fetcher thread settings, request timeouts, etc.</p> </li> <li> <p>Control throughput and replication performance, and thus indirectly affect whether followers can keep up and remain in the ISR.</p> </li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#leader-election-data-loss-and-trade-offs","title":"Leader election, data loss, and trade-offs","text":"<ul> <li> <p>If the leader fails:</p> </li> <li> <p>The controller selects a new leader from the ISR (if any). Because ISR members have replicated all messages up to the leader\u2019s committed point, electing from ISR preserves acknowledged data.</p> </li> <li>If ISR is empty and <code>unclean.leader.election.enable=true</code>, Kafka may elect a stale follower as leader \u2014 this recovers availability faster but can cause data loss (some previously acknowledged writes might be missing).</li> <li> <p>Trade-offs:</p> </li> <li> <p>Durability-first: Keep <code>unclean.leader.election.enable=false</code>, set <code>min.insync.replicas</code> &gt;= 2 for replication factor &gt;= 3, use <code>acks=all</code>. This increases risk of temporary unavailability (if too many replicas fail or are removed from ISR) but prevents data loss.</p> </li> <li>Availability-first: Allow unclean leader election (<code>true</code>) to continue serving reads/writes even when ISR members are not available \u2014 higher availability, higher risk of data loss.</li> <li>The <code>replica.lag.time.max.ms</code> setting influences both availability and durability: very small values cause replicas to be dropped from ISR quickly (which can block writes if <code>min.insync.replicas</code> not met); very large values risk electing a leader that is behind.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#life-cycle-of-isr-changes-expansion-and-shrinkage","title":"Life-cycle of ISR changes (expansion and shrinkage)","text":"<ul> <li> <p>Replica falls behind \u2192 removed from ISR</p> </li> <li> <p>If a follower stops fetching or lags beyond <code>replica.lag.time.max.ms</code>, controller marks it out-of-sync and removes it from ISR.</p> </li> <li>If <code>min.insync.replicas</code> is configured and ISR size &lt; <code>min.insync.replicas</code>, writes requiring <code>acks=all</code> are rejected until the ISR grows again.</li> <li> <p>Replica catches up \u2192 re-added to ISR</p> </li> <li> <p>When a replica catches up (its fetched offset reaches the leader\u2019s LEO or within acceptable bounds) and remains active, the controller re-adds it to the ISR.</p> </li> <li>Re-adding to ISR is important because it restores redundancy and allows that replica to be a leader candidate again.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#replication-internals-fetcher-threads-order-guarantees-and-hwm","title":"Replication internals: fetcher threads, order guarantees, and HWM","text":"<ul> <li>Replication uses the same Fetch protocol as consumers; follower fetchers request messages starting at a given offset and apply them in order.</li> <li>Ordering: Because fetches are for ordered offsets, followers replicate messages in the same order as the leader.</li> <li> <p>High Watermark (HW) or Replica High Watermark (rHW):</p> </li> <li> <p>The leader tracks a high watermark which is the highest offset that is known to be replicated to all in-sync replicas. Consumers are only allowed to read messages up to the high watermark; this prevents reading messages that some replicas might not have persisted.</p> </li> <li>When followers replicate messages and the leader sees those offsets replicated across the ISR, it advances the high watermark.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#example-scenario-concrete","title":"Example scenario (concrete)","text":"<ol> <li>Partition has replication factor = 3: Leader L, Followers F1, F2. ISR = {L, F1, F2}.</li> <li>Leader appends messages up to offset 100 (LEO = 101).</li> <li>F1 fetches up to offset 100; F2 fetches only up to offset 90 because of network slowness.</li> <li>If F2 doesn\u2019t fetch any new offsets for longer than <code>replica.lag.time.max.ms</code>, the controller removes F2 from ISR \u2192 ISR becomes {L, F1}.</li> <li>If <code>min.insync.replicas = 2</code> and producer uses <code>acks=all</code>, writes succeed because L and F1 are in ISR. If <code>min.insync.replicas = 3</code>, new writes requiring <code>acks=all</code> will be rejected until F2 returns.</li> <li>If L fails now, controller elects a new leader from ISR (L or F1). Because F2 is not in ISR, it cannot become leader (so no data loss relative to committed offsets).</li> <li>If F2 had been promoted (unclean election enabled), messages offset 91\u2013100 might be lost if F2 never received them.</li> </ol>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#operational-implications-and-what-to-monitor","title":"Operational implications and what to monitor","text":"<p>Monitor these metrics to understand replication health:</p> <ul> <li>ISR size per partition: sudden drops indicate followers are falling out-of-sync.</li> <li>Follower fetch-lag: <code>LEO - follower_offset</code> for each follower over time (shows how many messages behind).</li> <li>Fetch request latency and errors: high latencies or repeated errors cause followers to lag.</li> <li>Under-replicated partitions: partitions with fewer replicas than replication factor (indicates durability risk).</li> <li>Controller logs: show when replicas are removed/added to ISR and when leader elections occur.</li> <li>Broker CPU, disk I/O, and network: resource saturation causes replication lag.   Operational alerts often include:</li> <li>Partitions with ISR size &lt; configured threshold.</li> <li>Partitions with under-replicated partitions &gt; 0.</li> <li>Frequent leader elections for a partition.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#best-practices-and-recommendations","title":"Best practices and recommendations","text":"<ul> <li>Use replication factor &gt;= 3 for production topic partitions to tolerate broker failures.</li> <li>Use <code>acks=all</code> for producers that need strong durability, and set <code>min.insync.replicas</code> to at least 2 (for RF=3).</li> <li>Keep <code>unclean.leader.election.enable=false</code> for durability-critical data (accepts temporary unavailability rather than data loss).</li> <li>Tune <code>replica.lag.time.max.ms</code> to balance responsiveness and tolerance for transient delays. Start with sensible defaults and adapt to your environment\u2019s latency characteristics.</li> <li>Ensure broker/network resources (IO, CPU, NIC) are sufficient so followers can keep up: replication is network- and disk-bound.</li> <li>Monitor the metrics above and test failure scenarios (broker restart, network partition) in a staging environment.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#failure-modes-to-understand","title":"Failure modes to understand","text":"<ul> <li>Network partition isolates broker: follower or leader cannot fetch \u2192 follower is removed from ISR or leader is isolated and elections occur.</li> <li>Broker crash and restart: replicas on that broker fall behind until restart and catch-up; during this period they may be out of ISR.</li> <li>Slow disks or GC pauses: cause long replication delays and ISR shrinkage.</li> <li>Unclean leader election allowed: rapid availability recovery at cost of possible data loss.</li> </ul>"},{"location":"streaming/kafka/36-Kafka_InSync_OutOfSync_Replicas/#summary-short","title":"Summary (short)","text":"<ul> <li>Followers fetch from leaders with ordered Fetch requests; the fetch offset tells the leader exactly how far each follower has replicated.</li> <li>The ISR contains replicas considered sufficiently up-to-date; only ISR members are eligible for leader election (unless unclean elections are allowed).</li> <li><code>replica.lag.time.max.ms</code> controls how long a replica can lag before being removed from ISR; <code>min.insync.replicas</code> and producer <code>acks</code> settings determine durability guarantees.</li> <li>Choices are trade-offs between availability and durability; monitoring replication lag, ISR membership, and under-replicated partitions is essential.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/","title":"Kafka Request Processing Part 1","text":""},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#1-high-level-overview-kafka-as-a-requestresponse-server","title":"1. High-level overview \u2014 Kafka as a request/response server","text":"<p>Kafka brokers are fundamentally request\u2013response servers built on top of a binary TCP protocol. Every action that clients (producers, consumers, admin tools, or other brokers for replication) perform \u2014 producing messages, fetching messages, creating topics, fetching metadata, etc. \u2014 happens through this protocol.</p> <ul> <li>Clients always initiate connections (brokers never call clients).</li> <li>Requests flow in one direction: client \u2192 broker.</li> <li>Responses flow back: broker \u2192 client.</li> </ul> <p>This simple design \u2014 clients initiate, brokers respond \u2014 allows Kafka to scale horizontally with predictable connection behavior.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#2-kafka-binary-protocol","title":"2. Kafka binary protocol","text":"<p>Kafka has its own binary protocol defined over TCP. It is versioned and explicitly documented in Kafka\u2019s source code and official documentation. Every request follows a well-defined structure containing a header and a payload.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#request-header-components","title":"Request Header Components","text":"Field Description API Key (Request Type) Identifies the operation \u2014 e.g., <code>Produce</code>, <code>Fetch</code>, <code>Metadata</code>, <code>OffsetCommit</code>, <code>FindCoordinator</code>, etc. API Version Indicates the version of the API so brokers can interact correctly with older/newer clients. This enables forward and backward compatibility. Correlation ID A unique integer assigned by the client per request. The broker includes this same ID in the response. Used by clients (and humans reading logs) to match requests and responses for debugging. Client ID A string identifying the client application \u2014 often useful for logging, quotas, and metrics. For example, a Spark job\u2019s Kafka source might use <code>spark-streaming-consumer</code> as the client ID. <p>Every request type has its own payload schema (for example, a <code>ProduceRequest</code> has topic, partition, and message batch data). Kafka serializes and deserializes these messages efficiently using its internal protocol definitions.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#3-processing-order-and-ordering-guarantees","title":"3. Processing order and ordering guarantees","text":"<p>Kafka guarantees that:</p> <p>All requests sent over a single TCP connection are processed in the order they were received.</p> <p>This is extremely important because:</p> <ul> <li>It ensures message ordering per partition (since producers send messages to one leader partition in order).</li> <li>It prevents reordering caused by concurrent network delivery.</li> <li>If a client sends multiple produce or fetch requests, Kafka processes them serially per connection.</li> </ul> <p>This means: within a single connection, request <code>n</code> will always be fully processed before request <code>n+1</code>.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#4-broker-threading-model-acceptors-processors-and-handlers","title":"4. Broker threading model \u2014 acceptors, processors, and handlers","text":"<p>A Kafka broker is built around a multi-threaded, event-driven network model. It manages potentially tens of thousands of simultaneous client connections, so efficient concurrency is critical.</p> <p>Here\u2019s the workflow for every connection:</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#step-1-acceptor-thread","title":"Step 1: Acceptor Thread","text":"<ul> <li>Each Kafka broker port (by default <code>9092</code> for plaintext, or <code>9093</code> for SSL) has an acceptor thread.</li> <li>The acceptor listens for new TCP connections using the Java NIO (non-blocking I/O) framework.</li> <li> <p>When a new client connection arrives, the acceptor:</p> </li> <li> <p>Accepts it,</p> </li> <li>Configures it for non-blocking mode,</li> <li>Hands it over to one of the network processor threads for further handling.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#step-2-network-processor-threads-aka-network-threads","title":"Step 2: Network Processor Threads (a.k.a. Network Threads)","text":"<ul> <li> <p>These are responsible for:</p> </li> <li> <p>Reading data from client sockets,</p> </li> <li>Parsing Kafka requests,</li> <li>Enqueuing the parsed requests into a request queue,</li> <li>Taking completed responses from a response queue and writing them back to the sockets.</li> </ul> <p>You can control how many network threads are created using:</p> <pre><code>num.network.threads=3   # Default is usually 3\n</code></pre> <p>This is often increased on brokers that handle heavy network I/O (for example, many producers/consumers).</p> <p>Each processor thread can handle many connections using non-blocking I/O multiplexing.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#5-internal-queues-request-and-response-queues","title":"5. Internal queues: Request and Response Queues","text":"<p>Kafka uses two key in-memory queues per broker:</p> <ol> <li>Request Queue</li> <li>Response Queue</li> </ol>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#request-queue","title":"Request Queue","text":"<ul> <li>All incoming parsed requests go here.</li> <li>Requests in this queue are picked up by I/O handler threads (a.k.a. I/O threads or KafkaRequestHandler threads).</li> <li>Each handler thread processes the request (for example, appending messages to a log, reading data, updating offsets).</li> <li>Number of handler threads is configurable:</li> </ul> <p><pre><code>num.io.threads=8\n</code></pre> * Each handler thread executes business logic depending on the request type.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#response-queue","title":"Response Queue","text":"<ul> <li>Once processing is done, a handler thread places a response object into the response queue.</li> <li>The network processor picks up this response and sends it back to the client.</li> <li>The response contains the correlation ID, allowing the client to map the reply to its original request.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#6-delayed-responses-and-purgatories","title":"6. Delayed responses and \u201cpurgatories\u201d","text":"<p>Not every request can be immediately completed. Kafka has a mechanism called purgatory \u2014 a waiting area for requests that must be delayed until certain conditions are met.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#example-scenarios","title":"Example scenarios:","text":"<ol> <li> <p>Consumer Fetch Requests</p> </li> <li> <p>If a consumer issues a fetch and the partition has no new data yet, the broker does not respond immediately.</p> </li> <li> <p>Instead, it holds the fetch request in purgatory until:</p> <ul> <li>New data arrives, or</li> <li>A timeout (e.g., <code>fetch.max.wait.ms</code>) expires.</li> <li>This long-polling mechanism reduces unnecessary network churn and improves efficiency.</li> </ul> </li> <li> <p>Producer Acknowledgments</p> </li> <li> <p>When a producer sends data with <code>acks=all</code>, the broker only responds once all in-sync replicas have confirmed receipt.</p> </li> <li> <p>The request remains in purgatory until this replication condition is met.</p> </li> <li> <p>Admin Requests</p> </li> <li> <p>Example: <code>DeleteTopics</code> request.</p> </li> <li>Topic deletion is asynchronous \u2014 the request is acknowledged only once deletion is initiated or completed. Until then, it may wait in purgatory.</li> </ol>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#purgatory-management","title":"Purgatory management:","text":"<ul> <li>Kafka maintains several purgatories for different request types (e.g., one for produce, one for fetch).</li> <li>Internally, these are managed as time-indexed data structures that efficiently wake up waiting requests when conditions are satisfied.</li> <li>Requests in purgatory are tracked by keys like topic-partition identifiers or completion conditions.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#7-error-handling-and-correlation-ids","title":"7. Error handling and correlation IDs","text":"<p>Every broker response (success or failure) includes:</p> <ul> <li>The same correlation ID as the request,</li> <li>A response code (error code or success indicator),</li> <li>Possibly an error message.</li> </ul> <p>This enables:</p> <ul> <li>Clients to match responses to requests asynchronously,</li> <li>Operators to trace specific requests in logs using correlation IDs,</li> <li>Brokers to log meaningful error lines (e.g., <code>\"Request 1557 from client=producer_app failed with NOT_LEADER_FOR_PARTITION\"</code>).</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#8-putting-it-all-together-end-to-end-flow","title":"8. Putting it all together (end-to-end flow)","text":"<p>Let\u2019s trace a Produce request step by step:</p> <ol> <li>Producer Client sends a <code>ProduceRequest</code> to the leader broker for a given partition over a TCP connection.</li> <li>Broker\u2019s Acceptor Thread accepts the connection (if new) and assigns it to a Network Processor.</li> <li>Network Processor Thread reads the bytes, parses the request, and enqueues it into the Request Queue.</li> <li>IO Handler Thread picks the request from the queue, appends the messages to the commit log on disk, triggers replication to followers, and waits for acknowledgment if required.</li> <li>If the producer used <code>acks=all</code>, the request waits in ProduceRequestPurgatory until all ISR replicas have replicated the message.</li> <li>Once the condition is met, the response is enqueued in the Response Queue.</li> <li>Network Processor Thread dequeues the response and writes it to the producer\u2019s TCP connection.</li> <li>The Producer Client receives the response (with matching correlation ID) and marks the batch as successfully acknowledged.</li> </ol> <p>This process happens thousands of times per second per broker across thousands of connections.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#9-configuration-summary-key-performance-knobs","title":"9. Configuration summary (key performance knobs)","text":"Parameter Purpose <code>num.network.threads</code> Number of threads handling network I/O (socket read/write). <code>num.io.threads</code> Number of threads processing the business logic of requests. <code>queued.max.requests</code> Maximum number of requests that can be queued at once before throttling new ones. <code>replica.fetch.max.bytes</code> Max data size per fetch request for replication. <code>fetch.max.wait.ms</code> Maximum wait time for fetch requests (affects consumer purgatory)."},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#10-why-this-architecture-matters","title":"10. Why this architecture matters","text":"<p>Kafka\u2019s broker threading and request queue model:</p> <ul> <li>Enables high throughput (hundreds of thousands of requests per second),</li> <li>Ensures ordering and consistency per connection,</li> <li>Supports long-polling and asynchronous operations efficiently,</li> <li>Allows for fault isolation \u2014 network I/O, request processing, and delayed response management are handled by distinct thread pools,</li> <li>Provides clear metrics: each queue, thread pool, and purgatory exposes metrics that are vital for monitoring (e.g., queue sizes, response times, request rates).</li> </ul> <p>When you later monitor Kafka (via JMX or Prometheus), you\u2019ll see metrics like:</p> <ul> <li><code>RequestQueueSize</code></li> <li><code>ResponseQueueSize</code></li> <li><code>RequestHandlerAvgIdlePercent</code></li> <li><code>NetworkProcessorAvgIdlePercent</code></li> <li><code>ProduceRequestPurgatorySize</code></li> <li><code>FetchRequestPurgatorySize</code></li> </ul> <p>These correspond exactly to the architectural components described above.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#in-summary","title":"In summary","text":"<ul> <li>Kafka brokers are TCP servers that handle structured binary requests from clients, other brokers, and the controller.</li> <li>Requests are processed strictly in order per connection.</li> <li>The threading model (acceptor \u2192 network \u2192 handler) ensures scalability and concurrency.</li> <li>Internal queues decouple I/O from processing.</li> <li>\u201cPurgatories\u201d efficiently handle delayed operations like long polling and replication acknowledgment.</li> <li>Configuration parameters and metrics directly map to these internal components.</li> </ul> <p>Let\u2019s unpack that passage thoroughly and explain how Kafka clients discover partition leaders, how metadata management works, and how the broker\u2013client interaction ensures that produce and fetch requests always go to the right leader.</p> <p>This explanation builds on the previous section about Kafka\u2019s internal threading model and request queues, focusing now on what happens inside those requests, how clients choose where to send them, and how metadata refreshes maintain correctness.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#1-the-three-main-request-types-handled-by-kafka-brokers","title":"1. The three main request types handled by Kafka brokers","text":"<p>Once I/O (request handler) threads pick up requests from the request queue, they process different kinds of client operations. The three dominant categories are:</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#1-produce-requests","title":"1. Produce Requests","text":"<ul> <li>Sent by producers (e.g., Java/Python/Go clients).</li> <li>Contain batches of records (messages) to be appended to a specific topic partition.</li> <li>Must be sent to the leader replica of that partition.</li> <li> <p>The broker:</p> </li> <li> <p>Validates the request,</p> </li> <li>Appends the data to its local log,</li> <li>Waits for acknowledgments from ISR replicas (depending on <code>acks</code> setting),</li> <li>Sends a response back (success or error).</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#2-fetch-requests","title":"2. Fetch Requests","text":"<ul> <li>Sent by consumers and follower replicas.</li> <li>Consumers use them to read messages.</li> <li>Followers use them to replicate messages from the leader.</li> <li>Fetch requests must also go to the leader broker for a partition.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#3-admin-requests","title":"3. Admin Requests","text":"<ul> <li>Sent by administrative clients (e.g., using Kafka Admin API).</li> <li> <p>Examples:</p> </li> <li> <p><code>CreateTopics</code></p> </li> <li><code>DeleteTopics</code></li> <li><code>DescribeCluster</code></li> <li><code>ListOffsets</code></li> <li><code>AlterConfigs</code></li> <li>These can be sent to any broker, because admin requests are metadata-oriented and do not depend on specific partition leadership.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#2-why-produce-and-fetch-requests-must-go-to-the-leader","title":"2. Why produce and fetch requests must go to the leader","text":"<p>Each Kafka partition has:</p> <ul> <li>One leader replica (which handles all reads/writes),</li> <li>Zero or more follower replicas (which replicate data asynchronously).</li> </ul> <p>Kafka enforces single-leader semantics per partition to maintain ordering and consistency:</p> <ul> <li>The leader is the authoritative source for appending messages.</li> <li>Producers can\u2019t write to followers, and consumers can\u2019t fetch from followers (unless <code>fetch.from.follower</code> is explicitly supported, as in some special configurations).</li> </ul> <p>If a client sends a request to a broker that is not the leader, the broker immediately rejects it with the error:</p> <pre><code>NOT_LEADER_FOR_PARTITION\n</code></pre> <p>or in newer Kafka versions:</p> <pre><code>NOT_LEADER_OR_FOLLOWER\n</code></pre> <p>This tells the client:</p> <p>\u201cYou\u2019ve sent this request to the wrong broker \u2014 the partition\u2019s leader has changed.\u201d</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#3-how-clients-know-where-the-leader-is-metadata-requests","title":"3. How clients know where the leader is \u2014 metadata requests","text":"<p>This is where the Metadata API comes into play.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#metadata-request","title":"Metadata Request","text":"<ul> <li>A client sends a <code>MetadataRequest</code> to any broker in the cluster.</li> <li>The request lists one or more topics that the client cares about.</li> <li> <p>The broker responds with a metadata map describing:</p> </li> <li> <p>Each topic,</p> </li> <li>Each partition within that topic,</li> <li>The broker IDs and endpoints for all replicas of that partition,</li> <li>Which broker currently acts as leader for each partition.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#example-metadataresponse","title":"Example: MetadataResponse","text":"Topic Partition Leader Replicas ISR orders 0 Broker 101 101, 102, 103 101, 103 orders 1 Broker 102 101, 102, 103 102, 103 orders 2 Broker 103 101, 102, 103 101, 102, 103 <p>From this, the client learns:</p> <ul> <li>If it wants to send messages to <code>orders-0</code>, it must connect to broker 101.</li> <li>If it wants to fetch from <code>orders-1</code>, it must contact broker 102.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#4-the-brokers-role-in-serving-metadata","title":"4. The broker\u2019s role in serving metadata","text":"<p>Every Kafka broker maintains a metadata cache that is constantly updated by the controller broker.</p> <ul> <li>The controller (a special broker elected via ZooKeeper or KRaft, depending on Kafka mode) manages cluster metadata: who is leader for each partition, which brokers are alive, etc.</li> <li>Whenever leadership changes (e.g., due to broker failure), the controller broadcasts metadata updates to all brokers.</li> <li>Therefore, any broker can respond to a metadata request \u2014 not just the controller.</li> </ul> <p>This is why:</p> <p>Clients can send metadata requests to any broker in the cluster.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#5-client-caching-and-metadata-refreshes","title":"5. Client caching and metadata refreshes","text":""},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#client-side-metadata-cache","title":"Client-side metadata cache","text":"<ul> <li>After receiving metadata, Kafka clients cache it locally.</li> <li>The cache maps:</li> </ul> <p><pre><code>{ topic \u2192 [partition \u2192 leader_broker_id] }\n</code></pre> * This allows producers and consumers to route requests directly to the correct leader broker without asking the cluster each time.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#refresh-intervals","title":"Refresh intervals","text":"<p>The client\u2019s cached metadata can become stale over time, especially if:</p> <ul> <li>Brokers are added or removed,</li> <li>Partitions are rebalanced,</li> <li>A leader fails and a new one is elected.</li> </ul> <p>To handle this, clients automatically refresh metadata periodically.</p> <p>Controlled by:</p> <pre><code>metadata.max.age.ms=300000  # default = 5 minutes\n</code></pre> <p>Meaning:</p> <ul> <li>Every 5 minutes (by default), the client will re-fetch metadata proactively.</li> <li>This keeps the routing information current even if no errors occur.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#error-triggered-refresh","title":"Error-triggered refresh","text":"<p>If a client receives <code>NOT_LEADER_FOR_PARTITION</code> or <code>UNKNOWN_TOPIC_OR_PARTITION</code>, it immediately triggers a forced metadata refresh before retrying the request.</p> <p>This reactive behavior ensures that clients recover quickly from leadership changes.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#6-typical-producefetch-cycle-with-metadata-lookups","title":"6. Typical produce/fetch cycle with metadata lookups","text":""},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#example-workflow-producer","title":"Example workflow \u2014 Producer","text":"<ol> <li>The producer starts up and sends a MetadataRequest for topic <code>transactions</code>.</li> <li>Broker responds with:</li> </ol> <p><pre><code>Partition 0 \u2192 Leader = Broker 1\nPartition 1 \u2192 Leader = Broker 2\n</code></pre> 3. Producer caches this mapping:</p> <p><pre><code>transactions-0 \u2192 Broker 1\ntransactions-1 \u2192 Broker 2\n</code></pre> 4. Producer sends <code>ProduceRequest</code> batches to the appropriate brokers. 5. Later, if broker 2 goes down and broker 3 becomes leader for <code>transactions-1</code>, producer\u2019s next produce to broker 2 fails with:</p> <p><pre><code>NOT_LEADER_FOR_PARTITION\n</code></pre> 6. Producer triggers a metadata refresh, updates its cache, and resends the request to broker 3. 7. Processing continues seamlessly after a brief retry delay.</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#example-workflow-consumer","title":"Example workflow \u2014 Consumer","text":"<ol> <li>Consumer subscribes to topic <code>transactions</code>.</li> <li>Sends a MetadataRequest to any broker.</li> <li>Receives mapping of partitions to leaders.</li> <li>Connects to leader brokers directly and sends <code>FetchRequests</code>.</li> <li>If a broker fails and leadership changes, the consumer detects fetch errors \u2192 refreshes metadata \u2192 reconnects.</li> </ol>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#7-configuration-summary-client-side","title":"7. Configuration summary (client-side)","text":"Parameter Description <code>metadata.max.age.ms</code> Maximum time before client automatically refreshes metadata (default 5 minutes). <code>reconnect.backoff.ms</code> Time to wait before reconnecting to a failed broker. <code>retry.backoff.ms</code> Time to wait before retrying a failed produce/fetch request. <code>max.in.flight.requests.per.connection</code> Number of unacknowledged requests allowed per connection; preserves ordering when \u22641. <code>bootstrap.servers</code> Initial list of brokers used to obtain metadata. Only used for discovery; clients may later connect elsewhere."},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#8-internal-resiliency-and-efficiency","title":"8. Internal resiliency and efficiency","text":"<p>This metadata-driven routing model provides several advantages:</p>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#efficiency","title":"Efficiency","text":"<ul> <li>Clients communicate directly with the brokers that own the data they need.   There\u2019s no central proxy or router.</li> <li>Reduces network hops and central bottlenecks.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#resiliency","title":"Resiliency","text":"<ul> <li>Clients automatically recover from leadership changes by re-fetching metadata.</li> <li>No single broker failure halts all operations because metadata is available cluster-wide.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#scalability","title":"Scalability","text":"<ul> <li>Each broker only handles produce/fetch requests for the partitions it leads, distributing load evenly across the cluster.</li> </ul>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#9-putting-it-all-together-end-to-end-summary","title":"9. Putting it all together \u2014 end-to-end summary","text":"<ol> <li>Clients initiate requests over TCP using Kafka\u2019s binary protocol.</li> <li>Brokers accept connections, enqueue requests, and process them via I/O threads.</li> <li>Produce and Fetch requests must go to the leader broker for each partition.</li> <li>Metadata requests provide clients with up-to-date partition-to-leader mappings.</li> <li>Clients cache this metadata and refresh it periodically or when errors indicate stale information.</li> <li>If a broker leadership changes, clients retry the request after refreshing metadata.</li> <li>All brokers can answer metadata requests because they maintain an updated metadata cache synced from the controller.</li> </ol>"},{"location":"streaming/kafka/37-Kafka_Request_Processing_Pt1/#10-key-takeaway","title":"10. Key takeaway","text":"<p>Kafka\u2019s metadata-driven client routing is what enables the system to be:</p> <ul> <li>Decentralized (no single broker handles all traffic),</li> <li>Highly available (clients self-heal after leader changes),</li> <li>Efficient (requests go straight to the right broker),</li> <li>Ordered and consistent (per-partition request routing ensures proper sequencing).</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/","title":"Kafka Producer Requests","text":""},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#1-the-acks-configuration-what-it-means","title":"1. The <code>acks</code> configuration \u2014 what it means","text":"<p>When a producer sends a message to a Kafka topic, it can specify how many brokers must confirm the message before the producer considers it successfully written. This is controlled by the producer configuration parameter:</p> <pre><code>acks=0 | 1 | all\n</code></pre>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#option-1-acks0","title":"Option 1: <code>acks=0</code>","text":"<ul> <li>The producer does not wait for any acknowledgment.</li> <li>As soon as the producer sends the message over the network, it marks it as \u201csuccessfully sent.\u201d</li> <li>The broker may or may not have received it \u2014 the producer never checks.</li> <li>Fastest option, but least reliable. If the broker crashes or network drops packets, messages can be lost silently.</li> </ul> <p>Use case: High-throughput, low-value data where occasional loss is acceptable (e.g., metrics, logs, sensor streams).</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#option-2-acks1","title":"Option 2: <code>acks=1</code>","text":"<ul> <li>The producer waits for acknowledgment only from the leader replica of the partition.</li> <li>The leader appends the message to its local log (in memory or file system cache) and then responds.</li> <li>Followers replicate asynchronously \u2014 meaning the message might not yet be replicated when the acknowledgment is sent.</li> <li>If the leader crashes before followers replicate it, that message can be lost (because the new leader might not have that message).</li> </ul> <p>Use case: Balanced reliability and performance \u2014 commonly used in systems where minor data loss is tolerable.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#option-3-acksall-or-acks-1","title":"Option 3: <code>acks=all</code> (or <code>acks=-1</code>)","text":"<ul> <li>The producer waits for acknowledgment from all in-sync replicas (ISR) of that partition.</li> <li>The leader only replies after confirming that every ISR replica has written the message to its local log.</li> <li>Provides strongest durability guarantee: a message is safe even if the leader immediately crashes after acknowledging.</li> </ul> <p>Use case: Mission-critical data (financial transactions, customer actions, audit logs) where no data loss is acceptable.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#2-what-happens-when-a-broker-receives-a-produce-request","title":"2. What happens when a broker receives a produce request","text":"<p>When the leader broker for a partition receives a ProduceRequest from a producer, it performs the following steps:</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-1-validation-checks","title":"Step 1: Validation checks","text":"<p>The broker ensures:</p> <ol> <li> <p>Authorization:    The client (producer) has permission to write to the topic.</p> </li> <li> <p>Valid <code>acks</code> value:    Only <code>0</code>, <code>1</code>, or <code>all</code> (or <code>-1</code>, which means the same as \u201call\u201d) are accepted.</p> </li> <li> <p>Sufficient in-sync replicas (ISR):</p> </li> <li> <p>If <code>acks=all</code>, the broker checks if there are enough ISR members available.</p> </li> <li> <p>This check depends on another broker-side parameter:</p> <pre><code>min.insync.replicas=&lt;N&gt;\n</code></pre> <p>If the ISR count drops below this threshold, the broker rejects new produce requests with an error like:</p> <pre><code>NOT_ENOUGH_REPLICAS\n</code></pre> <p>This prevents acknowledging messages that would not be safely replicated.</p> </li> </ol>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-2-write-the-message-to-the-local-log","title":"Step 2: Write the message to the local log","text":"<ul> <li>The leader appends the record batch to its local log segment file.</li> <li> <p>Kafka uses the Linux filesystem page cache for performance:</p> </li> <li> <p>The message is written to the OS cache (in memory),</p> </li> <li>The OS asynchronously flushes it to disk later.</li> <li>Kafka does not call <code>fsync()</code> or wait for disk persistence before acknowledging.   Instead, replication (to followers) provides durability.</li> </ul> <p>Why this design? Disk <code>fsync</code> is slow. Kafka relies on replication to guarantee durability instead of waiting for every write to reach disk.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-3-behavior-based-on-acks-setting","title":"Step 3: Behavior based on <code>acks</code> setting","text":"<p>After writing to the log:</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#if-acks0","title":"If <code>acks=0</code>","text":"<ul> <li>The broker does not send any acknowledgment back.</li> <li>The producer assumes success immediately after sending.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#if-acks1","title":"If <code>acks=1</code>","text":"<ul> <li>The leader immediately sends a success response after writing to its local log (even before followers replicate).</li> <li>Replication happens asynchronously afterward.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#if-acksall","title":"If <code>acks=all</code>","text":"<ul> <li>The broker waits until all ISR replicas confirm they have replicated the message.</li> <li>To manage this waiting process efficiently, Kafka uses a buffer called \u201cpurgatory.\u201d</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#3-what-is-purgatory","title":"3. What is purgatory?","text":"<p>\u201cPurgatory\u201d in Kafka is an in-memory waiting area for requests that cannot be completed immediately.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#how-it-works-for-produce-requests","title":"How it works for produce requests:","text":"<ul> <li>When a message is produced with <code>acks=all</code>, the broker stores the request in ProduceRequestPurgatory.</li> <li> <p>The request waits there until:</p> </li> <li> <p>All ISR replicas have acknowledged replication of that offset.</p> </li> <li> <p>Once replication completes:</p> </li> <li> <p>The request is removed from purgatory,</p> </li> <li>The broker sends a success response back to the producer.</li> </ul> <p>This mechanism allows the broker to delay responses efficiently without blocking threads.</p> <p>Kafka also uses separate purgatories for fetch requests (for consumers performing long polling).</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#4-sequence-of-events-step-by-step","title":"4. Sequence of events (step-by-step)","text":"<p>Let\u2019s trace an example of a message being produced to a partition with replication factor = 3.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#scenario","title":"Scenario","text":"<ul> <li>Replicas: Broker 1 (Leader), Broker 2 (Follower), Broker 3 (Follower)</li> <li>ISR = {1, 2, 3}</li> <li>Producer sends a record with <code>acks=all</code>.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-1-producer-sends-a-producerequest-to-broker-1-leader","title":"Step 1: Producer sends a <code>ProduceRequest</code> to Broker 1 (leader)","text":"<p>The request includes:</p> <ul> <li>Topic and partition,</li> <li>Record batch,</li> <li><code>acks=all</code>.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-2-broker-1-validates-request","title":"Step 2: Broker 1 validates request","text":"<ul> <li>Checks permissions, acks value, ISR count.</li> <li>Appends message to its log (OS cache).</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-3-broker-1-forwards-the-new-message-to-followers","title":"Step 3: Broker 1 forwards the new message to followers","text":"<ul> <li>Followers 2 and 3 send <code>Fetch</code> requests to Broker 1 for replication.</li> <li>Broker 1 sends them the new record.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-4-followers-write-message-to-their-own-logs","title":"Step 4: Followers write message to their own logs","text":"<ul> <li>Each follower appends the message to its local log.</li> <li>Each follower sends back a replication acknowledgment to Broker 1.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-5-broker-1-waits-in-purgatory","title":"Step 5: Broker 1 waits in purgatory","text":"<ul> <li>The produce request remains in ProduceRequestPurgatory until Broker 1 sees acknowledgments from all ISR members for that offset.</li> </ul>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#step-6-broker-1-responds-to-producer","title":"Step 6: Broker 1 responds to producer","text":"<ul> <li>Once all ISR members have confirmed replication, Broker 1 removes the request from purgatory and sends:</li> </ul> <p><pre><code>ProduceResponse(correlation_id=1234, status=SUCCESS)\n</code></pre> * The producer marks the message as successfully written.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#5-key-configuration-parameters-influencing-this-process","title":"5. Key configuration parameters influencing this process","text":"Parameter Level Description <code>acks</code> Producer How many replicas must acknowledge before success. <code>min.insync.replicas</code> Broker/Topic Minimum ISR count required for accepting <code>acks=all</code> writes. <code>replication.factor</code> Topic Total number of replicas for the partition. <code>replica.lag.time.max.ms</code> Broker How long a follower can lag before being removed from ISR. <code>replica.fetch.max.bytes</code> Broker Max bytes per replication fetch. <code>flush.messages</code>, <code>flush.ms</code> Broker Control how often logs are flushed to disk (optional)."},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#6-design-rationale-why-kafka-behaves-this-way","title":"6. Design rationale \u2014 why Kafka behaves this way","text":"<p>Kafka\u2019s approach trades disk persistence latency for replication-based durability:</p> <ul> <li>Writing to the OS page cache is extremely fast.</li> <li>Replication ensures multiple brokers hold copies, so if one crashes before flushing to disk, others can recover the data.</li> <li>The producer controls how much durability it wants using <code>acks</code> and the broker\u2019s ISR configuration.</li> </ul> <p>This design allows Kafka to achieve very high throughput while still providing strong durability when needed.</p>"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#7-summary-table-trade-offs-among-acks-settings","title":"7. Summary table \u2014 trade-offs among <code>acks</code> settings","text":"Setting Who Acknowledges Latency Durability Risk of Data Loss Common Use Case <code>acks=0</code> No one Lowest None Very high Fire-and-forget telemetry <code>acks=1</code> Leader only Medium Moderate Possible if leader fails before replication Most common default <code>acks=all</code> All ISR replicas Highest Strong Very low Critical data (transactions, billing)"},{"location":"streaming/kafka/38-Kafka_Request_Processing_Pt2_Produce_Requests/#in-short","title":"In short","text":"<p>When a producer writes to Kafka:</p> <ol> <li>The leader broker receives and validates the request.</li> <li>The leader appends it to its log (in memory).</li> <li> <p>Depending on <code>acks</code>:</p> </li> <li> <p><code>0</code> \u2192 no response.</p> </li> <li><code>1</code> \u2192 immediate acknowledgment from leader.</li> <li><code>all</code> \u2192 waits in purgatory until all ISR replicas replicate the message.</li> <li>Replication provides durability \u2014 not immediate disk persistence.</li> <li>Once acknowledgment conditions are met, the broker replies and the producer considers the message successfully written.</li> </ol>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/","title":"Kafka Fetch Requests Part 1","text":""},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#1-what-a-fetch-request-is","title":"1. What a Fetch Request Is","text":"<p>A fetch request is how consumers and follower replicas ask a Kafka broker to read data from partitions.</p> <p>It\u2019s a structured request that looks conceptually like this:</p> <p>\u201cPlease send me messages starting from offset X for partition P of topic T, up to Y bytes.\u201d</p> <p>A single fetch request can include multiple topics and partitions, each with a starting offset and a maximum amount of data to return.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#2-types-of-clients-that-send-fetch-requests","title":"2. Types of clients that send fetch requests","text":"<p>There are two main kinds of clients that send fetch requests:</p> <ol> <li> <p>Consumers</p> </li> <li> <p>Fetch messages to deliver them to an application.</p> </li> <li> <p>Interested in the messages themselves.</p> </li> <li> <p>Follower replicas</p> </li> <li> <p>Fetch messages to replicate them from the leader.</p> </li> <li>Interested in copying bytes exactly, not processing the payload.</li> </ol> <p>The broker doesn\u2019t distinguish deeply between these two \u2014 both use the same fetch mechanism. The only difference is which offsets they fetch and how the responses are handled afterward.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#3-request-routing-must-go-to-the-leader","title":"3. Request routing \u2014 must go to the leader","text":"<p>Just like with produce requests:</p> <ul> <li>Fetch requests must go to the leader broker of the partition.</li> <li>If the request goes to a follower broker, that broker returns:</li> </ul> <p><pre><code>NOT_LEADER_FOR_PARTITION\n</code></pre> * Clients rely on metadata requests to know which broker currently leads each partition, and refresh metadata when they see that error.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#4-validation-of-the-fetch-request","title":"4. Validation of the fetch request","text":"<p>When a broker receives a fetch request, the first thing it does is validate it:</p> <ol> <li> <p>Does this topic-partition exist?    If the client refers to a partition that doesn\u2019t exist, return an error.</p> </li> <li> <p>Does the requested offset exist?</p> </li> <li> <p>If the offset is too old (i.e., the messages at that offset were already deleted due to log retention policies), the broker responds with:</p> <p><pre><code>OFFSET_OUT_OF_RANGE\n</code></pre>    * If the offset is too new (beyond the latest message written), the broker also responds with <code>OFFSET_OUT_OF_RANGE</code>.</p> </li> </ol> <p>Only if the offset is valid does the broker proceed to read data.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#5-reading-messages-upper-limit","title":"5. Reading messages \u2014 upper limit","text":"<p>Once validation passes, the broker begins reading messages sequentially from its local log file.</p> <ul> <li>The client\u2019s fetch request includes a maximum amount of data it can handle for each partition.</li> </ul> <p><pre><code>max_bytes_per_partition = N\n</code></pre> * The broker respects this limit and never sends more data than requested.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#why-this-limit-is-important","title":"Why this limit is important","text":"<ul> <li>It protects the client\u2019s memory.   A consumer needs to preallocate buffers large enough to hold the incoming data.</li> <li>Without it, a broker could send a massive response that crashes the client or causes out-of-memory errors.</li> </ul> <p>So the broker stops reading when either:</p> <ul> <li>The end of the log is reached, or</li> <li>The maximum requested bytes limit is reached.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#6-zero-copy-data-transfer-how-kafka-achieves-high-performance","title":"6. Zero-copy data transfer \u2014 how Kafka achieves high performance","text":"<p>Kafka uses a zero-copy I/O mechanism when sending messages from brokers to clients.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#how-traditional-systems-do-it","title":"How traditional systems do it:","text":"<p>Normally, reading data from disk and sending it over a socket involves multiple copy operations:</p> <ol> <li>Read data from disk \u2192 kernel buffer.</li> <li>Copy from kernel buffer \u2192 user-space buffer (application memory).</li> <li>Copy from user-space \u2192 network socket buffer.</li> <li>Send over the network.</li> </ol> <p>This involves multiple context switches and memory copies.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#how-kafkas-zero-copy-works","title":"How Kafka\u2019s zero-copy works:","text":"<p>Kafka uses the Linux system call <code>sendfile()</code>, which allows data to be transferred directly from the file descriptor (log segment) to the network socket \u2014 without passing through user space.</p> <p>That means:</p> <ul> <li>The broker doesn\u2019t copy data into an application buffer.</li> <li>The kernel copies bytes directly between the file system cache and the network card.</li> </ul> <p>This provides:</p> <ul> <li>Very high throughput (less CPU spent on copying bytes),</li> <li>Lower latency, and</li> <li>Reduced garbage collection overhead (since no large temporary buffers are created).</li> </ul> <p>In short: Kafka reads from disk (or Linux page cache) straight to the socket \u2014 skipping user memory entirely.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#7-lower-limit-waiting-for-enough-data-long-polling-behavior","title":"7. Lower limit \u2014 waiting for enough data (long-polling behavior)","text":"<p>Kafka clients can also specify a minimum amount of data they\u2019re willing to accept before the broker sends a response.</p> <p>For example:</p> <pre><code>min_bytes = 10000  # 10 KB\n</code></pre> <p>This tells the broker:</p> <p>\u201cOnly send me data once you have at least 10 KB of messages available across my requested partitions.\u201d</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#why-this-matters","title":"Why this matters","text":"<ul> <li>This feature is useful for topics with low traffic (few messages arriving).</li> <li>Without it, consumers would constantly poll the broker and get empty responses \u2014 wasting CPU and network resources.</li> <li> <p>With a <code>min_bytes</code> threshold:</p> </li> <li> <p>The broker holds the request open (\u201cdelayed response\u201d) until either:</p> <ul> <li>At least <code>min_bytes</code> of data are available, or</li> <li>A timeout expires (see below).</li> </ul> </li> </ul> <p>This model is called long polling.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#8-timeout-dont-wait-forever","title":"8. Timeout \u2014 don\u2019t wait forever","text":"<p>Kafka provides another parameter for consumers:</p> <pre><code>max_wait_ms = X\n</code></pre> <p>This tells the broker:</p> <p>\u201cIf you don\u2019t reach <code>min_bytes</code> of data within <code>X</code> milliseconds, send me whatever you have.\u201d</p> <p>For example:</p> <ul> <li><code>min_bytes = 10000</code></li> <li><code>max_wait_ms = 500</code></li> </ul> <p>This means:</p> <ul> <li>The broker will hold the fetch request for up to 500 ms waiting to accumulate 10 KB.</li> <li>If enough messages arrive before the timeout, the broker responds immediately.</li> <li>If not, the broker responds after 500 ms with however much data is available (even if it\u2019s less than 10 KB).</li> </ul> <p>This approach:</p> <ul> <li>Avoids busy polling,</li> <li>Reduces useless network chatter,</li> <li>Ensures the consumer eventually makes progress.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#9-how-brokers-handle-delayed-fetch-requests-fetch-purgatory","title":"9. How brokers handle delayed fetch requests \u2014 \u201cFetch Purgatory\u201d","text":"<p>Just like produce requests with <code>acks=all</code>, fetch requests that can\u2019t be completed immediately are placed in FetchRequestPurgatory.</p> <p>The logic:</p> <ol> <li>If <code>min_bytes</code> isn\u2019t yet satisfied, the broker holds the request in purgatory.</li> <li> <p>When new messages are appended to the log, the broker checks:</p> </li> <li> <p>Have we now accumulated enough bytes to meet <code>min_bytes</code>?</p> </li> <li>Or has the <code>max_wait_ms</code> timeout expired?</li> <li>If either condition is true, the broker completes the fetch request and sends the response.</li> </ol> <p>This mechanism allows efficient event-driven handling rather than polling.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#10-putting-it-all-together-full-fetch-flow-example","title":"10. Putting it all together \u2014 full fetch flow example","text":"<p>Let\u2019s walk through a complete example step by step.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#example-scenario","title":"Example scenario","text":"<p>Consumer <code>C1</code> wants to read data from:</p> <ul> <li><code>topic=Test</code></li> <li><code>partition=0</code></li> <li><code>starting_offset=53</code></li> <li><code>max_bytes=1MB</code></li> <li><code>min_bytes=10KB</code></li> <li><code>max_wait_ms=500</code></li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-1-consumer-sends-a-fetchrequest","title":"Step 1 \u2014 Consumer sends a FetchRequest","text":"<pre><code>FetchRequest(topic=Test, partition=0, offset=53,\n             max_bytes=1048576, min_bytes=10240, max_wait_ms=500)\n</code></pre> <p>Broker must be the leader for <code>Test-0</code>.</p>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-2-broker-validates","title":"Step 2 \u2014 Broker validates","text":"<ul> <li>Partition <code>Test-0</code> exists? \u2705</li> <li>Offset 53 valid? \u2705</li> <li>Data available beyond offset 53? Possibly not yet.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-3-broker-checks-available-data","title":"Step 3 \u2014 Broker checks available data","text":"<ul> <li>Only 3 KB of new messages available \u2192 less than <code>min_bytes=10KB</code>.</li> <li>Broker places this fetch request in Fetch Purgatory and waits.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-4-new-data-arrives","title":"Step 4 \u2014 New data arrives","text":"<ul> <li>15 KB of new messages written by producers.</li> <li>Broker rechecks the fetch request \u2192 condition met (<code>&gt;= 10KB</code>).</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-5-broker-sends-response","title":"Step 5 \u2014 Broker sends response","text":"<ul> <li>Reads up to 1 MB (the upper limit) but returns only what\u2019s available.</li> <li>Uses zero-copy to send bytes directly from disk cache to network.</li> <li>Consumer receives messages starting from offset 53.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#step-6-consumer-processes-and-sends-next-fetch","title":"Step 6 \u2014 Consumer processes and sends next fetch","text":"<ul> <li>After processing, consumer updates its committed offset (e.g., now 68) and sends another FetchRequest starting from offset 69.</li> </ul>"},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#11-configuration-summary","title":"11. Configuration summary","text":"Parameter Scope Meaning <code>fetch.min.bytes</code> Consumer Minimum data (in bytes) the broker must accumulate before responding. <code>fetch.max.bytes</code> Consumer Maximum total data (in bytes) the broker can return in one response. <code>fetch.max.wait.ms</code> Consumer Maximum time broker will wait for <code>min.bytes</code> before sending whatever it has. <code>max.partition.fetch.bytes</code> Consumer Per-partition data limit in each fetch request. <code>socket.receive.buffer.bytes</code> Broker Size of socket receive buffer \u2014 affects network throughput. <code>num.io.threads</code> Broker Threads handling fetch/produce requests."},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#12-key-performance-and-reliability-benefits","title":"12. Key performance and reliability benefits","text":"Feature Benefit Zero-copy transfer Very high throughput, low CPU overhead. Upper fetch limit Prevents out-of-memory errors on clients. Lower fetch limit (<code>min_bytes</code>) Reduces CPU/network churn for low-traffic topics. Timeout (<code>max_wait_ms</code>) Ensures responsiveness \u2014 consumers eventually receive data even if traffic is low. Purgatory mechanism Efficient event-driven waiting for new data without constant polling."},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#13-summary-big-picture","title":"13. Summary (big picture)","text":"Step What Happens 1 Client sends fetch request (topics, partitions, offsets, max/min bytes, timeout). 2 Broker validates the request (partition exists, offsets valid). 3 If enough data is available, reads directly from log using zero-copy I/O. 4 If not enough data, broker delays the response in Fetch Purgatory. 5 When either <code>min_bytes</code> satisfied or timeout expires, broker sends response. 6 Consumer processes messages and repeats the cycle."},{"location":"streaming/kafka/39-Kafka_Fetch_Requests_Pt1/#in-essence","title":"In essence","text":"<p>Kafka fetch requests are designed for efficiency, safety, and flexibility:</p> <ul> <li>Efficiency: Zero-copy transfer minimizes overhead.</li> <li>Safety: Upper limits protect client memory.</li> <li>Flexibility: Lower limits and timeouts optimize polling frequency.</li> <li>Scalability: Same mechanism used by both consumers and replica followers, simplifying the system.</li> </ul>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/","title":"Kafka Fetch Requests Part II","text":""},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#1-the-main-idea-not-all-data-on-the-leader-is-visible-to-consumers","title":"1. The main idea \u2014 \u201cNot all data on the leader is visible to consumers\u201d","text":"<p>Every Kafka leader broker continuously receives new messages from producers. However, consumers cannot immediately see every message the leader has written.</p> <p>Why?</p> <p>Because Kafka\u2019s design ensures that consumers only see data that is guaranteed to be durable \u2014 that is, data replicated to all in-sync replicas (ISR).</p> <p>This rule guarantees that if the current leader fails, another replica (which becomes the new leader) still has those same messages. Therefore, consumers will never read messages that could later \u201cdisappear\u201d due to a broker failure.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#2-two-categories-of-messages-on-the-leader","title":"2. Two categories of messages on the leader","text":"<p>Let\u2019s define two sets of messages on the leader\u2019s log:</p> Category Description Visibility Replicated messages Messages that have been written to all in-sync replicas (ISR). Visible to consumers. Unreplicated messages Messages that are still only on the leader; followers haven\u2019t fetched them yet. Hidden from consumers. <p>These unreplicated messages are considered unsafe.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#3-why-unsafe-messages-are-hidden","title":"3. Why unsafe messages are hidden","text":"<p>Let\u2019s imagine what would happen if Kafka allowed consumers to read messages that only existed on the leader:</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#example-scenario","title":"Example scenario:","text":"<ul> <li> <p>Partition <code>A-0</code> has a replication factor of 3.</p> </li> <li> <p>Leader = Broker 1</p> </li> <li>Followers = Broker 2, Broker 3</li> <li> <p>ISR = {1, 2, 3}</p> </li> <li> <p>Producer sends a message <code>M1</code> to Broker 1 (leader).</p> </li> <li>Broker 1 appends <code>M1</code> to its local log.</li> <li>Broker 1 has not yet replicated <code>M1</code> to the followers (2, 3).</li> </ul> <p>Now two things could happen:</p> <ul> <li>If Broker 1 crashes now, followers 2 and 3 don\u2019t have <code>M1</code>.   When Broker 2 becomes the new leader, that message is lost forever \u2014 it never existed on the new leader\u2019s log.</li> <li>If a consumer had read <code>M1</code> before the crash, it would have seen data that no longer exists in Kafka \u2014 an inconsistency.</li> </ul> <p>Kafka\u2019s design explicitly prevents this: Consumers are not allowed to read messages that haven\u2019t been replicated to all ISR members yet. So, in this scenario, <code>M1</code> stays invisible to consumers until it\u2019s safely replicated.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#4-how-kafka-enforces-this-the-high-watermark-hw","title":"4. How Kafka enforces this \u2014 the \u201cHigh Watermark\u201d (HW)","text":"<p>Kafka tracks a special marker for each partition called the High Watermark (HW).</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#definition","title":"Definition:","text":"<p>The High Watermark is the highest offset that has been replicated to all in-sync replicas.</p> <ul> <li>All messages below or equal to the HW are considered committed.</li> <li>All messages above the HW are uncommitted and invisible to consumers.</li> </ul> <p>This is how Kafka decides what data can be fetched by consumer fetch requests.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#visual-example","title":"Visual example:","text":"<pre><code>Partition log on the leader:\nOffset: 0   1   2   3   4   5   6\nData:   A   B   C   D   E   F   G\n               ^ \n               |\n             High Watermark = 3\n</code></pre> <ul> <li>Messages A, B, C, D \u2192 replicated to all ISR \u2192 committed, visible to consumers.</li> <li>Messages E, F, G \u2192 not yet replicated to all ISR \u2192 uncommitted, invisible to consumers.</li> </ul> <p>A consumer fetching from this partition will only receive A\u2013D, even though the leader\u2019s local log already contains E\u2013G.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#5-followers-and-replication-interaction","title":"5. Followers and replication interaction","text":"<p>Each follower continuously fetches data from the leader. As followers replicate new messages and acknowledge them:</p> <ol> <li>The leader updates the ISR status.</li> <li>The leader advances the High Watermark (HW) to the highest offset replicated to all ISR members.</li> <li>Messages up to this new HW now become visible to consumers.</li> </ol> <p>Followers themselves are not subject to this visibility rule, because:</p> <ul> <li>Followers must read all messages (even uncommitted ones) from the leader to stay in sync.</li> <li>If followers were restricted like consumers, replication would never catch up.</li> </ul>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#6-the-durability-reason-avoiding-phantom-reads","title":"6. The durability reason \u2014 avoiding \u201cphantom reads\u201d","text":"<p>Kafka hides unreplicated messages to prevent inconsistency across consumers after a leader failure.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#example-of-inconsistency-if-this-rule-didnt-exist","title":"Example of inconsistency (if this rule didn\u2019t exist):","text":"<ol> <li>Leader receives and appends message <code>M1</code> (not replicated yet).</li> <li>Consumer reads <code>M1</code>.</li> <li>Leader crashes before <code>M1</code> is replicated.</li> <li>Follower becomes the new leader \u2014 <code>M1</code> doesn\u2019t exist on it.</li> <li> <p>Now:</p> </li> <li> <p>Consumer A (who read <code>M1</code>) believes <code>M1</code> exists.</p> </li> <li>Consumer B (starting later) never sees <code>M1</code>.</li> </ol> <p>This breaks Kafka\u2019s fundamental guarantee that:</p> <p>Once a message is committed (visible), it will never disappear.</p> <p>Hence, Kafka enforces the rule:</p> <ul> <li>Only committed (replicated) messages are readable by consumers.</li> </ul> <p>This preserves linearizability and read-after-write consistency for durable data.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#7-performance-impact-replication-lag-delays-consumer-visibility","title":"7. Performance impact \u2014 replication lag delays consumer visibility","text":"<p>Because Kafka waits for replication before advancing the High Watermark, slow replication directly affects how quickly consumers see new data.</p> <p>If followers lag behind due to:</p> <ul> <li>Network delays,</li> <li>Disk I/O bottlenecks,</li> <li>Broker overload,</li> </ul> <p>then:</p> <ul> <li>The leader cannot advance the HW.</li> <li>Consumers will not see newly produced messages until followers catch up.</li> </ul> <p>This is a key operational consideration for Kafka performance tuning.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#8-how-long-can-a-follower-lag-before-its-removed-from-isr","title":"8. How long can a follower lag before it\u2019s removed from ISR?","text":"<p>The delay in message visibility is bounded by a broker configuration:</p> <pre><code>replica.lag.time.max.ms\n</code></pre>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#meaning","title":"Meaning:","text":"<p>If a follower fails to catch up (replicate new messages) within this time window:</p> <ul> <li>The follower is removed from the ISR.</li> <li>The leader then only waits for the remaining ISR replicas to acknowledge new messages.</li> </ul> <p>This prevents one slow follower from blocking message visibility for all consumers.</p>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#9-lifecycle-summary-from-produce-to-consumer-visibility","title":"9. Lifecycle summary \u2014 from produce to consumer visibility","text":"Stage Description Visible to Consumers? 1. Producer sends message Leader appends message locally. \u274c Not yet replicated. 2. Followers replicate message Followers fetch and append it. \u274c Still waiting for all ISR acknowledgments. 3. All ISR replicas have replicated Leader advances High Watermark (HW). \u2705 Now visible. 4. Consumer fetches from broker Broker serves messages \u2264 HW. \u2705 Consumer sees it."},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#10-summary-of-key-terms-and-concepts","title":"10. Summary of key terms and concepts","text":"Term Meaning ISR (In-Sync Replicas) The set of replicas currently up to date with the leader. High Watermark (HW) Highest offset replicated to all ISR members. Defines the \u201ccommitted\u201d boundary. Committed messages Messages \u2264 HW; safe and visible to consumers. Uncommitted messages Messages &gt; HW; exist only on leader; hidden from consumers. replica.lag.time.max.ms Maximum allowed replication delay before a follower is removed from ISR. Unsafe messages Messages not yet replicated to ISR; can be lost if leader fails."},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#11-practical-implications","title":"11. Practical implications","text":""},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#for-developers","title":"For developers:","text":"<ul> <li>A producer might think a message was written (if <code>acks=1</code>) before it\u2019s visible to consumers. Visibility depends on replication speed and HW movement.</li> <li>Consumers will never see messages that might later disappear \u2014 even if they exist in the leader\u2019s local log.</li> </ul>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#for-operators","title":"For operators:","text":"<ul> <li> <p>Monitor replication lag and ISR size:</p> </li> <li> <p>If lag increases, consumer delay increases.</p> </li> <li>If ISR shrinks, the system risks losing durability if the leader fails.</li> <li>Tune <code>replica.lag.time.max.ms</code> to balance tolerance for temporary slowness vs. responsiveness.</li> </ul>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#12-analogy","title":"12. Analogy","text":"<p>Think of the High Watermark as Kafka\u2019s \u201ccommit point.\u201d</p> <ul> <li>Messages below HW \u2192 committed, safe, visible.</li> <li>Messages above HW \u2192 uncommitted, pending replication.</li> </ul> <p>This is conceptually similar to:</p> <ul> <li>Database transactions where only committed transactions are visible to readers.</li> <li>The write-ahead log (WAL) mechanism where unflushed entries are not yet permanent.</li> </ul>"},{"location":"streaming/kafka/40-Kafka_Fetch_Requests_Pt2/#13-in-summary","title":"13. In summary","text":"<ol> <li>Leaders receive messages first; followers replicate them.</li> <li>Consumers only read up to the High Watermark (HW).</li> <li>Messages not yet replicated to all ISR members are invisible and unsafe.</li> <li>This ensures data consistency and no phantom messages after leader failure.</li> <li>Replication lag can delay visibility \u2014 bounded by <code>replica.lag.time.max.ms</code>.</li> <li>Kafka\u2019s durability and ordering guarantees depend on this careful distinction between committed and uncommitted data.</li> </ol> <p>In some cases, a consumer consumes events from a large number of partitions. Sending the list of all the partitions it is interested in to the broker with every request and having the broker send all its metadata back can be very inefficient\u2014the set of partitions rarely changes, their metadata rarely changes, and in many cases there isn\u2019t that much data to return. To minimize this overhead, Kafka has fetch session cache. </p> <p>Consumers can attempt to create a cached session that stores the list of partitions they are consuming from and its metadata. Once a session is created, consumers no longer need to specify all the partitions in each request and can use incremental fetch requests instead. Brokers will only include metadata in the response if there were any changes. </p> <p>The session cache has limited space, and Kafka prioritizes follower replicas and consumers with a large set of partitions, so in some cases a session will not be created or will be evicted. In both these cases the broker will return an appropriate error to the client, and the consumer will transparently resort to full fetch requests that include all the partition metadata.</p>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/","title":"Physical Storage in Kafka","text":"<p>Got it \u2014 let\u2019s keep RAID short and focus mainly on what that passage about Kafka\u2019s storage architecture means.</p>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#1-the-core-idea-of-the-passage","title":"1. The core idea of the passage","text":"<p>The passage explains how Kafka stores its data on disk \u2014 specifically, how partitions and replicas are physically laid out and managed by brokers.</p> <p>Kafka\u2019s basic storage unit is a partition replica:</p> <ul> <li>Each topic is divided into partitions.</li> <li>Each partition has one or more replicas (copies), stored on different brokers for fault tolerance.</li> <li>Each broker stores some replicas on its local disks.</li> </ul>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#2-one-partition-replica-one-storage-location","title":"2. One partition replica = one storage location","text":"<ul> <li>A partition replica cannot be split across multiple brokers or disks.   \u2192 This means all the data for that partition\u2019s replica sits entirely on one mount point (disk location).</li> <li>Therefore, the maximum size of a single partition is limited by how much free space exists on one disk or one RAID volume.</li> </ul>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#3-disks-and-mount-points","title":"3. Disks and mount points","text":"<p>A mount point is a storage path that Kafka can write data to. Administrators can configure multiple mount points so Kafka can spread partitions across disks.</p> <ul> <li>If Kafka uses JBOD (Just a Bunch Of Disks) \u2192 each disk is a separate mount point.</li> <li>If Kafka uses RAID \u2192 multiple disks can act as one large logical mount point.</li> </ul> <p>Simple difference:</p> <ul> <li>JBOD: each disk independent \u2192 lose partitions on that disk if it fails.</li> <li>RAID: combines disks into one logical volume \u2192 can survive certain disk failures depending on RAID level.</li> </ul>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#4-kafkas-configuration-logdirs","title":"4. Kafka\u2019s configuration \u2014 <code>log.dirs</code>","text":"<p>Kafka has a configuration parameter:</p> <pre><code>log.dirs=/data1/kafka,/data2/kafka,/data3/kafka\n</code></pre> <p>This tells the broker:</p> <p>\u201cHere are the directories (mount points) where you can store partition data.\u201d</p> <p>Kafka automatically distributes partitions across these directories to balance disk usage. This is not the same as <code>log4j.properties</code>, which controls where Kafka writes error and system logs (the broker\u2019s own event logs).</p>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#5-how-kafka-uses-those-directories","title":"5. How Kafka uses those directories","text":"<ol> <li>When a topic is created, Kafka\u2019s controller assigns each partition to a broker.</li> <li>On that broker, Kafka chooses one directory from <code>log.dirs</code> to place that partition\u2019s data.</li> <li>The partition data is stored as a set of log segment files and index files inside that directory.</li> </ol> <p>So, each partition replica physically resides in a folder path like:</p> <pre><code>/data1/kafka/topicA-0\n/data2/kafka/topicA-1\n/data3/kafka/topicB-0\n</code></pre>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#7-brief-note-on-raid-as-mentioned-in-the-passage","title":"7. Brief note on RAID (as mentioned in the passage)","text":"<ul> <li>RAID (Redundant Array of Independent Disks) = a way to combine multiple disks into one logical unit.</li> <li> <p>Kafka can run on:</p> </li> <li> <p>JBOD: separate disks per <code>log.dirs</code> (common setup; simpler for Kafka).</p> </li> <li>RAID: group of disks acting as one big mount point (used if you want redundancy at the storage level).</li> </ul> <p>The passage just references RAID to explain what a \u201cmount point\u201d might represent \u2014 it could be:</p> <ul> <li>a single disk (JBOD), or</li> <li>a RAID array (multiple disks combined).</li> </ul>"},{"location":"streaming/kafka/41-Kafka_Physical_Storage_Introduction/#8-summary-simple","title":"8. Summary (simple)","text":"Concept Meaning Partition replica Smallest unit of Kafka storage; one per broker per partition. No splitting Each partition replica\u2019s data stays on one disk/mount point only. <code>log.dirs</code> List of directories (one per disk or RAID volume) where Kafka stores partition data. Mount point A disk or RAID volume where data can be written. RAID mention Just an example that a mount point might represent multiple disks acting together. Next topics Allocation, file management, retention, and log compaction."},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/","title":"Tiered Storage in Kafka","text":""},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#1-why-kafka-needed-tiered-storage","title":"1. Why Kafka needed tiered storage","text":"<p>Before tiered storage, Kafka stored all data locally on broker disks. This worked well for moderate retention, but created hard limits when Kafka was used for high-throughput or long-retention use cases.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#problems-with-local-only-storage","title":"Problems with local-only storage","text":""},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#a-storage-capacity-limits","title":"a. Storage capacity limits","text":"<ul> <li>Each partition replica resides on a single broker and cannot be split across disks.</li> <li>The partition size is therefore limited by the disk size of a single mount point.</li> <li>If a team wanted to retain weeks or months of data, Kafka\u2019s local disks often filled up quickly.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#b-cost-inefficiency","title":"b. Cost inefficiency","text":"<ul> <li>To increase storage, you had to add more brokers, even if CPU and memory were underutilized.</li> <li>This made Kafka clusters larger and more expensive than necessary, since scaling storage required scaling compute too.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#c-low-elasticity","title":"c. Low elasticity","text":"<ul> <li>When scaling up or down (e.g., adding/removing brokers), moving large partitions between brokers took a long time.</li> <li>Kafka rebalance and recovery operations were slower because huge amounts of data had to be copied between nodes.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#d-external-pipelines","title":"d. External pipelines","text":"<ul> <li>Organizations often created additional pipelines to offload old Kafka data to cheaper storage (e.g., S3, HDFS).</li> <li>This duplicated infrastructure and increased operational complexity.</li> </ul> <p>So Kafka needed a way to: \u2705 Keep recent, \u201chot\u201d data close to the broker for low-latency access. \u2705 Offload older, \u201ccold\u201d data to cheaper and more scalable storage. \u2705 Do this transparently, without breaking existing clients or APIs.</p> <p>That\u2019s what Tiered Storage provides.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#2-what-is-tiered-storage-in-kafka","title":"2. What is Tiered Storage in Kafka?","text":"<p>Tiered storage introduces a two-layer (dual-tier) architecture for Kafka logs:</p> Tier Description Typical storage type Purpose Local Tier Existing Kafka storage on local broker disks SSDs or HDDs For hot, recent data; low latency Remote Tier New, external long-term storage S3, HDFS, or cloud blob store For older, cold data; cheap &amp; scalable"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#in-simple-terms","title":"In simple terms:","text":"<p>Kafka keeps recent segments locally (fast disk), and older, completed segments are uploaded to remote storage.</p> <p>This allows Kafka to behave both as a high-performance message broker and a long-term event store \u2014 without separate data pipelines.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#3-how-it-works","title":"3. How it works","text":""},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#a-log-segments","title":"a. Log segments","text":"<ul> <li>Kafka stores each partition\u2019s data as a series of log segments (files).</li> <li>As the partition grows, older segments are rolled over (closed) and new ones are started.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#b-remote-offloading","title":"b. Remote offloading","text":"<ul> <li>Once a segment is closed (no new messages are appended), Kafka can upload it to the remote tier (e.g., S3 or HDFS).</li> <li>The local copy can then be retained for a shorter time and eventually deleted based on local retention policy.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#c-dual-retention-policies","title":"c. Dual retention policies","text":"<p>You can define different retention policies for the two tiers:</p> <ul> <li>Local retention: how long segments stay on broker disk (e.g., a few hours).</li> <li>Remote retention: how long segments stay in the remote tier (e.g., days, months, or indefinitely).</li> </ul> <p>This means brokers maintain only the active tail of the log locally, while the remote tier keeps historical data for long-term access.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#4-accessing-data-from-both-tiers","title":"4. Accessing data from both tiers","text":"<p>Kafka abstracts both storage layers under the same log interface.</p> <ul> <li> <p>Consumers reading recent data (tail reads)   \u2192 Served from local tier (fast disk I/O and page cache).   These are typically real-time stream processors or dashboards.</p> </li> <li> <p>Consumers reading older data (backfill or reprocessing)   \u2192 Served from remote tier (fetched over the network from S3/HDFS).   These are typically analytics, ETL, or replay jobs.</p> </li> </ul> <p>The switch between local and remote is transparent \u2014 clients don\u2019t need to know where the data physically lives.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#5-components-introduced-by-tiered-storage","title":"5. Components introduced by Tiered Storage","text":""},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#a-remotelogmanager","title":"a. RemoteLogManager","text":"<p>A new internal Kafka component introduced in KIP-405. It handles:</p> <ul> <li>Tracking which log segments have been uploaded.</li> <li>Coordinating uploads, downloads, and deletions of remote segments.</li> <li>Integrating with retention policies for both tiers.</li> </ul> <p>Each broker runs a RemoteLogManager to manage the remote copies of segments it owns as leader.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#b-remote-storage-connector-plugin-layer","title":"b. Remote Storage Connector (plugin layer)","text":"<p>Kafka uses a pluggable interface to talk to various remote systems. For example:</p> <ul> <li>AWS S3 connector</li> <li>HDFS connector</li> <li>Azure Blob connector</li> </ul> <p>These connectors implement how to upload, download, list, and delete log segments.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#c-metadata-tracking","title":"c. Metadata tracking","text":"<p>Kafka must keep track of:</p> <ul> <li>Which segments exist locally,</li> <li>Which are in remote storage,</li> <li>And which offsets correspond to each.</li> </ul> <p>This metadata is replicated via the Kafka controller and metadata topic, ensuring consistency across brokers.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#6-benefits-of-tiered-storage","title":"6. Benefits of Tiered Storage","text":""},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#1-infinite-practically-retention","title":"1. Infinite (practically) retention","text":"<ul> <li>Because old data is offloaded, retention is no longer constrained by broker disk size.</li> <li>You can store months or years of data at a fraction of the cost.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#2-lower-cost-per-gb","title":"2. Lower cost per GB","text":"<ul> <li>Brokers only need fast storage for recent data.</li> <li>Older data can live in cheaper cloud object storage (S3, GCS, Azure Blob, HDFS).</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#3-independent-scaling","title":"3. Independent scaling","text":"<ul> <li>Storage can now scale independently from compute.</li> <li>You can add capacity to S3 without adding brokers.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#4-faster-elasticity-and-recovery","title":"4. Faster elasticity and recovery","text":"<ul> <li> <p>Rebalancing or replacing brokers becomes much faster:</p> </li> <li> <p>Old segments already in remote storage don\u2019t need to be copied again.</p> </li> <li>Brokers only rebuild metadata and local active segments.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#5-isolation-between-workloads","title":"5. Isolation between workloads","text":"<ul> <li>Previously, consumers reading old data competed with real-time consumers for disk I/O.</li> <li> <p>With tiered storage:</p> </li> <li> <p>Old reads \u2192 remote storage (network I/O)</p> </li> <li>New reads \u2192 local disk</li> <li>Result: better latency isolation and less interference between workloads.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#6-simplified-architecture","title":"6. Simplified architecture","text":"<ul> <li>No need for separate ETL pipelines to move data from Kafka to S3 or HDFS.</li> <li>Kafka itself becomes a complete event store.</li> </ul>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#7-performance-impact-based-on-kip-405-measurements","title":"7. Performance impact (based on KIP-405 measurements)","text":"<p>The Kafka team tested tiered storage in several workloads:</p> Use Case Without Tiered Storage With Tiered Storage Observation Normal high-throughput workload p99 latency \u2248 21 ms p99 latency \u2248 25 ms Slight latency increase due to background uploads to remote storage. Consumers reading old data p99 latency \u2248 60 ms p99 latency \u2248 42 ms Significant improvement because old reads come from remote storage, not competing with hot disk reads. <p>So even though there\u2019s a small penalty for offloading, the overall cluster performance improves for mixed workloads.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#8-trade-offs-and-considerations","title":"8. Trade-offs and considerations","text":"Aspect Notes Latency Slightly higher due to remote fetch for old data, but negligible for most use cases. Complexity Additional moving parts (RemoteLogManager, remote connectors, new metadata tracking). Network usage Uploading/downloading segments adds bandwidth requirements. Consistency Remote segment metadata must stay consistent during leader changes and replica catch-up. Cost Remote storage is cheaper per GB, but total cost depends on egress and API call pricing in cloud environments."},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#9-comparison-before-vs-after-tiered-storage","title":"9. Comparison: Before vs After Tiered Storage","text":"Feature Pre-Tiered Storage With Tiered Storage Storage Entirely local Split: local + remote Retention Limited by disk Practically unlimited Scaling Add brokers for more storage Scale storage separately (e.g., add S3 capacity) Broker recovery Copies all data Copies only local data, references remote Cost efficiency High cost per TB Low cost per TB (cheap cloud storage) Performance isolation Old readers compete for disk I/O Old readers use remote storage, no competition"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#10-key-takeaway","title":"10. Key takeaway","text":"<p>Tiered storage transforms Kafka from a short-term streaming buffer into a long-term, cost-efficient event store.</p> <p>It provides: \u2705 Infinite retention (practically) \u2705 Lower storage cost \u2705 Easier scaling and faster recovery \u2705 Isolation between real-time and historical consumers \u2705 Seamless access to historical data</p> <p>All while maintaining Kafka\u2019s same client APIs and streaming semantics.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#11-analogy","title":"11. Analogy","text":"<p>Think of Kafka Tiered Storage like your phone\u2019s photo library:</p> <ul> <li>Recent photos \u2192 stored locally on your phone (fast, small).</li> <li>Old photos \u2192 automatically moved to cloud storage (cheap, large).   You can still view any photo seamlessly \u2014 the system just fetches it from the right place when needed.</li> </ul> <p>Kafka\u2019s tiered storage works the same way for log segments.</p>"},{"location":"streaming/kafka/42-Kafka_Tiered_Storage/#12-summary","title":"12. Summary","text":"Concept Description Motivation Overcome storage limits, reduce cost, improve elasticity. Architecture Two tiers \u2014 local (fast disks) and remote (cheap, scalable). New component <code>RemoteLogManager</code> manages uploads, downloads, metadata. Retention Separate policies for local and remote tiers. Performance Slight latency increase for writes, improved isolation for reads. Benefit Kafka becomes a long-term, scalable, cost-effective event store. <p>One interesting result that is documented in KIP-405 is the performance implications of tiered storage. The team implementing tiered storage measured performance in several use cases. The first was using Kafka\u2019s usual highthroughput workload. In that case, latency increased a bit (from 21 ms in p99 to 25 ms), since brokers also have to ship segments to remote storage. The second use case was when some consumers are reading old data. </p> <p>Without tiered storage, consumers reading old data have a large impact on latency (21 ms versus 60 ms p99), but with tiered storage enabled, the impact is significantly lower (25 ms versus 42 ms p99); this is because tiered storage reads are read from HDFS or S3 via a network path. Network reads do not compete with local reads on disk I/O or page cache, and leave the page cache intact with fresh data. This means that in addition to infinite storage, lower costs, and elasticity, tiered storage also delivers isolation between historical reads and real-time reads.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/","title":"Partition Allocation Concepts in Kafka","text":""},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#1-the-goal-balanced-fault-tolerant-partition-placement","title":"1. The goal: balanced, fault-tolerant partition placement","text":"<p>When you create a topic in Kafka, you specify:</p> <ul> <li>Number of partitions (e.g., 10)</li> <li>Replication factor (e.g., 3)</li> </ul> <p>This means:</p> <p>For each partition, Kafka needs to create 3 copies (replicas) \u2014 one leader and two followers.</p> <p>So, 10 partitions \u00d7 3 replicas = 30 total partition replicas to be distributed across the brokers.</p> <p>In the example:</p> <ul> <li>Brokers = 6</li> <li>Partitions = 10</li> <li>Replication factor = 3</li> <li>Total replicas = 30</li> </ul> <p>Kafka\u2019s job is to spread those 30 replicas across the 6 brokers in a way that achieves:</p> <ol> <li>Even distribution (no broker overloaded with replicas)</li> <li>No duplicate replicas of the same partition on one broker</li> <li>Rack-level fault tolerance (if rack info is available)</li> </ol>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#2-step-1-even-distribution-across-brokers","title":"2. Step 1 \u2014 Even distribution across brokers","text":"<p>Kafka tries to assign roughly the same number of replicas per broker.</p> <p>In the example:</p> <ul> <li>30 replicas \u00f7 6 brokers = 5 replicas per broker (on average).</li> </ul> <p>This ensures load balance \u2014 each broker holds approximately the same amount of data and handles similar traffic.</p> <p>So, no single broker becomes a hotspot for storage or leader requests.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#3-step-2-one-replica-per-broker-per-partition","title":"3. Step 2 \u2014 One replica per broker per partition","text":"<p>Kafka ensures that:</p> <p>For any given partition, its replicas (leader and followers) are placed on different brokers.</p> <p>Example:</p> <ul> <li>Partition 0 leader \u2192 Broker 2</li> <li>Followers \u2192 Brokers 3 and 4</li> </ul> <p>Not allowed:</p> <ul> <li>Two replicas of partition 0 on the same broker.</li> <li>Leader and follower on the same broker.</li> </ul> <p>This guarantees that if one broker fails, at least one copy of each partition still exists on another broker.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#4-step-3-assigning-partition-leaders-round-robin","title":"4. Step 3 \u2014 Assigning partition leaders (round-robin)","text":"<p>Kafka determines which broker will hold the leader replica for each partition using a round-robin approach.</p> <p>Example:</p> <ul> <li>Brokers: 0, 1, 2, 3, 4, 5</li> <li>Random starting broker: say Broker 4</li> </ul> <p>Kafka loops over brokers in order to assign leaders:</p> Partition Leader Broker 0 4 1 5 2 0 3 1 4 2 5 3 6 4 7 5 8 0 9 1 <p>This round-robin pattern keeps leaders evenly distributed across all brokers.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#5-step-4-assigning-follower-replicas","title":"5. Step 4 \u2014 Assigning follower replicas","text":"<p>Once the leader brokers are chosen, Kafka assigns the follower replicas.</p> <p>Rule:</p> <p>Each follower is placed on a broker at an increasing offset from the leader.</p> <p>Example:</p> <ul> <li> <p>Leader for partition 0 \u2192 Broker 4   \u2192 Follower 1 on Broker 5   \u2192 Follower 2 on Broker 0</p> </li> <li> <p>Leader for partition 1 \u2192 Broker 5   \u2192 Follower 1 on Broker 0   \u2192 Follower 2 on Broker 1</p> </li> </ul> <p>This ensures:</p> <ul> <li>Replicas are spread across multiple brokers.</li> <li>No two replicas of the same partition end up on the same node.</li> </ul>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#6-step-5-rack-awareness-added-in-kafka-0100","title":"6. Step 5 \u2014 Rack awareness (added in Kafka 0.10.0+)","text":"<p>If your cluster defines rack information (for example, <code>broker.rack=rackA</code> in each broker\u2019s config), Kafka can make smarter placement decisions.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#why-this-matters","title":"Why this matters","text":"<p>If an entire rack fails (for example, due to power or network outage), you don\u2019t want all replicas of a partition to be on that rack \u2014 otherwise the partition becomes unavailable.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#how-kafka-handles-it","title":"How Kafka handles it","text":"<p>Instead of assigning brokers in numeric order (0, 1, 2, 3\u2026), Kafka builds a rack-alternating broker list.</p> <p>Example:</p> <ul> <li>Rack 1 \u2192 Brokers 0 and 1</li> <li>Rack 2 \u2192 Brokers 2 and 3</li> </ul> <p>Normal order: 0, 1, 2, 3 Rack-alternating order: 0, 2, 1, 3</p> <p>Now, if the leader for a partition is on broker 2 (rack 2):</p> <ul> <li>The first follower might be on broker 1 (rack 1).</li> <li>The second follower could be on broker 3 (rack 2) or broker 0 (rack 1), depending on the pattern.</li> </ul> <p>Result: \u2705 Each partition has replicas spread across different racks, \u2705 So, if one rack goes down, there\u2019s still a live replica on another rack.</p> <p>This dramatically improves fault tolerance and availability.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#7-step-6-choosing-the-disk-directory-for-each-partition","title":"7. Step 6 \u2014 Choosing the disk directory for each partition","text":"<p>Once the broker for each replica is chosen, Kafka decides which disk (directory) on that broker will store the partition.</p> <p>This is where the <code>log.dirs</code> setting comes in.</p> <p>Example:</p> <pre><code>log.dirs=/data1/kafka,/data2/kafka,/data3/kafka\n</code></pre> <p>Each path represents a mount point or disk.</p> <p>Kafka uses a simple rule:</p> <p>Place the new partition on the directory that currently has the fewest partitions.</p> <p>So if <code>/data3/kafka</code> is empty or least used, new partitions go there first.</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#why-this-helps","title":"Why this helps","text":"<ul> <li>Balances storage evenly across disks.</li> <li>If you add a new disk, Kafka automatically starts using it immediately because it initially has fewer partitions.</li> <li>This dynamic balancing happens only during partition creation, not for existing data (Kafka doesn\u2019t move old partitions automatically).</li> </ul>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#8-example-end-to-end","title":"8. Example end-to-end","text":"<p>Let\u2019s put it all together.</p> <p>Cluster setup:</p> <ul> <li>6 brokers (0\u20135)</li> <li>10 partitions</li> <li>Replication factor = 3</li> <li>Rack-aware placement enabled</li> </ul> <p>Resulting behavior:</p> <ol> <li>Kafka creates 30 replicas total.</li> <li>Distributes leaders evenly among brokers.</li> <li>Assigns followers at increasing broker offsets.</li> <li>Ensures all replicas of a partition are on different brokers.</li> <li>Uses rack info to ensure replicas are on different racks.</li> <li>On each broker, assigns replicas to the disk (directory) with the fewest partitions.</li> </ol> <p>This yields: \u2705 Balanced leader load \u2705 Balanced follower load \u2705 Fault tolerance across brokers and racks \u2705 Balanced use of storage disks</p>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#9-why-this-matters","title":"9. Why this matters","text":"<ul> <li>Performance:   Load is evenly distributed, so no broker or disk becomes a bottleneck.</li> <li>Reliability:   Replicas on different brokers and racks protect against single points of failure.</li> <li>Scalability:   Easy to add new brokers or disks \u2014 Kafka will place future partitions intelligently.</li> <li>Simplicity:   Rules are deterministic and easy to reason about; administrators can predict where data will go.</li> </ul>"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#10-summary-table","title":"10. Summary table","text":"Step Goal Mechanism 1 Even distribution Spread replicas evenly across brokers 2 Avoid duplication No two replicas of same partition on one broker 3 Balanced leaders Assign leaders round-robin 4 Balanced followers Assign followers with offset placement 5 Rack fault tolerance Use rack-alternating broker list 6 Disk balancing Assign to directory with fewest partitions"},{"location":"streaming/kafka/43-Kafka_Partition_Allocation/#11-key-takeaway","title":"11. Key takeaway","text":"<p>When you create a topic, Kafka\u2019s internal partition assignment algorithm:</p> <ul> <li>Ensures balanced, resilient distribution of data and leadership across brokers,</li> <li>Accounts for rack-level redundancy if available,</li> <li>And balances disk usage across available mount points.</li> </ul> <p>This allows Kafka clusters to scale predictably and stay highly available even when disks, brokers, or entire racks fail.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/","title":"Introduction to File Formats","text":"<p>Excellent \u2014 this passage goes deep into how Kafka physically stores and transmits messages, and how its message format, batching, and compression work together to make Kafka so efficient.</p> <p>Let\u2019s go step-by-step and make everything clear.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#1-each-partition-segment-one-data-file-on-disk","title":"1. Each partition segment = one data file on disk","text":"<p>Kafka stores data for each partition as a series of segment files on disk. Each segment corresponds to a range of message offsets.</p> <p>Example folder structure:</p> <pre><code>/data/kafka/orders-0/\n \u251c\u2500\u2500 00000000000000000000.log      \u2190 first segment (offsets 0\u2013999)\n \u251c\u2500\u2500 00000000000000001000.log      \u2190 second segment (offsets 1000\u20131999)\n \u251c\u2500\u2500 00000000000000002000.log      \u2190 third segment, etc.\n</code></pre> <p>Each <code>.log</code> file (segment) contains the Kafka messages themselves \u2014 as a continuous byte stream of records.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#2-the-data-inside-a-segment-message-format","title":"2. The data inside a segment \u2014 message format","text":"<p>Inside each segment, Kafka stores:</p> <ul> <li>The message payload (your data)</li> <li>The offset (the unique sequential number for ordering)</li> <li>And headers and metadata (CRC checksums, timestamps, keys, etc.)</li> </ul> <p>The key point is this line:</p> <p>\u201cThe format of the data on disk is identical to the format of the messages that are sent over the network.\u201d</p> <p>That\u2019s one of Kafka\u2019s most brilliant design choices.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#3-why-kafka-uses-the-same-format-on-disk-and-on-the-wire","title":"3. Why Kafka uses the same format on disk and on the wire","text":"<p>This design means that:</p> <ul> <li>When producers send data \u2192 it\u2019s written to disk as-is.</li> <li>When consumers fetch data \u2192 it\u2019s read from disk as-is.</li> </ul> <p>Kafka doesn\u2019t need to:</p> <ul> <li>Decode or re-encode messages.</li> <li>Decompress or recompress payloads.</li> </ul> <p>This has two massive performance advantages:</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#a-zero-copy-optimization","title":"a. Zero-copy optimization","text":"<p>Kafka uses the Linux system call <code>sendfile()</code>, which transfers data directly:</p> <pre><code>Disk \u2192 Kernel buffer \u2192 Network socket\n</code></pre> <p>No extra copy into user-space memory.</p> <p>Result:</p> <ul> <li>Fewer CPU cycles.</li> <li>Higher throughput.</li> <li>Lower latency.</li> <li>Lower garbage collection overhead.</li> </ul> <p>This is called zero-copy I/O \u2014 data goes straight from disk to network.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#b-no-decompressionrecompression-overhead","title":"b. No decompression/recompression overhead","text":"<p>If the producer sent compressed data (e.g., gzip, Snappy, LZ4), Kafka writes it to disk still compressed.</p> <p>Later, when a consumer fetches the same data:</p> <ul> <li>The broker doesn\u2019t decompress it.</li> <li>The consumer receives the same compressed bytes and decompresses them itself.</li> </ul> <p>That saves CPU time on the broker and reduces both:</p> <ul> <li>Disk I/O (less data written)</li> <li>Network usage (less data sent)</li> </ul> <p>So Kafka is extremely efficient at moving large volumes of data.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#4-message-structure-record-format","title":"4. Message structure (record format)","text":"<p>Each Kafka message (also called a record) contains two parts:</p> Section Contents User payload - Optional key (used for partitioning)   - Value (the actual data you produce)   - Optional headers (key/value metadata like <code>source=app1</code>) System metadata - Offset (position in the log)  - Timestamp  - CRC checksum  - Compression info  - Batch information <p>Example conceptual view:</p> <pre><code>Offset: 105\nTimestamp: 2025-10-22T20:00:00Z\nKey: \"user_123\"\nValue: {\"order_id\": 987, \"amount\": 49.99}\nHeaders: {\"region\": \"APAC\", \"version\": \"v2\"}\n</code></pre>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#5-kafka-message-format-evolution","title":"5. Kafka message format evolution","text":"<p>Kafka\u2019s message format has evolved over time. Starting with Kafka 0.11 (Message Format v2), several key improvements were introduced:</p> Version Introduced Key Features v0/v1 (pre-0.11) Older releases Each message handled individually v2 (0.11 and later) Kafka 0.11+ Introduced message batching, headers, better compression"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#6-message-batching-introduced-in-kafka-011","title":"6. Message batching (introduced in Kafka 0.11+)","text":"<p>Kafka producers always send messages in batches \u2014 even if the batch has just one record.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#why-batching-matters","title":"Why batching matters","text":"<p>Without batching:</p> <ul> <li>Each message incurs protocol overhead (headers, checksums, network round trips).</li> <li>Disk and network utilization are inefficient.</li> </ul> <p>With batching:</p> <ul> <li>The broker receives one large blob containing multiple messages.</li> <li>Kafka writes that batch as a single unit to the log segment.</li> </ul> <p>Result: \u2705 Fewer I/O operations \u2705 Less network overhead \u2705 Better compression efficiency \u2705 Higher throughput</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#how-batching-works","title":"How batching works","text":"<p>Producers collect multiple messages in memory per partition, then send them together in one produce request.</p> <ul> <li>Each partition has its own batch buffer.</li> <li>When the buffer is full or the producer waits long enough, the batch is sent.</li> </ul> <p>Kafka uses the setting:</p> <pre><code>linger.ms\n</code></pre> <p>This defines how long to wait before sending a batch.</p> <ul> <li><code>linger.ms = 0</code> \u2192 send immediately (low latency, less batching)</li> <li><code>linger.ms = 10</code> \u2192 wait up to 10 ms to collect more messages (higher throughput, better compression)</li> </ul>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#example","title":"Example","text":"<p>If your producer sends small messages rapidly:</p> <ul> <li>With <code>linger.ms=0</code>, each message goes in its own batch (inefficient).</li> <li>With <code>linger.ms=10</code>, many messages get grouped together in one batch (efficient).</li> </ul>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#7-batching-and-compression-work-together","title":"7. Batching and compression work together","text":"<p>Producers can compress data before sending (highly recommended):</p> <pre><code>compression.type=gzip|lz4|snappy|zstd\n</code></pre> <p>When batching + compression are combined:</p> <ul> <li>Kafka compresses the entire batch (not individual messages).</li> <li>Larger batches = better compression ratio.</li> </ul> <p>So, batching reduces disk space and network traffic even more.</p> <p>Example:</p> <ul> <li>1,000 messages \u2192 compressed as one large block instead of 1,000 small ones.</li> </ul>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#8-multiple-batches-per-produce-request","title":"8. Multiple batches per produce request","text":"<p>Kafka can also send multiple batches in a single network request, as long as they belong to different partitions.</p> <p>Example:</p> <ul> <li>Batch 1 \u2192 topic A, partition 0</li> <li>Batch 2 \u2192 topic A, partition 1</li> <li>Batch 3 \u2192 topic B, partition 2</li> </ul> <p>All sent together in one produce request.</p> <p>This further minimizes network overhead (fewer TCP round-trips).</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#9-putting-it-all-together","title":"9. Putting it all together","text":"Concept Description Benefit Same format on disk and network Kafka writes exactly what it receives, no reformatting Enables zero-copy I/O Zero-copy optimization Data streamed directly from disk to socket via OS kernel Very low CPU overhead No recompression Compressed messages remain compressed on disk Lower CPU, faster throughput Message batching Producer groups messages per partition Less overhead, better performance linger.ms Wait time to collect messages before sending batch Balances latency vs throughput Compression Entire batches are compressed together Saves disk and network bandwidth"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#10-why-this-design-is-so-powerful","title":"10. Why this design is so powerful","text":"<p>Kafka\u2019s architecture is all about high-throughput, low-latency data movement. By:</p> <ul> <li>Writing data exactly as received,</li> <li>Avoiding re-encoding or recompressing,</li> <li>And leveraging batching and zero-copy I/O,</li> </ul> <p>Kafka turns disk into an extension of memory \u2014 it can serve millions of messages per second with minimal CPU and memory cost.</p>"},{"location":"streaming/kafka/44-Kafka_File_Formats_Intro/#11-quick-summary","title":"11. Quick summary","text":"Feature Description Log segment Each partition\u2019s data is split into segment files on disk. Message format = wire format Kafka uses identical binary structure on disk and over network. Zero-copy I/O OS sends data directly from disk to network, bypassing user space. Batching Producers group messages per partition before sending. linger.ms Wait time to collect messages into a batch. Compression Whole batch compressed once; improves disk and network efficiency. Result High throughput, efficient storage, minimal CPU usage. <p>\u2705 In simple terms: Kafka stores messages on disk in exactly the same format they are sent and received. This enables fast, low-overhead data transfer using zero-copy, efficient batching, and compression \u2014 which together make Kafka one of the fastest messaging systems in the world.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/","title":"Message Batch Headers in Kafka","text":""},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#1-context-what-is-a-message-batch","title":"1. Context \u2014 What is a Message Batch?","text":"<p>Starting from Kafka 0.11 (Message Format v2), producers don\u2019t send messages one by one. Instead, they send batches of messages, all belonging to the same topic-partition.</p> <p>Each batch contains:</p> <ul> <li>A batch header (metadata about the batch)</li> <li>The messages (payload records)</li> </ul> <p>You can think of a batch like this:</p> <pre><code>+--------------------------------------------------------+\n| BATCH HEADER                                           |\n|   magic number, offsets, timestamps, attributes, etc.  |\n+--------------------------------------------------------+\n| MESSAGE 1                                              |\n| MESSAGE 2                                              |\n| MESSAGE 3                                              |\n| ...                                                    |\n+--------------------------------------------------------+\n</code></pre> <p>The batch header is essential because it tells Kafka brokers and consumers how to interpret the data in that batch.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#2-batch-header-fields-explained-in-detail","title":"2. Batch header fields explained in detail","text":"<p>Let\u2019s go one by one.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#1-magic-number","title":"1\ufe0f\u20e3 Magic Number","text":"<ul> <li>A single byte that identifies the version of the message format.</li> <li>For message format v2 (Kafka 0.11+), <code>magic = 2</code>.</li> </ul> <p>Purpose: When Kafka upgrades the message format (e.g., adds new fields or changes encoding), the \u201cmagic number\u201d lets brokers and clients know how to parse it.</p> <p>If a broker receives a message with a newer version than it understands, it can handle it safely (or reject it gracefully).</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#2-offsets","title":"2\ufe0f\u20e3 Offsets","text":"<p>Each message in a partition has a sequential offset \u2014 a unique integer that defines its position in the log.</p> <p>In a batch:</p> <ul> <li><code>baseOffset</code> (or first offset): Offset of the first message in the batch.</li> <li><code>lastOffsetDelta</code>: Difference between the first and last offsets in the batch (e.g., if 10 messages, delta = 9).</li> </ul> <p>Example:</p> <pre><code>baseOffset = 1050\nlastOffsetDelta = 9\n\u2192 Messages in batch have offsets 1050\u20131059\n</code></pre> <p>Why this matters:</p> <ul> <li>Kafka needs offsets to maintain message order and to track consumer progress.</li> <li>Even if the batch is compacted later (some messages deleted), these offset ranges remain as they were originally.</li> </ul> <p>Note: When the producer first creates the batch, it doesn\u2019t know the real offsets (those are assigned by the broker leader). So, the producer sets <code>baseOffset = 0</code>. When the broker receives and writes it, the leader assigns real offsets.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#3-timestamps","title":"3\ufe0f\u20e3 Timestamps","text":"<p>Each batch stores:</p> <ul> <li><code>baseTimestamp</code>: Timestamp of the first message.</li> <li><code>maxTimestamp</code>: Highest timestamp in the batch.</li> </ul> <p>Two timestamp modes:</p> <ul> <li>Create time: Set by producer (when event is created).</li> <li>Log append time: Set by broker (when message written to log).</li> </ul> <p>This is controlled by:</p> <pre><code>log.message.timestamp.type = CreateTime | LogAppendTime\n</code></pre> <p>Why it matters:</p> <ul> <li>Helps time-based retention policies (<code>retention.ms</code>).</li> <li>Allows consumers to search or filter messages by timestamp.</li> <li>Used in metrics and stream processing time semantics.</li> </ul>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#4-batch-size-in-bytes","title":"4\ufe0f\u20e3 Batch size (in bytes)","text":"<p>Indicates how large the entire batch is \u2014 including header and messages.</p> <p>Purpose:</p> <ul> <li>Allows the broker or consumer to know how much data to read.</li> <li>Useful for validating data integrity and efficient parsing.</li> </ul>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#5-leader-epoch","title":"5\ufe0f\u20e3 Leader Epoch","text":"<p>The leader epoch is a number that identifies which broker was the leader of a partition when this batch was written.</p> <p>Why it\u2019s needed:</p> <ul> <li>During leader elections, offsets can diverge.</li> <li>The leader epoch helps Kafka detect and truncate stale data on replicas after leader changes.</li> <li>It ensures all replicas realign to the same log during recovery.</li> </ul> <p>This mechanism was standardized in KIP-101 and KIP-279.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#6-checksum-crc32c","title":"6\ufe0f\u20e3 Checksum (CRC32C)","text":"<p>A checksum is a small hash (number) used to verify that the data hasn\u2019t been corrupted.</p> <p>Purpose:</p> <ul> <li>When Kafka reads a batch from disk or over the network, it recalculates the checksum and compares it.</li> <li>If it doesn\u2019t match, the broker knows the data is corrupted and can handle it safely (e.g., skip or delete segment).</li> </ul> <p>This is crucial for data integrity across disks and network transfers.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#7-attributes-16-bits-2-bytes","title":"7\ufe0f\u20e3 Attributes (16 bits / 2 bytes)","text":"<p>A bit field that encodes several binary attributes about the batch.</p> <p>It includes:</p> Bit field Meaning Bits 0\u20132 Compression type: None, GZIP, Snappy, LZ4, or ZSTD Bit 3 Timestamp type: 0 = CreateTime, 1 = LogAppendTime Bit 4 Transactional batch flag: whether this batch is part of a transaction Bit 5 Control batch flag: indicates internal control messages (e.g., commit markers) Remaining bits Reserved for future features <p>Example: <code>0000000000000101</code> \u2192 means compressed with GZIP and timestamp type = CreateTime.</p> <p>Why it matters:</p> <ul> <li>Tells Kafka how to decode the data.</li> <li>Supports transactions and control messages.</li> <li>Enables efficient compression handling.</li> </ul>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#8-producer-id-producer-epoch-and-first-sequence","title":"8\ufe0f\u20e3 Producer ID, Producer Epoch, and First Sequence","text":"<p>These fields enable Kafka\u2019s exactly-once delivery (EOS) guarantees.</p> Field Description Producer ID (PID) Unique ID assigned to each producer session. Producer Epoch Incremented each time the producer restarts or recovers. Prevents reuse of old IDs. First Sequence Sequence number of the first message in this batch (each message in batch increments it by 1). <p>Purpose:</p> <ul> <li>Kafka can detect duplicates and discard them.</li> <li>If a producer retries due to network issues, brokers use <code>(PID, Epoch, Sequence)</code> to ensure the same message isn\u2019t written twice.</li> </ul> <p>Together, these fields make exactly-once semantics possible.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#9-the-actual-messages","title":"9\ufe0f\u20e3 The actual messages","text":"<p>Finally, the batch contains the array of messages themselves.</p> <p>Each message includes:</p> <ul> <li>Key</li> <li>Value</li> <li>Headers</li> <li>Individual timestamp</li> <li>CRC for message-level validation</li> </ul> <p>But the batch header provides the context and metadata that apply to the whole group.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#3-summary-table-all-header-fields-at-a-glance","title":"3. Summary table \u2014 all header fields at a glance","text":"Field Purpose Example Magic number Version of message format <code>2</code> Base offset Offset of first message in batch <code>1050</code> Last offset delta Difference to last message offset <code>9</code> Base timestamp Timestamp of first message <code>2025-10-22T10:00:00Z</code> Max timestamp Highest timestamp in batch <code>2025-10-22T10:00:10Z</code> Batch size Total bytes in batch <code>5120 bytes</code> Leader epoch ID of leader that wrote batch <code>42</code> Checksum CRC to detect corruption <code>0x9ad33f12</code> Attributes Compression, timestamp type, transaction flag (bit flags) Producer ID Unique ID of producer <code>PID=12345</code> Producer epoch Current producer epoch <code>1</code> First sequence Sequence of first message in batch <code>seq=570</code> Messages Actual records (key, value, headers) varies"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#4-why-batching-metadata-matters","title":"4. Why batching metadata matters","text":"<p>Kafka\u2019s batch header plays several vital roles:</p> Feature Enabled by Version compatibility Magic number Ordered delivery Base offset + delta Timestamp-based retention/search Timestamps Corruption detection Checksum Compression Attributes Transaction support Transaction flags Exactly-once semantics Producer ID + Epoch + Sequence Data consistency after leader change Leader Epoch <p>So, the header isn\u2019t just metadata \u2014 it\u2019s the foundation for Kafka\u2019s reliability, speed, and correctness.</p>"},{"location":"streaming/kafka/45-Kafka_Message_Batch_Headers/#5-big-picture-what-happens-in-practice","title":"5. Big picture: what happens in practice","text":"<p>When a producer sends a batch:</p> <ol> <li>It builds the batch (with temporary base offset = 0, local timestamps).</li> <li>Sends it to the leader broker.</li> <li>Broker assigns real offsets, leader epoch, and persists the batch.</li> <li>Followers replicate the same batch.</li> <li>Consumers read the batch (zero-copy), decompress if needed, and process the individual records.</li> </ol> <p>All this happens seamlessly thanks to the information encoded in the batch header.</p> <p>As you can see, the batch header includes a lot of information. The records themselves also have system headers (not to be confused with headers that can be set by users). </p> <p>Each record includes: Size of the record, in bytes Attributes\u2014currently there are no record-level attributes, so this isn\u2019t used The difference between the offset of the current record and the first offset in the batch The difference, in milliseconds, between the timestamp of this record and the first timestamp in the batch The user payload: key, value, and headers. </p> <p>Note that there is very little overhead to each record, and most of the system information is at the batch level. Storing the first offset and timestamp of the batch in the header and only storing the difference in each record dramatically reduces the overhead of each record, making larger batches more efficient. </p> <p>In addition to message batches that contain user data, Kafka also has control batches\u2014indicating transactional commits, for instance. Those are handled by the consumer and not passed to the user application, and currently they include a version and a type indicator: 0 for an aborted transaction, 1 for a commit.</p> <p>\u2705 In short: The message batch header is a compact but powerful structure that tells Kafka how to handle, replicate, validate, and deliver messages. It enables features like exactly-once delivery, compression, timestamping, and data integrity, all while maintaining high speed and backward compatibility.</p>"},{"location":"streaming/kafka/46-Kafka_Indexes/","title":"Kafka Indexes","text":""},{"location":"streaming/kafka/46-Kafka_Indexes/#indexes-in-kafka","title":"Indexes in Kafka","text":"<p>Kafka allows consumers to start fetching messages from any available offset. This means that if a consumer asks for 1 MB messages starting at offset 100, the broker must be able to quickly locate the message for offset 100 (which can be in any of the segments for the partition) and start reading the messages from that offset on. In order to help brokers quickly locate the message for a given offset, Kafka maintains an index for each partition. The index maps offsets to segment files and positions within the file. Similarly, Kafka has a second index that maps timestamps to message offsets. </p> <p>This index is used when searching for messages by timestamp. Kafka Streams uses this lookup extensively, and it is also useful in some failover scenarios. Indexes are also broken into segments, so we can delete old index entries when the messages are purged. </p> <p>Kafka does not attempt to maintain checksums of the index. If the index becomes corrupted, it will get regenerated from the matching log segment simply by rereading the messages and recording the offsets and locations. It is also completely safe (albeit, it can cause a lengthy recovery) for an administrator to delete index segments if needed\u2014they will be regenerated automatically.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/","title":"Compaction in Kafka","text":"<p>Normally, Kafka will store messages for a set amount of time and purge messages older than the retention period. However, imagine a case where you use Kafka to store shipping addresses for your customers. In that case, it makes more sense to store the last address for each customer rather than data for just the last week or year. </p> <p>This way, you don\u2019t have to worry about old addresses, and you still retain the address for customers who haven\u2019t moved in a while. Another use case can be an application that uses Kafka to store its current state. Every time the state changes, the application writes the new state into Kafka. </p> <p>When recovering from a crash, the application reads those messages from Kafka to recover its latest state. In this case, it only cares about the latest state before the crash, not all the changes that occurred while it was running. </p> <p>Kafka supports such use cases by allowing the retention policy on a topic to be delete, which deletes events older than retention time, or to be compact, which only stores the most recent value for each key in the topic. </p> <p>Obviously, setting the policy to compact only makes sense on topics for which applications produce events that contain both a key and a value. If the topic contains null keys, compaction will fail. Topics can also have a delete.and.compact policy that combines compaction with a retention period. </p> <p>Messages older than the retention period will be removed even if they are the most recent value for a key. This policy prevents compacted topics from growing overly large and is also used when the business requires removing records after a certain time period.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#in-depth-process-of-compacting-events","title":"In Depth Process of Compacting Events","text":""},{"location":"streaming/kafka/47-Kafka_Compaction/#whats-happening-here","title":"What\u2019s happening here:","text":"<p>Kafka has a log for each partition. This log contains a sequence of messages (records), and each message has:</p> <ul> <li>a key (used for identifying which record it belongs to)</li> <li>a value (the actual data)</li> <li>an offset (its position in the log)</li> </ul> <p>When log compaction is enabled, Kafka removes old versions of messages that have the same key\u2014keeping only the latest value for each key. This ensures that for every unique key, the log always contains the most recent state.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#the-split-clean-and-dirty-sections","title":"The split: Clean and Dirty sections","text":"<p>Kafka divides each log into two portions:</p> <ol> <li>Clean section:    This part has already been compacted.    It contains exactly one message per key \u2014 the latest value known at the last compaction.</li> </ol> <p>Think of it like the \u201ccleaned shelves\u201d in a library where only one copy of each book remains, nicely arranged.</p> <ol> <li>Dirty section:    This part contains new messages that were written after the last compaction.    There may be multiple versions of the same key here (e.g., old updates that haven\u2019t yet been cleaned).</li> </ol> <p>These are like the \u201cnewly arrived books\u201d that haven\u2019t yet been organized on the shelves.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#how-compaction-works-step-by-step","title":"How compaction works (step-by-step)","text":"<p>When <code>log.cleaner.enabled=true</code>, Kafka runs background threads (called log cleaner threads) to perform compaction.</p> <p>Here\u2019s what happens internally:</p> <ol> <li> <p>The cleaner thread selects a partition to compact    Each partition has a ratio of dirty messages to total partition size.    The partition with the highest ratio (most unclean data) is chosen first.    Essentially, Kafka asks: \u201cWhich shelf has the most messy, duplicate books? Let\u2019s clean that first.\u201d</p> </li> <li> <p>The cleaner reads the dirty section    It scans through all new messages written since the last compaction.</p> </li> <li> <p>It builds an in-memory map    This is where the interesting part begins.</p> </li> </ol>"},{"location":"streaming/kafka/47-Kafka_Compaction/#understanding-the-in-memory-map-with-analogy","title":"Understanding the in-memory map (with analogy)","text":"<p>The compaction thread creates an in-memory map while reading messages. Each entry in this map represents one unique key seen in the dirty section.</p> <p>Each entry contains:</p> <ul> <li>A 16-byte hash of the message key (like a short fingerprint of the key)</li> <li>An 8-byte offset (the position in the log of the previous message with the same key)</li> </ul> <p>So each entry = 24 bytes total (16 + 8).</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#analogy-library-shelf-cleaner","title":"Analogy: Library Shelf Cleaner","text":"<p>Imagine Kafka\u2019s log as a long shelf of books.</p> <ul> <li>Each book = a message</li> <li>The book title = message key</li> <li>The content inside = message value</li> <li>The shelf position number = message offset</li> </ul> <p>Now, when new editions of the same book arrive (same key but new value), they\u2019re added to the right end of the shelf (dirty section). So you might have:</p> <pre><code>Offset 1: Book \u201cA\u201d, Edition 1\nOffset 2: Book \u201cB\u201d, Edition 1\nOffset 3: Book \u201cA\u201d, Edition 2\nOffset 4: Book \u201cC\u201d, Edition 1\nOffset 5: Book \u201cA\u201d, Edition 3\n</code></pre> <p>Now, \u201cBook A\u201d appears three times (old editions + latest edition).</p> <p>When the cleaner thread comes by, it wants to keep only the latest edition of each book (each key).</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#how-the-map-helps-the-cleaner","title":"How the map helps the cleaner","text":"<p>As the cleaner scans through the dirty section:</p> <ul> <li> <p>It writes an entry into the in-memory map saying:   \u201cI saw Book A, and the previous copy was at offset 3.\u201d</p> </li> <li> <p>Later, when it sees another Book A, it updates that map entry:   \u201cNow the latest copy is at offset 5.\u201d</p> </li> </ul> <p>This way, by the end of the scan, the map knows where the latest version of each key is.</p> <p>So the map acts like a catalog of latest editions:</p> <ul> <li>Key \u2192 Latest known offset</li> </ul> <p>Once the map is ready, the cleaner uses it to rewrite the log:</p> <ul> <li>It copies only the latest messages (one per key) to a new compacted segment (the new clean section).</li> <li>The older duplicates are skipped (the old editions are thrown away).</li> </ul>"},{"location":"streaming/kafka/47-Kafka_Compaction/#memory-efficiency-explained","title":"Memory efficiency explained","text":"<p>Let\u2019s say a segment is 1 GB and each message is 1 KB.</p> <ul> <li>That means there are 1,000,000 messages in that segment.</li> <li>For each message, the cleaner might store 24 bytes in memory.</li> <li>So total memory = 1,000,000 \u00d7 24 bytes = 24 MB.</li> </ul> <p>That\u2019s very efficient \u2014 using just 24 MB of RAM to manage the compaction of a 1 GB log file.</p> <p>And if some keys repeat (for example, 100 messages share the same key), the cleaner reuses the same entry in the map \u2014 reducing memory even further.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#summary-table","title":"Summary Table","text":"Concept What it means Analogy Clean section Already compacted messages (latest version per key) Organized shelf with one copy per book Dirty section New un-compacted messages Newly arrived messy books Cleaner thread Background worker doing compaction Librarian cleaning the shelf In-memory map Tracks latest offsets for each key Catalog of latest book editions Hash (16 bytes) Fingerprint of key Unique short ID for each book Offset (8 bytes) Where the latest version sits Shelf number of the latest edition <p>Excellent \u2014 this is the second half of the log compaction process: how Kafka allocates memory for compaction, decides what fits in memory, and rewrites compacted data to disk safely.</p> <p>Let\u2019s unpack this carefully and use an analogy again (continuing with our library idea).</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#1-the-offset-map-memory-configuration","title":"1. The Offset Map Memory Configuration","text":"<p>Kafka uses an in-memory offset map for each cleaner thread during compaction. This map is where it tracks the latest offset for each key (as we explained earlier).</p> <p>However, memory is limited \u2014 so Kafka allows you to configure how much total memory all cleaner threads can use through:</p> <pre><code>log.cleaner.dedupe.buffer.size\n</code></pre> <p>This is a global limit shared among all cleaner threads.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#example","title":"Example","text":"<p>If:</p> <ul> <li>You configure 1 GB total for the cleaner offset map memory, and</li> <li>You have 5 cleaner threads running,</li> </ul> <p>then each thread will get 1 GB \u00f7 5 = 200 MB of memory for its own offset map.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#2-why-this-matters-fitting-segments-in-memory","title":"2. Why this matters: fitting segments in memory","text":"<p>Each thread needs enough memory to hold the offset map for at least one segment (the piece of the log file it\u2019s compacting).</p> <p>Each entry in that map = 24 bytes (16-byte hash + 8-byte offset). If a segment has 1 million messages (each 1 KB), that\u2019s roughly 24 MB per map.</p> <p>So, if your 200 MB per thread can fit 8 such maps (8 \u00d7 24 MB \u2248 192 MB), that\u2019s fine.</p> <p>But if your segment is huge, say 20 GB, and your map needs 480 MB, your 200 MB limit is too small \u2014 Kafka will log an error saying it can\u2019t compact that segment.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#analogy","title":"Analogy:","text":"<p>Think of this as how many library shelves you can clean at once.</p> <ul> <li>Each cleaner (thread) has a cart with limited space (its offset map memory).</li> <li>The administrator gives the entire cleaning team one big cart space budget \u2014 1 GB total.</li> <li>If there are 5 cleaners, each gets a 200 MB cart.</li> </ul> <p>Now:</p> <ul> <li>Each shelf (segment) must fit into one cleaner\u2019s cart so they can organize it.</li> <li>If a shelf has too many books to fit, the cleaner can\u2019t handle it \u2014 they leave it for later.</li> <li>The librarian (Kafka) will log an error and say:   \u201cNeed bigger carts (more memory) or fewer cleaners (threads).\u201d</li> </ul>"},{"location":"streaming/kafka/47-Kafka_Compaction/#3-handling-limited-memory-which-segments-get-compacted","title":"3. Handling limited memory: which segments get compacted","text":"<p>Kafka doesn\u2019t need the entire dirty section to fit at once. It only needs at least one segment to fit into memory.</p> <p>If there\u2019s room for more segments, Kafka will compact as many as fit \u2014 starting with the oldest ones first (like cleaning the oldest shelves first).</p> <p>Any remaining segments stay \u201cdirty\u201d until the next compaction round, when more memory or threads become available.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#analogy_1","title":"Analogy:","text":"<p>If your cleaners have carts that can only hold 2 shelves\u2019 worth of books at once:</p> <ul> <li>They\u2019ll start with the oldest shelves.</li> <li>Once done, they\u2019ll go back later for the newer ones.</li> </ul>"},{"location":"streaming/kafka/47-Kafka_Compaction/#4-after-the-offset-map-is-built-the-actual-compaction","title":"4. After the offset map is built \u2014 the actual compaction","text":"<p>Once the in-memory map is built, the cleaner moves to the compaction stage. Here\u2019s the sequence:</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#step-1-read-through-clean-segments-starting-from-oldest","title":"Step 1: Read through clean segments (starting from oldest)","text":"<p>The cleaner begins reading existing \u201cclean\u201d parts of the log (which contain previously compacted data).</p> <p>For each message in those segments:</p> <ul> <li>It checks if the key is still in the offset map.</li> </ul>"},{"location":"streaming/kafka/47-Kafka_Compaction/#step-2-check-each-message-key","title":"Step 2: Check each message key","text":"<p>There are two cases:</p> <ol> <li> <p>Key NOT in offset map:</p> </li> <li> <p>That means this message is still the latest value for that key.</p> </li> <li> <p>The message is copied into the replacement segment (the new, compacted file).</p> </li> <li> <p>Key IS in offset map:</p> </li> <li> <p>That means the cleaner has already seen a newer version of this key later in the log.</p> </li> <li>The old message is skipped (not copied).</li> <li>The key stays in the map so later duplicates can still be identified.</li> </ol>"},{"location":"streaming/kafka/47-Kafka_Compaction/#step-3-write-to-a-new-replacement-segment","title":"Step 3: Write to a new replacement segment","text":"<p>As it filters, the cleaner writes the retained messages to a new temporary segment file.</p> <p>Once the full segment is processed, Kafka:</p> <ul> <li>Atomically swaps the new compacted segment for the old one.</li> <li>Deletes the old un-compacted file.</li> <li>Moves on to the next segment.</li> </ul> <p>This ensures no data loss even if Kafka crashes midway \u2014 it never overwrites the original file until the new one is fully written.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#analogy-librarian-cleaning-the-shelves","title":"Analogy: Librarian cleaning the shelves","text":"<p>Imagine the librarian (the cleaner thread) doing this:</p> <ol> <li>They have their cart (offset map) filled with the latest edition of each book (latest offsets).</li> <li> <p>They go back to the oldest shelf and check every book:</p> </li> <li> <p>If that title is not in their cart, it means no new edition has replaced it \u2192 keep it.</p> </li> <li>If the title is in their cart, it means a newer edition exists \u2192 skip this one.</li> <li>They then place the kept books onto a new clean shelf (replacement segment).</li> <li>When done, they swap the old shelf for the new one and move to the next.</li> </ol> <p>By the end, there\u2019s only one copy per book, and always the latest edition.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#5-the-end-result","title":"5. The end result","text":"<p>After compaction:</p> <ul> <li>The clean section now contains one message per key \u2014 the most recent version.</li> <li>The dirty section will fill up again over time as new updates arrive.</li> <li>Cleaner threads will periodically repeat this process.</li> </ul> <p>Kafka\u2019s log thus becomes a \u201cstateful changelog\u201d, where the log always reflects the latest state per key.</p>"},{"location":"streaming/kafka/47-Kafka_Compaction/#summary-table_1","title":"Summary Table","text":"Concept Kafka Behavior Analogy log.cleaner.dedupe.buffer.size Total memory for all offset maps Total cart space for all cleaners Cleaner threads Each gets a portion of total memory Each librarian gets a cart At least one segment must fit Otherwise compaction fails Each cleaner must be able to hold at least one shelf Compacts oldest segments first When memory is tight Start cleaning from oldest shelves Offset map lookup Skip old keys, keep latest Keep only latest book editions Replacement segment Written safely, then swapped Clean shelf replaces messy one Final state One message per key One edition per book title"},{"location":"streaming/kafka/48-Kafka_Tombstoning_Records/","title":"Tombstoning Process in Kafka","text":"<p>If we always keep the latest message for each key, what do we do when we really want to delete all messages for a specific key, such as if a user left our service and we are legally obligated to remove all traces of that user from our system? To delete a key from the system completely, not even saving the last message, the application must produce a message that contains that key and a null value. </p> <p>When the cleaner thread finds such a message, it will first do a normal compaction and retain only the message with the null value. It will keep this special message (known as a tombstone) around for a configurable amount of time. During this time, consumers will be able to see this message and know that the value is deleted. </p> <p>So if a consumer copies data from Kafka to a relational database, it will see the tombstone message and know to delete the user from the database. After this set amount of time, the cleaner thread will remove the tombstone message, and the key will be gone from the partition in Kafka. </p> <p>It is important to give consumers enough time to see the tombstone message, because if our consumer was down for a few hours and missed the tombstone message, it will simply not see the key when consuming and therefore not know that it was deleted from Kafka or that it needs to be deleted from the database. </p> <p>It\u2019s worth remembering that Kafka\u2019s admin client also includes a deleteRecords method. This method deletes all records before a specified offset, and it uses a completely different mechanism. When this method is called, Kafka will move the low-water mark, its record of the first offset of a partition, to the specified offset.</p> <p>This will prevent consumers from consuming the records below the new lowwater mark and effectively makes these records inaccessible until they get deleted by a cleaner thread. This method can be used on topics with a retention policy and on compacted topics.</p>"},{"location":"streaming/kafka/48-Kafka_Tombstoning_Records/#when-are-topics-compacted","title":"When are topics compacted?","text":"<p>In the same way that the delete policy never deletes the current active segments, the compact policy never compacts the current segment. Messages are eligible for compaction only on inactive segments. By default, Kafka will start compacting when 50% of the topic contains dirty records. </p> <p>The goal is not to compact too often (since compaction can impact the read/write performance on a topic) but also not to leave too many dirty records around (since they consume disk space). Wasting 50% of the disk space used by a topic on dirty records and then compacting them in one go seems like a reasonable trade-off, and it can be tuned by the administrator.</p> <p>In addition, administrators can control the timing of compaction with two configuration parameters: min.compaction.lag.ms can be used to guarantee the minimum length of time that must pass after a message is written before it could be compacted. max.compaction.lag.ms can be used to guarantee the maximum delay between the time a message is written and the time the message becomes eligible for compaction. </p> <p>This configuration is often used in situations where there is a business reason to guarantee compaction within a certain period; for example, GDPR requires that certain information will be deleted within 30 days after a request to delete has been made.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/","title":"Reliability Guarantees in Kafka","text":""},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#1-reliability-and-guarantees-in-distributed-systems","title":"1. Reliability and Guarantees in Distributed Systems","text":"<p>When we discuss reliability, we are talking about predictable and consistent behavior under failure. A guarantee means that no matter what happens (crashes, network issues, restarts), a system behaves in a way that conforms to certain well-defined rules.</p> <p>In traditional databases, those guarantees are captured in the ACID model:</p> <ul> <li>Atomicity: Transactions are all-or-nothing.</li> <li>Consistency: Every transaction brings the database from one valid state to another valid state.</li> <li>Isolation: Concurrent transactions do not interfere with each other.</li> <li>Durability: Once a transaction is committed, it is permanent \u2014 it survives system failures.</li> </ul> <p>Because these guarantees are strict and well understood, developers can design systems confidently. They know that if the database claims to be ACID compliant, they can safely assume certain behaviors during concurrent operations and failures.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#2-kafkas-reliability-context","title":"2. Kafka\u2019s Reliability Context","text":"<p>Kafka is not a database; it\u2019s a distributed streaming platform. Its main responsibility is to ensure that messages are delivered and ordered correctly, even under distributed system failures.</p> <p>Kafka provides a different set of guarantees compared to ACID, focused on message ordering, durability, and delivery semantics rather than transactional isolation or consistency constraints.</p> <p>The reliability guarantees Kafka offers are the foundation upon which developers can design fault-tolerant streaming systems.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#3-kafkas-core-guarantees","title":"3. Kafka\u2019s Core Guarantees","text":""},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#a-ordering-guarantee","title":"a) Ordering Guarantee","text":"<p>Kafka guarantees message order within a partition.</p> <p>If message B is produced after message A by the same producer to the same partition, Kafka ensures:</p> <ul> <li>The offset of message B &gt; offset of message A.</li> <li>Consumers reading that partition will always read message A before message B.</li> </ul> <p>This is a per-partition guarantee \u2014 there is no ordering guarantee across partitions of a topic.</p> <p>This guarantee allows you to process data sequentially within a logical key (e.g., all updates for a single customer ID).</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#b-commit-guarantee","title":"b) Commit Guarantee","text":"<p>A message is considered committed when it is successfully written to the partition\u2019s leader and replicated to all in-sync replicas (ISRs).</p> <p>However, committed does not necessarily mean \u201cflushed to disk.\u201d Kafka may temporarily hold data in memory buffers and rely on replication for durability between brokers.</p> <p>Kafka allows the producer to configure when it receives an acknowledgment that a message is successfully written, through the <code>acks</code> parameter:</p> <code>acks</code> Value Meaning Reliability Latency <code>acks=0</code> Producer doesn\u2019t wait for any acknowledgment Lowest Lowest <code>acks=1</code> Wait for leader to write message Medium Medium <code>acks=all</code> or <code>acks=-1</code> Wait for all in-sync replicas to confirm Highest Highest <p>Thus, the developer controls the trade-off between latency and durability.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#c-durability-guarantee","title":"c) Durability Guarantee","text":"<p>Kafka guarantees that committed messages will not be lost as long as at least one in-sync replica remains alive.</p> <p>This is because each partition has multiple replicas stored across different brokers. If the leader broker fails, one of the in-sync replicas is automatically promoted to leader, ensuring the data remains available and consistent.</p> <p>However, durability also depends on configurations like:</p> <ul> <li><code>min.insync.replicas</code> (minimum number of replicas that must acknowledge a write)</li> <li><code>replication.factor</code> (total number of replicas)</li> <li><code>acks</code> (producer acknowledgment level)</li> </ul>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#d-consumer-visibility-guarantee","title":"d) Consumer Visibility Guarantee","text":"<p>Consumers can only read committed messages. That means they never see messages that were written to the leader but not yet replicated to all in-sync replicas.</p> <p>This ensures that consumers do not process messages that could be lost if a leader failure happens before replication completes.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#4-reliability-is-configurable-in-kafka","title":"4. Reliability is Configurable in Kafka","text":"<p>Kafka was designed to allow operators and developers to tune the reliability\u2013performance balance. Unlike traditional databases that enforce ACID behavior by design, Kafka exposes configuration parameters so you can decide:</p> <ul> <li>How much you value data consistency versus availability</li> <li>How much latency you are willing to trade for durability</li> <li>How much hardware replication you can afford</li> </ul> <p>For example:</p> <ul> <li>If you prioritize durability, you\u2019d use <code>acks=all</code>, <code>min.insync.replicas=2</code>, and a replication factor of 3.</li> <li>If you prioritize throughput and can tolerate some loss, you might use <code>acks=1</code> or even <code>acks=0</code>.</li> </ul> <p>This flexibility lets Kafka be used in both critical financial systems and high-speed telemetry pipelines, depending on what reliability trade-offs are acceptable.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#5-summary-of-kafkas-reliability-guarantees","title":"5. Summary of Kafka\u2019s Reliability Guarantees","text":"Guarantee Type Description Notes Ordering Messages in a partition are read in the same order they were written No global ordering across partitions Commit Message considered committed after all in-sync replicas acknowledge it Configurable via <code>acks</code> Durability Committed messages are not lost as long as one replica is alive Depends on replication factor and ISR configuration Consumer Read Consumers only read committed messages Prevents reading unreplicated (unsafe) data"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#6-the-trade-offs","title":"6. The Trade-Offs","text":"<p>Kafka\u2019s reliability model is not absolute \u2014 it\u2019s tunable.</p> <p>You trade between:</p> <ul> <li>Reliability (durability, consistency)</li> <li>Availability (system continues serving clients during failures)</li> <li>Performance (throughput and latency)</li> <li>Cost (more replicas mean more storage and network overhead)</li> </ul> <p>This flexibility is what makes Kafka powerful but also requires developers to understand the guarantees deeply. If you understand how Kafka behaves under different failure scenarios, you can configure it to meet your specific reliability goals.</p>"},{"location":"streaming/kafka/49-Kafka_Reliability_Guarantees/#final-summary","title":"Final Summary","text":"<p>Kafka provides strong but configurable guarantees:</p> <ol> <li>Ordered delivery within partitions</li> <li>Committed message durability across in-sync replicas</li> <li>Safe consumer reads (no uncommitted data exposure)</li> <li>Adjustable trade-offs between reliability, latency, and cost</li> </ol> <p>Unlike databases with rigid ACID guarantees, Kafka\u2019s model gives you the tools to decide how much reliability your application requires \u2014 and to build systems that behave predictably under failure once those configurations are set.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/","title":"Replication In Kafka","text":""},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#1-replication-as-the-foundation-of-reliability","title":"1. Replication as the Foundation of Reliability","text":"<p>Kafka\u2019s reliability \u2014 particularly its durability and fault tolerance \u2014 depends primarily on its replication mechanism.</p> <ul> <li>Each Kafka topic is divided into partitions, which are the smallest unit of data storage and parallelism.</li> <li>Each partition can have multiple replicas stored across different brokers (physical servers in the Kafka cluster).</li> </ul> <p>Replication ensures that even if one broker crashes, another broker can take over with no data loss. This mechanism is what gives Kafka its strong guarantees about message survival and recoverability.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#2-partition-structure-and-leadership","title":"2. Partition Structure and Leadership","text":""},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#a-partition-basics","title":"a) Partition Basics","text":"<p>A partition:</p> <ul> <li>Is an ordered, append-only log of messages.</li> <li>Is stored on a single disk of a broker.</li> <li>Preserves the order of messages within itself.</li> <li>Can be either online (available) or offline (unavailable).</li> </ul> <p>Kafka ensures that producers and consumers always interact with the leader replica of a partition.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#b-leader-and-follower-replicas","title":"b) Leader and Follower Replicas","text":"<p>Each partition has:</p> <ul> <li>One leader replica</li> <li>Zero or more follower replicas</li> </ul> <p>The leader handles:</p> <ul> <li>All produce requests (writing messages)</li> <li>All consume requests (reading messages)</li> </ul> <p>Followers:</p> <ul> <li>Do not serve client requests.</li> <li>Simply replicate data from the leader continuously to stay up to date.</li> </ul> <p>This replication model is known as leader-based replication.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#3-what-happens-during-normal-operation","title":"3. What Happens During Normal Operation","text":"<ol> <li>The producer sends data to the leader replica of the target partition.</li> <li>The leader appends the data to its local log.</li> <li>The followers fetch this data from the leader and append it to their logs.</li> <li>Once all in-sync replicas confirm they have the data, the message is considered committed.</li> <li>Consumers are allowed to read only committed messages.</li> </ol> <p>This process ensures that messages are safely stored in multiple locations before they are visible to consumers, protecting against single-broker failures.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#4-handling-failures-leader-election","title":"4. Handling Failures: Leader Election","text":"<p>If the leader of a partition fails (for example, due to a broker crash), Kafka automatically promotes one of the in-sync replicas (ISRs) to become the new leader.</p> <p>The in-sync replicas are those that are confirmed to have fully replicated the leader\u2019s data up to a recent point in time. This guarantees that the new leader will have all committed messages, preserving data integrity.</p> <p>Kafka\u2019s controller (a special broker role) handles this leader election process automatically.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#5-how-kafka-defines-in-sync-replica-isr","title":"5. How Kafka Defines \u201cIn-Sync Replica\u201d (ISR)","text":"<p>A replica is part of the ISR set (In-Sync Replica set) if it satisfies certain conditions. The ISR always includes the current leader and all followers that are up to date.</p> <p>A follower is considered in sync if:</p> <ol> <li> <p>Active ZooKeeper session</p> </li> <li> <p>The broker hosting the follower must have an active session with ZooKeeper (or the Kafka controller in newer versions).</p> </li> <li>This means it has sent a heartbeat within a certain time limit (default 6 seconds, configurable).</li> <li> <p>If the follower fails to send heartbeats, Kafka assumes it\u2019s down and removes it from the ISR.</p> </li> <li> <p>Recent Fetch Activity</p> </li> <li> <p>The follower must have fetched data from the leader within a specific time window (default 10 seconds).</p> </li> <li> <p>If it hasn\u2019t fetched new data recently, it\u2019s marked as out of sync.</p> </li> <li> <p>No Lag for a Recent Period</p> </li> <li> <p>The follower must have been fully caught up with the leader at least once in the last 10 seconds (configurable).</p> </li> <li>It\u2019s not enough to merely fetch data slowly; the follower must prove it can keep up with the leader in real time at least occasionally.</li> </ol> <p>If a follower falls behind (due to network delay, slow disk, or high load), Kafka removes it from the ISR. Only ISRs are eligible for leadership if the current leader fails.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#6-why-these-rules-matter","title":"6. Why These Rules Matter","text":"<p>These conditions guarantee that:</p> <ul> <li>Only up-to-date replicas can take over as leader.</li> <li>Kafka never promotes a follower that is missing committed data.</li> <li>Data consistency is maintained even during broker crashes.</li> </ul> <p>This mechanism prevents data loss and inconsistent reads, which are critical for reliability.</p> <p>However, it also means that if too many followers are slow or disconnected, Kafka might have fewer in-sync replicas, which can affect availability. This is one of Kafka\u2019s core reliability\u2013availability trade-offs.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#7-example-walkthrough","title":"7. Example Walkthrough","text":"<p>Imagine a topic <code>orders</code> with:</p> <ul> <li>Replication factor = 3</li> <li>Partition P1 stored on brokers B1 (leader), B2 (follower), and B3 (follower)</li> </ul> <p>During normal operation:</p> <ol> <li>The producer writes to B1 (leader).</li> <li>B2 and B3 fetch and replicate data from B1.</li> <li>All three are in the ISR.</li> <li>A message is considered committed when all three confirm replication.</li> </ol> <p>If B1 (the leader) fails:</p> <ul> <li>Kafka elects either B2 or B3 (whichever is in the ISR) as the new leader.</li> <li>Producers and consumers are redirected to the new leader automatically.</li> <li>Data remains consistent \u2014 no committed message is lost.</li> </ul>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#8-configurable-parameters-defaults-may-vary","title":"8. Configurable Parameters (Defaults May Vary)","text":"Parameter Description Default <code>zookeeper.session.timeout.ms</code> Time for a broker to send a heartbeat to ZooKeeper 6000 ms <code>replica.lag.time.max.ms</code> How long a follower can go without fetching data before being removed from ISR 10000 ms <code>replica.lag.max.messages</code> Max number of messages a follower can lag before removal (deprecated in newer versions) N/A <code>min.insync.replicas</code> Minimum number of in-sync replicas required for a message to be committed 1 or 2 (recommended 2 for safety) <p>These settings allow administrators to fine-tune how strict Kafka should be about replication and consistency.</p>"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#9-summary","title":"9. Summary","text":"Concept Description Replication Each partition is copied across multiple brokers for durability Leader Replica Handles all read/write requests for a partition Follower Replica Replicates data from the leader ISR (In-Sync Replica) Set of replicas that are fully caught up and eligible to become leader Leader Election When a leader fails, an ISR replica is promoted automatically Durability Messages are safe as long as at least one ISR remains alive"},{"location":"streaming/kafka/50-Kafka_Replication_Procedures/#in-essence","title":"In essence:","text":"<p>Kafka\u2019s replication mechanism is the backbone of its reliability model. It ensures that:</p> <ul> <li>Messages are not lost even if brokers fail.</li> <li>Only up-to-date replicas can become leaders.</li> <li>Order and durability are preserved.</li> </ul> <p>This design \u2014 leader-based replication with in-sync follower tracking \u2014 is what allows Kafka to maintain both high availability and data integrity in distributed, failure-prone environments.</p> <p>If a replica loses connection to ZooKeeper, stops fetching new messages, or falls behind and can\u2019t catch up within 10 seconds, the replica is considered out of sync. An out-ofsync replica gets back into sync when it connects to ZooKeeper again and catches up to the most recent message written to the leader. </p> <p>This usually happens quickly after a temporary network glitch is healed but can take a while if the broker the replica is stored on was down for a longer period of time.</p> <p>An in-sync replica that is slightly behind can slow down producers and consumers\u2014since they wait for all the insync replicas to get the message before it is committed. Once a replica falls out of sync, we no longer wait for it to get messages. </p> <p>It is still behind, but now there is no performance impact. The catch is that with fewer in-sync replicas, the effective replication factor of the partition is lower, and therefore there is a higher risk for downtime or data loss.</p>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/","title":"Broker Configuration in Kafka : Replication Factor","text":"<p>All the below mentioned configurations can be applied to topic level as well to control reliability trade offs not only at broker but topic level.</p>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#replication-factor","title":"Replication Factor","text":"<p>The topic-level configuration is replication.factor. At the broker level, we control the default.replication.factor for automatically created topics. Until this point in the book, we have assumed that topics had a replication factor of three, meaning that each partition is replicated three times on three different brokers.</p> <p>This was a reasonable assumption, as this is Kafka\u2019s default, but this is a configuration that users can modify. Even after a topic exists, we can choose to add or remove replicas and thereby modify the replication factor using Kafka\u2019s replica assignment tool. </p> <p>A replication factor of N allows us to lose N-1 brokers while still being able to read and write data to the topic. So a higher replication factor leads to higher availability, higher reliability, and fewer disasters. On the flip side, for a replication factor of N, we will need at least N brokers and we will store N copies of the data, meaning we will need N times as much disk space. We are basically trading availability for hardware.</p>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#detemining-right-number-of-replicas","title":"Detemining Right Number of Replicas","text":""},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#1-availability","title":"1. Availability","text":"<p>Definition: Availability refers to the ability of Kafka to continue serving client requests (producing and consuming messages) even when some brokers fail.</p> <p>Explanation:</p> <ul> <li>If a partition has only one replica, it exists on a single broker.</li> <li>During a routine broker restart (for maintenance, software update, or crash recovery), that partition becomes unavailable, since there is no other copy.</li> <li>Producers cannot write to it, and consumers cannot read from it until the broker comes back online.</li> </ul> <p>Key Point:</p> <ul> <li>Adding more replicas improves availability.</li> <li>If one broker fails, Kafka can elect another in-sync replica (ISR) as the new leader, and the partition remains available.</li> </ul> <p>Rule of Thumb:</p> <ul> <li>A replication factor of 3 is generally recommended for production workloads.   This allows Kafka to tolerate the failure of one broker and still maintain a second replica for redundancy.</li> </ul>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#2-durability","title":"2. Durability","text":"<p>Definition: Durability is the guarantee that once data is acknowledged as committed, it will not be lost \u2014 even if brokers fail or disks crash.</p> <p>Explanation:</p> <ul> <li>Each replica stores a complete copy of the partition data.</li> <li>If there is only one replica and that broker\u2019s disk becomes corrupted or unavailable, all data in that partition is lost permanently.</li> <li>With multiple replicas on different brokers (and ideally different physical disks or racks), the likelihood that all replicas fail simultaneously is drastically reduced.</li> </ul> <p>Key Point:</p> <ul> <li>More replicas \u2192 higher durability.</li> <li>Replicas spread across fault domains (different servers, racks, or availability zones) provide much stronger fault tolerance.</li> </ul>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#3-throughput","title":"3. Throughput","text":"<p>Definition: Throughput is the rate at which Kafka can handle incoming and outgoing data (measured in MBps or messages per second).</p> <p>Explanation:</p> <ul> <li>Each additional replica adds replication traffic between brokers.</li> <li>The leader must send the same data to every follower replica to keep them synchronized.</li> </ul> <p>Example:</p> <ul> <li> <p>If producers write at 10 MBps:</p> </li> <li> <p>With 1 replica, there is no replication traffic (no extra copies).</p> </li> <li>With 2 replicas, there is 10 MBps of replication traffic (leader \u2192 follower).</li> <li>With 3 replicas, there is 20 MBps of replication traffic.</li> <li>With 5 replicas, there is 40 MBps of replication traffic.</li> </ul> <p>Impact:</p> <ul> <li>Higher replication factors increase inter-broker network usage and disk I/O.</li> <li>This means you must size your network bandwidth and storage throughput accordingly when planning the cluster.</li> </ul> <p>Key Point:</p> <ul> <li>There\u2019s a direct trade-off between durability (more replicas) and throughput (less replication traffic overhead).</li> </ul>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#4-end-to-end-latency","title":"4. End-to-End Latency","text":"<p>Definition: Latency is the time it takes for a produced message to become visible and readable by consumers.</p> <p>Explanation:</p> <ul> <li>Kafka considers a message committed only after it has been replicated to all in-sync replicas (ISRs).</li> <li>More replicas mean more acknowledgments required before the message is marked committed.</li> <li>If any one replica is slow (due to disk or network lag), it delays acknowledgment for that message.</li> <li>This, in turn, delays when the consumer can read it.</li> </ul> <p>In Practice:</p> <ul> <li>Usually, slow replicas are rare and localized issues.</li> <li>Even a single slow broker can affect latency, regardless of replication factor, since any client communicating with it will experience delays.</li> </ul> <p>Key Point:</p> <ul> <li>More replicas may slightly increase latency, but in a well-tuned cluster, this impact is usually small compared to the benefit in durability.</li> </ul>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#5-cost","title":"5. Cost","text":"<p>Definition: Cost refers to the additional storage, network, and infrastructure resources required for replication.</p> <p>Explanation:</p> <ul> <li>Each replica is a full copy of the partition data.</li> <li>A replication factor of 3 means triple the storage compared to a single replica.</li> <li>It also means more network bandwidth for replication traffic and more disk writes.</li> </ul> <p>Practical Considerations:</p> <ul> <li>Some administrators reduce replication factor to 2 for non-critical topics to save costs.</li> <li>However, this lowers availability because the system can only tolerate one broker failure without data unavailability.</li> <li> <p>In some environments, the underlying storage system (for example, cloud block storage or distributed file systems) already replicates data three times at the hardware level.</p> </li> <li> <p>In such cases, setting Kafka\u2019s replication factor to 2 can be a reasonable compromise \u2014 the durability is still guaranteed by the storage layer, even if availability is slightly lower.</p> </li> </ul> <p>Key Point:</p> <ul> <li>Cost scales directly with replication factor.</li> <li>A higher replication factor gives stronger reliability at the expense of more infrastructure cost.</li> </ul>"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#6-the-trade-off-summary","title":"6. The Trade-off Summary","text":"Factor Effect of Increasing Replication Factor Reason Availability Increases More replicas \u2192 more backup leaders available Durability Increases Data stored on multiple disks/nodes Throughput Decreases More inter-broker replication traffic Latency Slightly increases Must wait for all ISRs to acknowledge Cost Increases More storage and bandwidth required"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#7-typical-recommendations","title":"7. Typical Recommendations","text":"Use Case Recommended Replication Factor Rationale Production, critical data 3 Balances durability, availability, and performance Non-critical, transient data 2 Reduces cost while maintaining basic redundancy Testing / development 1 Simplifies setup, no fault tolerance required"},{"location":"streaming/kafka/51-Kafka_Broker_Config_Replication_Factor/#final-summary","title":"Final Summary","text":"<p>Kafka replication is a trade-off mechanism. It directly determines how reliable, available, and performant your system will be.</p> <ul> <li>More replicas improve fault tolerance (availability, durability).</li> <li>Fewer replicas improve efficiency (throughput, cost).</li> </ul> <p>Choosing the right replication factor depends on how critical the data is and how much resource cost you can afford. In most real-world production clusters, a replication factor of 3 is considered the optimal balance between safety, performance, and cost.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/","title":"Kafka Broker Configuration : Unclean Leader Elections","text":""},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#1-the-context-leader-election-in-kafka","title":"1. The Context: Leader Election in Kafka","text":"<p>Every Kafka partition has:</p> <ul> <li>One leader replica \u2014 handles all read and write requests.</li> <li>One or more follower replicas \u2014 replicate data from the leader.</li> </ul> <p>Kafka ensures that:</p> <ul> <li>Only in-sync replicas (ISRs) are eligible to become the leader during a failure.</li> <li>Committed data is defined as data successfully replicated to all ISRs.</li> </ul> <p>This mechanism ensures that when a leader fails, another replica can take over without losing committed data.</p> <p>That type of leader election is called a clean leader election.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#2-the-parameter-uncleanleaderelectionenable","title":"2. The Parameter: <code>unclean.leader.election.enable</code>","text":""},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#definition","title":"Definition:","text":"<p><code>unclean.leader.election.enable</code> is a broker-level (and effectively cluster-wide) configuration setting.</p> <p>It controls whether Kafka allows an out-of-sync replica to become the new leader if no in-sync replicas are available.</p> <pre><code>unclean.leader.election.enable=false\n</code></pre> <ul> <li>Default: <code>false</code></li> <li>Level: Broker (applies to all topics in the cluster)</li> </ul> <p>This setting defines Kafka\u2019s behavior when all in-sync replicas (ISRs) of a partition become unavailable.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#3-clean-leader-election-the-default","title":"3. Clean Leader Election (the Default)","text":"<p>When <code>unclean.leader.election.enable=false</code>:</p> <ul> <li>Kafka will not elect an out-of-sync replica as the new leader.</li> <li>The partition will remain unavailable until at least one in-sync replica comes back online.</li> </ul> <p>Guarantee:</p> <ul> <li>Kafka guarantees no data loss for committed messages.</li> <li>However, availability is sacrificed \u2014 producers and consumers cannot access that partition until an ISR returns.</li> </ul> <p>This is a strict consistency choice.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#4-unclean-leader-election","title":"4. Unclean Leader Election","text":"<p>When <code>unclean.leader.election.enable=true</code>:</p> <ul> <li>Kafka can elect an out-of-sync replica as the new leader if no in-sync replicas are available.</li> <li>This restores availability quickly \u2014 the partition becomes writable and readable again.</li> <li>However, this introduces a risk of data loss, because that out-of-sync replica does not contain the latest committed messages that were on the old leader.</li> </ul> <p>This is a high-availability, lower-consistency choice.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#5-scenarios-leading-to-this-situation","title":"5. Scenarios Leading to This Situation","text":"<p>The passage describes two real-world failure cases where this configuration becomes relevant.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#scenario-1-total-broker-failures","title":"Scenario 1: Total Broker Failures","text":"<ol> <li> <p>A partition has 3 replicas:</p> </li> <li> <p>1 leader, 2 followers.</p> </li> <li> <p>Two brokers (followers) crash \u2192 only the leader is left.</p> </li> <li> <p>The leader continues to accept writes.</p> </li> <li>Since there are no followers alive, it becomes the only in-sync replica.</li> <li> <p>Then, the leader itself crashes.</p> </li> <li> <p>All replicas are now offline.</p> </li> <li> <p>When one of the old followers restarts:</p> </li> <li> <p>It is out of sync (it missed all writes after it went down).</p> </li> <li>No ISR exists.</li> <li> <p>Kafka now has a choice:</p> </li> <li> <p>With unclean leader election disabled: Kafka waits until the old leader returns (no data loss, but partition is offline).</p> </li> <li>With unclean leader election enabled: Kafka promotes the out-of-sync follower as leader (partition is available again, but data that was on the old leader is lost).</li> </ol>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#scenario-2-network-lag-followers-falling-behind","title":"Scenario 2: Network Lag (Followers Falling Behind)","text":"<ol> <li>The same partition with 3 replicas (1 leader, 2 followers).</li> <li> <p>Due to network slowness or lag, the followers fall behind.</p> </li> <li> <p>They still fetch data, but too slowly to remain in the ISR.</p> </li> <li>The leader is now the only ISR.</li> <li>The leader continues accepting new writes.</li> <li> <p>The leader fails.</p> </li> <li> <p>The remaining replicas are alive but out of sync.</p> </li> <li> <p>Kafka faces the same choice:</p> </li> <li> <p>Promote an out-of-sync replica (risk data loss).</p> </li> <li>Wait for the leader to return (maintain consistency but reduce availability).</li> </ol>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#6-the-trade-off","title":"6. The Trade-Off","text":"<p>This setting is one of the clearest examples of the CAP theorem (Consistency, Availability, Partition tolerance) at work in Kafka.</p> Setting Behavior Data Loss Risk Availability <code>unclean.leader.election.enable=false</code> Only in-sync replicas can be leaders None Lower <code>unclean.leader.election.enable=true</code> Out-of-sync replicas can become leaders Possible Higher <p>Kafka administrators must decide which property to prioritize:</p> <ul> <li>Set to false (default): Guarantees no committed data loss (preferred for financial, transactional, or critical data).</li> <li>Set to true: Keeps data available during severe failures (useful for log or metrics data where some data loss is acceptable).</li> </ul>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#7-why-its-cluster-wide","title":"7. Why It\u2019s \u201cCluster-Wide\u201d","text":"<p>This configuration is broker-level and effectively cluster-wide, because:</p> <ul> <li>It\u2019s applied consistently to all partitions across the brokers.</li> <li>It influences the cluster controller\u2019s decision logic during leader elections.</li> <li>Allowing different brokers to use different settings would lead to unpredictable data consistency behavior.</li> </ul>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#8-real-world-best-practices","title":"8. Real-World Best Practices","text":"Environment Recommended Setting Reason Production (financial / critical systems) <code>unclean.leader.election.enable=false</code> Prevents committed data loss Development / testing <code>unclean.leader.election.enable=true</code> Keeps topics available during broker restarts Non-critical telemetry / log data <code>unclean.leader.election.enable=true</code> Small data loss acceptable; high availability preferred <p>In production environments where data integrity is more important than short-term availability, this should remain disabled (the default).</p> <p>However, for use cases where losing a few seconds of data is acceptable, enabling it can help maintain service continuity.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#9-summary","title":"9. Summary","text":"Concept Description Clean leader election Promotes only in-sync replicas to leader; ensures no committed data loss Unclean leader election Allows out-of-sync replicas to become leader; restores availability faster but can lose data Configuration parameter <code>unclean.leader.election.enable</code> (broker-level) Default value <code>false</code> Trade-off Data integrity (false) vs. Availability (true) When to enable Only for non-critical data or test environments <p>In essence: Kafka\u2019s <code>unclean.leader.election.enable</code> setting is a direct lever between data safety and system uptime.</p> <p>Keeping it disabled ensures zero data loss but may cause temporary unavailability during multiple failures. Enabling it improves availability but at the risk of losing recently committed messages.</p>"},{"location":"streaming/kafka/52-Kafka_Broker_Configuration_Unclean_Leader_Election/#the-correct-choice-depends-entirely-on-the-criticality-of-your-data-and-the-tolerance-for-temporary-downtime-in-your-system","title":"The correct choice depends entirely on the criticality of your data and the tolerance for temporary downtime in your system.","text":""},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/","title":"What happens when Out of Sync Replica becomes Leader?","text":"<p>When an out-of-sync replica is promoted to follower of a new leader, it will truncate (delete) any log records that the new leader doesn\u2019t have. Those truncated records are permanently lost if no other replica holds them. The truncation is how Kafka enforces single truth (the leader\u2019s log) and prevents the follower and leader remaining permanently inconsistent.</p>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#what-actually-happens-step-by-step","title":"What actually happens (step-by-step)","text":"<ol> <li> <p>Divergence happened earlier.</p> </li> <li> <p>While Replica A was leader, followers B and C fell behind (crashed or lagged). A accepted writes with offsets 100\u2013200. B and C only have offsets 0\u201399.</p> </li> <li> <p>Leader A fails.</p> </li> <li> <p>If <code>unclean.leader.election.enable=true</code>, Kafka may elect Replica B (which only has 0\u201399) as the new leader even though it lacks 100\u2013200.</p> </li> <li> <p>Replica A later comes back online.</p> </li> <li> <p>Replica A still contains 100\u2013200 (the records written while B/C were out-of-sync). But now B is the leader and B\u2019s log is the canonical log for that partition.</p> </li> <li> <p>Replica A becomes a follower and must catch up.</p> </li> <li> <p>The follower fetcher reads the leader\u2019s state and realizes Replica A contains offsets that the leader does not have (100\u2013200).</p> </li> <li> <p>Kafka decides the follower\u2019s local log beyond the leader\u2019s log end offset is invalid relative to the leader.</p> </li> <li> <p>Log truncation on the follower.</p> </li> <li> <p>Replica A deletes (or truncates) those suffix records (100\u2013200) from its local log so its log matches the leader\u2019s log. In Kafka code this is a controlled truncation, not a merge \u2014 the follower\u2019s log is brought to the leader\u2019s last offset.</p> </li> <li> <p>Result: those records are gone cluster-wide.</p> </li> <li> <p>Because B (the new leader) never had 100\u2013200 and other replicas lack them as well (or were out-of-sync), there is no remaining copy anywhere. Consumers can no longer access those messages.</p> </li> </ol>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#why-kafka-does-this-the-reason-behind-deletion","title":"Why Kafka does this (the reason behind deletion)","text":"<p>Kafka enforces that the leader\u2019s log is the authoritative copy. To keep all replicas consistent, followers must match the leader exactly. If followers kept their extra records, different replicas would disagree about the content and order of the partition \u2014 which would break correctness guarantees and make future leader elections chaotic.</p> <p>So when a replica sees that the leader doesn\u2019t have some suffix of its log, the follower truncates that suffix. That preserves the invariant: every replica\u2019s log is a prefix of the current leader\u2019s log.</p>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#what-delete-means-technically","title":"What \u201cdelete\u201d means technically","text":"<ul> <li>Kafka doesn\u2019t just hide the messages; it actually truncates log segments or removes suffixes so those records are physically removed from the follower\u2019s local storage (or at least logically discarded from the replica\u2019s log).</li> <li>The follower\u2019s log end offset is reduced to match the leader\u2019s log end offset. The follower will request new data from the leader starting at that matched offset.</li> </ul>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#consequences-for-consumers-and-applications","title":"Consequences for consumers and applications","text":"<ol> <li> <p>Some consumers may have already read the now-deleted messages.</p> </li> <li> <p>If consumers read offsets 100\u2013150 when Replica A was leader, but later those offsets are deleted when logs are truncated, those messages are no longer available to read again. Consumers that recorded or acted on that data have seen information that no one else can reproduce from the topic.</p> </li> <li> <p>Inconsistent views across consumers.</p> </li> <li> <p>Different consumers may have read different sets (some saw the old 100\u2013200, others saw the new   leader\u2019s sequence). Downstream systems that aggregate or reconcile data can end up with inconsistent results.</p> </li> <li> <p>Offset problems (OffsetOutOfRange).</p> </li> <li> <p>A consumer whose committed offset points to a now-deleted range will hit <code>OffsetOutOfRange</code> on fetch and must decide how to proceed (reset to earliest/latest or use stored checkpoints).</p> </li> <li> <p>No recovery from Kafka alone.</p> </li> <li> <p>Once truncated and no other replica retains the messages, Kafka cannot restore those records. They are lost unless some external copy existed (e.g., an external log, sink, or backup).</p> </li> <li> <p>Transactional / exactly-once implications.</p> </li> <li> <p>If producers used transactions or relied on idempotence, the loss of committed-looking data can violate application-level invariants (for example, duplicated side effects or missing transactions).</p> </li> </ol>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#analogies","title":"Analogies","text":""},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#1-library-copies-analogy","title":"1. Library copies analogy","text":"<p>Imagine a book where pages are numbered:</p> <ul> <li>Librarian A (leader) distributes copies and adds pages 100\u2013200 to her copy while assistants B and C are away.</li> <li>Librarian A\u2019s copy is the latest and patrons read pages 100\u2013200 from it.</li> <li>A goes home. Assistant B returns and is made the new head librarian, but B only has pages up to 99.</li> <li>When A returns, A\u2019s extra pages (100\u2013200) are removed from A\u2019s copy because the library now uses B\u2019s copy as the canonical edition. The extra pages disappear from the library \u2014 nobody has them anymore. Patrons who read those pages earlier have read material that no longer exists in the library collection.</li> </ul>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#2-ledger-bank-account-analogy","title":"2. Ledger / bank account analogy","text":"<p>A partition is like a ledger:</p> <ul> <li>Leader A wrote transactions T100\u2013T200 while B and C were disconnected.</li> <li>When A goes offline and B becomes the ledger owner, the ledger that B holds does not contain T100\u2013T200.</li> <li>When A rejoins, A\u2019s extra transactions are removed to reconcile with B\u2019s ledger. Those transactions are permanently erased from the official ledger; if any customer saw those transactions earlier, their view is now inconsistent with the official ledger.</li> </ul>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#3-git-force-push-analogy","title":"3. Git force-push analogy","text":"<p>Think of leader A\u2019s log like a branch that had commits C100\u2013C200. If B becomes the canonical branch at an earlier commit and the project decides to reset the branch to B\u2019s commit, then commits C100\u2013C200 are lost from the canonical history (unless someone kept a separate copy). That\u2019s like a forced reset (force-push) that discards commits.</p>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#how-to-avoid-or-mitigate-this-risk","title":"How to avoid or mitigate this risk","text":"<ul> <li>Keep unclean leader election disabled (<code>false</code>) (default) so Kafka never promotes out-of-sync replicas \u2014 prevents this class of data loss at the expense of availability.</li> <li> <p>Tune replication and ISR settings:</p> </li> <li> <p><code>replication.factor &gt;= 3</code> and <code>min.insync.replicas</code> to ensure enough replicas remain in ISR before commits are acknowledged.</p> </li> <li>Use <code>acks=all</code> on producers to ensure writes are replicated to ISRs before being acknowledged.</li> <li>Avoid long follower lag: ensure network, disk I/O, and broker health are monitored; use rack awareness and capacity planning to reduce chance of slow followers.</li> <li>Controlled broker shutdowns (graceful shutdown) so leadership transfers cleanly and followers remain in sync.</li> <li>Back up critical data externally (mirror to durable storage) if any data loss is unacceptable.</li> <li>Operational plan: if you ever enable unclean leader election in an emergency to restore availability, be aware you may lose messages and should reconcile downstream systems accordingly; switch it back to false after recovery.</li> </ul>"},{"location":"streaming/kafka/53-Kafka_Log_Truncation_On_Out_Of_Sync_Leader/#final-takeaway","title":"Final takeaway","text":"<p>When a follower deletes records that the current leader does not have, Kafka is enforcing that the leader is the single source of truth. That deletion is permanent inside the Kafka cluster \u2014 if no replica retained those messages, they are irrecoverably lost from the topic. This behavior is the exact trade-off controlled by <code>unclean.leader.election.enable</code>: higher availability with possible data loss versus strong durability and consistency with possible temporary unavailability.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/","title":"Kafka - Keeping Replicas in Sync","text":""},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#1-background-replication-and-in-sync-replicas-isr","title":"1. Background: Replication and In-Sync Replicas (ISR)","text":"<p>Kafka maintains multiple copies of data (replicas) for each partition to achieve durability and availability.</p> <ul> <li>When you create a topic, you specify a replication factor (for example, 3).   That means each partition has 3 replicas distributed across different brokers.</li> <li>Among these replicas, one is the leader, and the others are followers.</li> <li>Followers constantly replicate the leader\u2019s data.</li> </ul> <p>The set of replicas that are fully caught up with the leader is called the ISR (In-Sync Replica) set.</p> <p>If all replicas are healthy, ISR = {leader, follower1, follower2}. But if some followers fall behind or fail, they are temporarily removed from ISR.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#2-what-mininsyncreplicas-means","title":"2. What <code>min.insync.replicas</code> means","text":"<p>The <code>min.insync.replicas</code> configuration defines the minimum number of replicas that must acknowledge a write before Kafka considers that write committed.</p> <p>You can define it:</p> <ul> <li>At the broker level (default behavior for topics on that broker)</li> <li>At the topic level (overrides broker-level setting)</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#3-how-writes-work","title":"3. How writes work","text":"<p>When a producer sends data to a partition:</p> <ol> <li>The producer writes to the leader broker.</li> <li>The leader writes the record locally and waits for acknowledgments from its followers (replicas).</li> <li> <p>The leader then decides whether to commit the message, depending on:</p> </li> <li> <p>The producer\u2019s <code>acks</code> configuration</p> </li> <li>The <code>min.insync.replicas</code> setting</li> </ol>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#4-interaction-with-producer-acks","title":"4. Interaction with producer <code>acks</code>","text":"<p>The producer controls how many acknowledgments it expects before it considers a write successful.</p> <code>acks</code> value Meaning <code>acks=0</code> Producer doesn\u2019t wait for acknowledgment. Fast but unsafe. <code>acks=1</code> Producer waits for leader acknowledgment only. Followers may still lag. <code>acks=all</code> (or <code>-1</code>) Producer waits for acknowledgment from all in-sync replicas (ISR). <p>Now, when <code>acks=all</code>, Kafka uses <code>min.insync.replicas</code> to decide if enough replicas are available to safely accept the write.</p> <p>If the ISR count &lt; <code>min.insync.replicas</code>, Kafka rejects the produce request with:</p> <pre><code>NotEnoughReplicasException\n</code></pre>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#5-example-scenario","title":"5. Example scenario","text":""},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#configuration","title":"Configuration:","text":"<ul> <li>Replication factor = 3</li> <li><code>min.insync.replicas = 2</code></li> <li>Producer uses <code>acks=all</code></li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#case-1-all-replicas-healthy","title":"Case 1: All replicas healthy","text":"<p>ISR = {leader, follower1, follower2} \u2192 Producer sends a record \u2192 All 3 replicas acknowledge \u2192 Write succeeds.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#case-2-one-replica-down","title":"Case 2: One replica down","text":"<p>ISR = {leader, follower1} (2 replicas) \u2192 Still &gt;= 2 \u2192 Producer can continue writing safely.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#case-3-two-replicas-down","title":"Case 3: Two replicas down","text":"<p>ISR = {leader} (1 replica only) \u2192 1 &lt; 2 \u2192 Kafka rejects the write. Producer gets <code>NotEnoughReplicasException</code>. Consumers can still read existing committed data, but no new writes are accepted. The partition becomes effectively read-only until at least one more replica rejoins ISR.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#6-why-this-matters-consistency-vs-availability","title":"6. Why this matters (Consistency vs Availability)","text":"<p>This configuration directly affects how Kafka behaves during broker failures:</p> <ul> <li> <p>If you set <code>min.insync.replicas=1</code>, Kafka prioritizes availability:</p> </li> <li> <p>You can always produce data as long as one replica (the leader) is up.</p> </li> <li> <p>But if that replica crashes before followers catch up, data can be lost.</p> </li> <li> <p>If you set <code>min.insync.replicas=2</code>, Kafka prioritizes consistency:</p> </li> <li> <p>Kafka only commits data when at least two replicas have it.</p> </li> <li>If one replica remains, the system refuses new writes to prevent data loss.</li> <li>The tradeoff is reduced availability during broker outages.</li> </ul> <p>This is the CAP theorem tradeoff \u2014 balancing consistency and availability under failure conditions.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#7-why-data-can-disappear-when-misconfigured","title":"7. Why data can disappear when misconfigured","text":"<p>If <code>min.insync.replicas</code> is too low (for example, <code>1</code>), then:</p> <ul> <li>Kafka marks data as committed even when only one replica has it.</li> <li>If that single replica fails before the others replicate the data, the message is lost permanently.</li> <li>Later, when a new leader is elected (one that didn\u2019t have the data), the old data simply disappears \u2014 because from the cluster\u2019s point of view, it was never truly committed to a quorum.</li> </ul> <p>Setting <code>min.insync.replicas=2</code> (with replication factor 3) prevents this.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#8-recovering-from-a-read-only-situation","title":"8. Recovering from a read-only situation","text":"<p>When Kafka refuses new writes because too few replicas are in-sync:</p> <ul> <li>You must bring failed brokers back online.</li> <li>Once the followers catch up with the leader\u2019s log, they rejoin the ISR.</li> <li>When ISR size \u2265 <code>min.insync.replicas</code>, producers can write again.</li> </ul> <p>Kafka does this automatically \u2014 you just need to restore the unavailable brokers and let replication complete.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#9-best-practice","title":"9. Best practice","text":"<p>For production-grade reliability:</p> <ul> <li>Replication factor: 3</li> <li><code>min.insync.replicas</code>: 2</li> <li>Producer <code>acks</code>: <code>all</code></li> </ul> <p>This ensures:</p> <ul> <li>Data is only acknowledged when written to at least 2 brokers.</li> <li>The system can tolerate one broker failure without losing data.</li> <li>Writes are temporarily paused (not lost) if two brokers fail.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#10-summary","title":"10. Summary","text":"Concept Description ISR (In-Sync Replicas) Replicas fully caught up with the leader min.insync.replicas Minimum number of replicas that must acknowledge a write for it to succeed <code>acks=all</code> Producer waits for all in-sync replicas High <code>min.insync.replicas</code> Improves consistency, reduces availability Low <code>min.insync.replicas</code> Improves availability, increases risk of data loss Typical setting Replication factor = 3, min.insync.replicas = 2 <p>In short: <code>min.insync.replicas</code> is Kafka\u2019s mechanism to enforce a write quorum \u2014 ensuring that data acknowledged to the producer truly exists on more than one broker. It\u2019s the key parameter that balances durability and availability in distributed Kafka clusters.</p> <p>Let\u2019s break this passage down carefully and explain what it really means \u2014 especially in the context of Kafka replication reliability and cluster tuning.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#1-why-out-of-sync-replicas-are-a-problem","title":"1. Why out-of-sync replicas are a problem","text":"<p>Kafka\u2019s replication model depends on the concept of in-sync replicas (ISR) \u2014 these are the replicas that are fully caught up with the leader.</p> <p>When a replica falls behind or stops communicating, it becomes out of sync (OOSR). Having out-of-sync replicas means:</p> <ul> <li>Fewer copies of the latest data are available.</li> <li>If the leader fails, the new leader may not have the most recent messages.</li> <li>Data loss risk increases.</li> <li><code>min.insync.replicas</code> violations can occur, leading to write failures.</li> </ul> <p>So Kafka tries to detect and handle out-of-sync replicas quickly \u2014 but not too aggressively, to avoid unnecessary instability.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#2-two-reasons-why-replicas-go-out-of-sync","title":"2. Two reasons why replicas go out of sync","text":"<p>Kafka recognizes two primary conditions under which a replica can fall out of sync:</p> <ol> <li> <p>Loss of connectivity to ZooKeeper</p> </li> <li> <p>In pre-KIP-500 versions of Kafka (those still using ZooKeeper), each broker maintains a session with ZooKeeper via periodic heartbeats.</p> </li> <li> <p>If a broker doesn\u2019t send a heartbeat for a certain amount of time, ZooKeeper assumes it is dead or unreachable and removes it from the cluster.</p> </li> <li> <p>Replication lag</p> </li> <li> <p>A follower replica constantly fetches data from the leader.</p> </li> <li>If it stops fetching (e.g., due to network issues, GC pauses, overloaded broker) or cannot keep up, its replication lag increases.</li> <li>If that lag exceeds a configured limit, the broker marks it as out of sync.</li> </ol>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#3-first-setting-zookeepersessiontimeoutms","title":"3. First setting: <code>zookeeper.session.timeout.ms</code>","text":"<p>This configuration determines how long ZooKeeper waits before declaring a broker \u201cdead\u201d due to missed heartbeats.</p> <ul> <li>Default (after Kafka 2.5.0): 18,000 ms (18 seconds)</li> <li>Previously: 6,000 ms (6 seconds)</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#how-it-works","title":"How it works:","text":"<p>Each Kafka broker sends periodic heartbeats to ZooKeeper. If ZooKeeper doesn\u2019t receive a heartbeat within this timeout:</p> <ul> <li>It removes the broker\u2019s ephemeral nodes.</li> <li>The controller broker notices and triggers a leader re-election for all partitions that broker was leading.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#why-this-setting-matters","title":"Why this setting matters:","text":"<ul> <li>If too short:   Temporary slowdowns (e.g., GC pauses or network hiccups) cause unnecessary broker removals \u2192 frequent leader re-elections \u2192 cluster instability.</li> <li>If too long:   It takes longer to detect a genuinely dead broker \u2192 slower failover \u2192 decreased availability.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#practical-insight","title":"Practical insight:","text":"<p>In cloud or virtualized environments, network latency is often less predictable, so Kafka 2.5 increased the default from 6s \u2192 18s to make clusters more tolerant of brief network fluctuations.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#4-second-setting-replicalagtimemaxms","title":"4. Second setting: <code>replica.lag.time.max.ms</code>","text":"<p>This controls how long a follower can be behind the leader before Kafka considers it out of sync.</p> <ul> <li>Default (after Kafka 2.5.0): 30,000 ms (30 seconds)</li> <li>Previously: 10,000 ms (10 seconds)</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#how-it-works_1","title":"How it works:","text":"<p>If a follower doesn\u2019t fetch data from its leader within this time window, or if its last fetch is too far behind, the leader removes it from the ISR.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#why-this-setting-matters_1","title":"Why this setting matters:","text":"<ul> <li>Shorter value:   Kafka quickly removes lagging replicas from the ISR \u2192 better consistency but higher chance of instability (\u201cISR flapping\u201d).   Example: if a follower momentarily pauses (e.g., during a GC), it\u2019ll be removed immediately.</li> <li>Longer value:   Kafka gives replicas more time to catch up \u2192 more stable ISR membership, fewer unnecessary leader changes.   The tradeoff: longer replication delay means data takes more time to reach all replicas.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#5-impact-on-consumer-latency","title":"5. Impact on consumer latency","text":"<p>There\u2019s an important implication here: When you use <code>acks=all</code>, a message is considered committed only when all in-sync replicas have acknowledged it.</p> <p>Therefore, if Kafka keeps replicas in the ISR longer (e.g., by setting <code>replica.lag.time.max.ms = 30000</code>), it may take longer for:</p> <ul> <li>All replicas to receive a message.</li> <li>Consumers configured with read committed isolation (like transactional consumers) to read it.</li> </ul> <p>So, increasing this setting improves cluster stability, but potentially increases end-to-end latency \u2014 the time between when a producer writes a message and when all consumers can see it.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#6-balance-between-stability-and-responsiveness","title":"6. Balance between stability and responsiveness","text":"<p>You can think of these two settings as sensitivity knobs:</p> Setting Detects Too Low \u2192 Too High \u2192 Default (since Kafka 2.5) <code>zookeeper.session.timeout.ms</code> Broker heartbeat loss Frequent re-elections, instability Slow failure detection 18000 ms <code>replica.lag.time.max.ms</code> Replication delay Frequent ISR removals Slower replication / higher latency 30000 ms <p>Kafka 2.5 raised both defaults because production clusters, especially in cloud environments, were becoming too \u201csensitive\u201d \u2014 reacting to transient conditions that weren\u2019t real failures.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#7-how-these-interact","title":"7. How these interact","text":"<p>If a broker experiences a GC pause or network lag:</p> <ul> <li>It might stop sending ZooKeeper heartbeats.</li> <li>It might stop fetching from the leader.</li> </ul> <p>These two timeouts act independently but together:</p> <ul> <li>If the broker fails ZooKeeper heartbeats \u2192 it\u2019s considered dead cluster-wide.</li> <li>If the broker fetches too slowly \u2192 it\u2019s marked out of sync but remains alive.</li> </ul> <p>Kafka\u2019s updated defaults aim to minimize false positives (temporary slowness causing unnecessary failovers) while still ensuring reasonable detection of real failures.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#8-summary","title":"8. Summary","text":"Concept Description Out-of-sync replicas Followers that are lagging or disconnected from the leader Cause 1: ZooKeeper session loss Broker missed heartbeats \u2192 considered dead Cause 2: Replication lag Follower too far behind the leader <code>zookeeper.session.timeout.ms</code> How long ZooKeeper waits for heartbeats (default: 18s) <code>replica.lag.time.max.ms</code> How long a follower can lag before being removed from ISR (default: 30s) Effect of longer timeouts More stable clusters, but slightly higher failover and data replication latency"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#in-essence","title":"In essence:","text":"<p>Kafka 2.5 made these parameters more tolerant to transient slowdowns. The goal is to prevent flapping \u2014 situations where brokers or replicas oscillate between \u201cin sync\u201d and \u201cout of sync\u201d due to short-lived lags \u2014 at the cost of slightly higher end-to-end latency for committed messages.</p> <p>The overall tradeoff is stability and durability over minimal latency, which is the right choice for most production systems.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#persisting-to-disk","title":"Persisting to Disk","text":""},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#1-kafkas-durability-model-in-context","title":"1. Kafka\u2019s durability model in context","text":"<p>Kafka is designed for high-throughput, low-latency data streaming, and part of that efficiency comes from asynchronous disk I/O. When a producer sends a message:</p> <ul> <li>The message is written to the broker\u2019s write-ahead log (in memory or OS page cache).</li> <li>The broker then acknowledges the message to the producer before it\u2019s necessarily written to physical disk.</li> </ul> <p>This design lets Kafka achieve very high throughput while maintaining good fault tolerance through replication.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#2-acknowledgments-vs-persistence","title":"2. Acknowledgments vs. persistence","text":"<p>Kafka acknowledges a message when the configured number of replicas (controlled by <code>acks</code> and <code>min.insync.replicas</code>) have received it. However:</p> <ul> <li>Those replicas may not have flushed the message to disk yet.</li> <li>The data could still be in the Linux page cache (in memory), waiting to be written to disk later.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#why-this-is-still-safe-most-of-the-time","title":"Why this is still \u201csafe\u201d (most of the time)","text":"<p>Kafka assumes that having multiple replicas \u2014 ideally on separate racks or availability zones \u2014 gives enough durability:</p> <ul> <li>Even if one machine crashes before flushing to disk, others will still have the data in memory.</li> <li>It\u2019s statistically rare for multiple racks to fail simultaneously.</li> </ul> <p>So, Kafka prioritizes replication across nodes over immediate disk flushes for performance.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#3-linux-page-cache-and-kafkas-io-behavior","title":"3. Linux page cache and Kafka\u2019s I/O behavior","text":"<p>Kafka relies on the Linux page cache to handle writes efficiently:</p> <ul> <li>When Kafka appends data to a log segment, it writes to the filesystem, which typically buffers the data in RAM.</li> <li> <p>The operating system decides when to flush (sync) those buffers to disk \u2014 usually based on:</p> </li> <li> <p>Memory pressure</p> </li> <li>Time since last flush</li> <li>Background kernel flush daemons</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#benefit","title":"Benefit:","text":"<p>High throughput, because Kafka avoids blocking the write path for disk I/O on every message.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#risk","title":"Risk:","text":"<p>If a broker and its OS both crash before the page cache is flushed, recent data not yet synced to disk can be lost \u2014 even if it was acknowledged to the producer.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#4-when-kafka-flushes-to-disk","title":"4. When Kafka flushes to disk","text":"<p>Kafka explicitly flushes messages to disk in three main situations:</p> <ol> <li> <p>When rotating log segments</p> </li> <li> <p>Each topic partition log is split into segments (default 1 GB).</p> </li> <li>When the active segment reaches 1 GB, Kafka closes it and opens a new one.</li> <li> <p>During this rotation, Kafka flushes the closed segment to disk.</p> </li> <li> <p>Before a broker restart (graceful shutdown)</p> </li> <li> <p>During a controlled shutdown, Kafka flushes all logs to disk to avoid data loss.</p> </li> <li> <p>When triggered by manual or time-based configuration</p> </li> <li> <p>Through the <code>flush.messages</code> or <code>flush.ms</code> configurations.</p> </li> </ol> <p>Otherwise, Kafka relies on the operating system to flush data automatically.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#5-tuning-with-flushmessages-and-flushms","title":"5. Tuning with <code>flush.messages</code> and <code>flush.ms</code>","text":"<p>Kafka provides two configuration parameters that control how often it forces a disk flush:</p> Parameter Description <code>flush.messages</code> The maximum number of messages that can be written to a log segment before Kafka forces a flush to disk. <code>flush.ms</code> The maximum time (in milliseconds) that Kafka will wait before forcing a flush to disk."},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#example","title":"Example","text":"<ul> <li><code>flush.messages=10000</code> \u2192 Kafka flushes after every 10,000 messages.</li> <li><code>flush.ms=5000</code> \u2192 Kafka flushes every 5 seconds.</li> </ul> <p>Setting either value to <code>0</code> disables time/message-based forced flushing, letting the OS handle it.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#6-trade-offs-of-frequent-flushing","title":"6. Trade-offs of frequent flushing","text":"Setting Pros Cons Frequent flush (low values) Minimizes risk of data loss if the broker crashes (more durable). Reduces throughput \u2014 frequent disk writes cause I/O bottlenecks. Infrequent flush (high values / disabled) Maximizes throughput \u2014 Kafka benefits from OS caching and batched writes. Increases risk of losing a few seconds of acknowledged data if a broker crashes before flush. <p>In practice, most Kafka deployments rely on the defaults, allowing Linux to manage flush timing, because:</p> <ul> <li>Kafka\u2019s replication provides redundancy.</li> <li>Disk I/O is expensive, and immediate syncs would drastically reduce performance.</li> </ul>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#7-modern-reliability-context","title":"7. Modern reliability context","text":"<p>Even though Kafka doesn\u2019t flush every message to disk, reliability is still very high due to:</p> <ul> <li>Replication \u2014 multiple brokers have copies.</li> <li>Leader election rules \u2014 only in-sync replicas can become new leaders (when <code>unclean.leader.election.enable=false</code>).</li> <li>Producer acks and retries \u2014 producers can require acknowledgment from all replicas (<code>acks=all</code>).</li> </ul> <p>So Kafka effectively treats replication across brokers as more durable than synchronous disk persistence on one node.</p>"},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#8-summary_1","title":"8. Summary","text":"Concept Description Acknowledgment Kafka confirms to producers once replicas receive the message (not necessarily flushed to disk). Disk flush Writing data from memory (page cache) to physical disk. Flush timing Happens during log rotation, shutdown, or as configured by <code>flush.messages</code> / <code>flush.ms</code>. Default behavior Kafka relies on the OS page cache for efficiency; replication ensures durability. Tradeoff Frequent flushing = safer but slower; infrequent flushing = faster but less durable if multiple failures occur."},{"location":"streaming/kafka/54-Kafka_Keeping_Replicas_In_Sync/#in-short","title":"In short:","text":"<p>Kafka\u2019s design assumes that replication across brokers is safer and faster than forcing each broker to sync every message to disk. However, if your system cannot tolerate any acknowledged-message loss \u2014 such as in financial transaction logs or audit trails \u2014 you can configure:</p> <pre><code>flush.messages=1\nflush.ms=1000\n</code></pre> <p>to ensure each message (or small batch) is flushed promptly, at the cost of throughput.</p> <p>In most production use cases, leaving flushing asynchronous provides the right balance of performance, resilience, and efficiency.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/","title":"Scenarios Of Potential Data Loss in Producers","text":""},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#1-kafka-reliability-depends-on-both-broker-and-producer-configuration","title":"1. Kafka reliability depends on both broker and producer configuration","text":"<p>Kafka\u2019s fault-tolerance and durability model is shared responsibility:</p> <ul> <li>Brokers ensure replicated storage, consistent logs, and controlled leader election.</li> <li>Producers control acknowledgment behavior and error handling.</li> </ul> <p>If either side is misconfigured, data can still be lost \u2014 even in a \u201cperfect\u201d cluster.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#2-broker-reliability-setup-the-good-part","title":"2. Broker reliability setup (the good part)","text":"<p>In both examples, the brokers are configured properly for reliability:</p> <ul> <li>Replication factor = 3: Each partition has three replicas on different brokers.</li> <li> <p>Unclean leader election disabled (<code>unclean.leader.election.enable=false</code>):</p> </li> <li> <p>Kafka will not promote an out-of-sync replica to leader.</p> </li> <li>This ensures that only replicas with the latest committed data can become leaders.</li> <li>Prevents data loss due to \u201cdirty\u201d leader elections.</li> </ul> <p>So from the broker\u2019s point of view, everything is safe and consistent. But reliability also depends on how the producer sends messages and interprets acknowledgments.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#3-scenario-1-producer-sends-with-acks1","title":"3. Scenario 1 \u2014 Producer sends with <code>acks=1</code>","text":"<p>This is one of the most common (and dangerous) misconfigurations.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#what-happens","title":"What happens:","text":"<ol> <li>Producer sends a message to the partition leader.</li> <li>Leader writes it to its local log (in memory or disk buffer).</li> <li>Leader immediately responds \u201cSuccess\u201d to the producer.</li> <li>Followers haven\u2019t yet replicated the message.</li> <li>Leader crashes before followers can copy the message.</li> <li>A follower is elected as new leader \u2014 but it never received the message.</li> </ol>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#result","title":"Result:","text":"<ul> <li>The message is lost, because no surviving replica had it.</li> <li>The producer thinks the message was stored successfully.</li> <li>The consumer never sees the message.</li> <li>The system remains consistent (no consumer sees phantom data), but the producer\u2019s data is gone.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#why-it-happens","title":"Why it happens:","text":"<p><code>acks=1</code> tells Kafka:</p> <p>\u201cAcknowledge once the leader writes the message \u2014 I don\u2019t need confirmation that replicas have it.\u201d</p> <p>This sacrifices durability for latency. If the leader fails immediately after acknowledging, the message vanishes.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#takeaway","title":"Takeaway","text":"<ul> <li>Broker replication can\u2019t protect you if producers don\u2019t wait for replication confirmation.</li> <li><code>acks=1</code> = high throughput, low durability.</li> <li><code>acks=all</code> = slower, but guarantees durability once enough replicas have data.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#4-scenario-2-producer-uses-acksall-better","title":"4. Scenario 2 \u2014 Producer uses <code>acks=all</code> (better)","text":"<p>Now we use a more reliable setting:</p> <pre><code>acks=all\nmin.insync.replicas=2\n</code></pre> <p>This means a write is only considered successful when at least two replicas have acknowledged it.</p> <p>But reliability can still fail if the producer mishandles transient errors.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#what-happens_1","title":"What happens:","text":"<ol> <li>Producer sends a message to the leader.</li> <li>At that moment, the leader crashes before responding.</li> <li>Kafka hasn\u2019t yet elected a new leader.</li> <li>Broker responds with <code>LeaderNotAvailable</code> (or possibly a timeout error).</li> <li>The producer gets an exception.</li> </ol> <p>If the producer is not programmed to retry, it simply moves on. The message was never committed to Kafka \u2014 so it\u2019s lost forever.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#why-this-happens","title":"Why this happens:","text":"<p>The broker side was fine \u2014 replication worked correctly. The issue is on the producer side, which failed to:</p> <ul> <li>Recognize the transient nature of the error.</li> <li>Retry the write once the new leader was elected.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#what-the-producer-should-have-done","title":"What the producer should have done:","text":"<p>Implement idempotent, retryable writes:</p> <pre><code>acks=all\nretries=Integer.MAX_VALUE\nenable.idempotence=true\ndelivery.timeout.ms=120000\nmax.in.flight.requests.per.connection=1\n</code></pre> <p>This ensures that:</p> <ul> <li>Transient network or leader election errors are retried.</li> <li>Duplicate messages aren\u2019t created (thanks to idempotent producer IDs).</li> <li>The producer waits long enough for recovery.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#5-difference-between-producer-side-and-broker-side-data-loss","title":"5. Difference between producer-side and broker-side data loss","text":"Type of failure Root cause Broker fault? Producer fault? Data lost? acks=1, leader crash Message acknowledged by leader only, not replicated \u274c \u2705 \u2705 LeaderNotAvailable, no retry Producer gave up during leader election \u274c \u2705 \u2705 Broker crash, ISR replication loss Unclean leader election allowed \u2705 \u274c \u2705 Network delay / GC pauses Temporary ISR flapping \u26a0\ufe0f \u274c Possible transient loss"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#6-best-practices-for-producer-reliability","title":"6. Best practices for producer reliability","text":"<p>To ensure no acknowledged message is lost, configure producers carefully:</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#core-reliability-configs","title":"Core reliability configs:","text":"<pre><code>acks=all\nretries=Integer.MAX_VALUE\nenable.idempotence=true\nmax.in.flight.requests.per.connection=1\ndelivery.timeout.ms=120000\nlinger.ms=5\nbatch.size=32768\n</code></pre> <ul> <li><code>acks=all</code>: Waits for acknowledgment from all in-sync replicas.</li> <li><code>enable.idempotence=true</code>: Prevents duplicates during retries.</li> <li><code>retries</code>: Keeps retrying until success or timeout.</li> <li><code>max.in.flight.requests.per.connection=1</code>: Maintains order guarantees.</li> <li><code>delivery.timeout.ms</code>: Overall time window for retries.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#optional-safety-layer","title":"Optional safety layer:","text":"<p>Use producer transaction APIs (<code>initTransactions()</code>, <code>beginTransaction()</code>, <code>commitTransaction()</code>) if you require exactly-once semantics (EOS).</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#7-summary","title":"7. Summary","text":"Concept Description Broker replication Protects data after it\u2019s committed; cannot protect uncommitted leader-only writes. <code>acks=1</code> risk Leader crash before replication causes silent data loss. <code>acks=all</code> advantage Waits for all in-sync replicas; ensures durability once acknowledged. Error handling Producers must retry transient errors to avoid message loss. Idempotent producers Prevent duplicates during retries; ensure exactly-once delivery semantics. Unclean leader election (disabled) Prevents stale data from replacing newer data after failure."},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#in-essence","title":"In essence:","text":"<p>Kafka brokers can be configured for strong durability guarantees, but those guarantees only apply to committed messages \u2014 data successfully replicated to the required number of in-sync replicas.</p> <p>If producers don\u2019t:</p> <ul> <li>Wait for all replicas (<code>acks=all</code>), and</li> <li>Retry after transient errors (especially during leader re-election),</li> </ul> <p>then data can still be lost before it ever becomes committed \u2014 even in a perfectly tuned cluster.</p> <p>True reliability in Kafka requires end-to-end correctness \u2014 from producer configuration and retry logic to broker replication and consumer offsets.</p> <p>Absolutely \u2014 let\u2019s go through this in a detailed, technical explanation of how <code>linger.ms</code> works in Kafka producers, and how it fits into the reliability and performance model described above.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#1-context-producer-batching-and-efficiency","title":"1. Context: Producer batching and efficiency","text":"<p>When a Kafka producer sends messages, it doesn\u2019t necessarily transmit them immediately, one by one, over the network. Instead, it batches messages together per partition before sending them to the broker.</p> <p>Batching helps:</p> <ul> <li>Reduce network overhead (fewer TCP requests).</li> <li>Increase throughput (more messages per request).</li> <li>Improve compression efficiency (since larger batches compress better).</li> </ul> <p>The producer collects messages in a buffer (a memory region managed by the producer client), and then sends them either:</p> <ul> <li>When the batch is full (reaches <code>batch.size</code> in bytes), or</li> <li>When a certain amount of time passes (<code>linger.ms</code>).</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#2-what-lingerms-does","title":"2. What <code>linger.ms</code> does","text":"<p><code>linger.ms</code> controls how long the producer waits before sending a batch of messages, even if it\u2019s not full.</p> <p>Definition:</p> <p><code>linger.ms</code> = the maximum time (in milliseconds) that the producer will wait for additional messages before sending the current batch to the broker.</p> <p>So <code>linger.ms</code> adds an intentional, small delay to allow more messages to accumulate in the batch.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#example","title":"Example","text":"<p>If <code>linger.ms=0</code> (the default):</p> <ul> <li>The producer sends messages as soon as possible, as soon as there\u2019s at least one record ready to send.</li> <li>Batching still happens, but only if multiple messages arrive very close together (within the same poll loop or CPU cycle).</li> </ul> <p>If <code>linger.ms=5</code>:</p> <ul> <li>The producer will wait up to 5 milliseconds before sending the batch.</li> <li>If enough messages arrive during that interval to fill the batch, they are sent together.</li> <li>If not, the batch is sent at the end of 5 milliseconds anyway.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#3-the-relationship-between-lingerms-and-batchsize","title":"3. The relationship between <code>linger.ms</code> and <code>batch.size</code>","text":"<p>Both <code>linger.ms</code> and <code>batch.size</code> control batching behavior, but in different ways:</p> Parameter Description Effect <code>batch.size</code> Maximum number of bytes per batch (per partition). When full, batch is sent immediately, even before <code>linger.ms</code> expires. <code>linger.ms</code> Maximum wait time before sending a batch. Forces a flush after this delay, even if batch isn\u2019t full. <p>You can think of them like two triggers:</p> <ul> <li>Send the batch if either condition is met first.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#4-why-this-improves-performance","title":"4. Why this improves performance","text":"<p>Without <code>linger.ms</code>, each message might be sent in its own request if messages arrive sporadically \u2014 this creates a high number of small network requests.</p> <p>By introducing a short linger period (typically 5\u201310 milliseconds):</p> <ul> <li>You allow the producer to group more messages together.</li> <li>Network utilization improves dramatically.</li> <li>Compression becomes more efficient (e.g., GZIP or Snappy work better with larger batches).</li> <li>CPU and broker load decrease because there are fewer total requests to process.</li> </ul> <p>The result is higher throughput and lower overall system overhead.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#5-trade-offs-and-impact-on-latency","title":"5. Trade-offs and impact on latency","text":"Setting Throughput End-to-End Latency Use Case <code>linger.ms = 0</code> Low Lowest Real-time, ultra-low-latency use cases <code>linger.ms = 5\u201310</code> High Slightly higher (adds a few ms) Most streaming / analytics workloads <code>linger.ms = 50+</code> Very high Noticeable delay Bulk ingestion, ETL, or log aggregation <p>So, increasing <code>linger.ms</code> improves throughput, but introduces a small additional delay \u2014 bounded by the configured time.</p> <p>For most workloads, values between 5\u201310 ms give a good balance of latency and efficiency.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#6-reliability-considerations","title":"6. Reliability considerations","text":"<p><code>linger.ms</code> affects when data is sent, not how reliably it\u2019s acknowledged. Reliability still depends on:</p> <ul> <li><code>acks</code></li> <li><code>min.insync.replicas</code></li> <li><code>enable.idempotence</code></li> <li><code>retries</code></li> </ul> <p>However, <code>linger.ms</code> interacts indirectly with reliability in these ways:</p> <ol> <li>Larger batches mean fewer requests, so fewer chances for network errors per message.</li> <li>But if the producer crashes before the batch is sent, those buffered messages are lost (they were never transmitted).</li> </ol> <p>To mitigate this:</p> <ul> <li>Keep <code>linger.ms</code> modest.</li> <li>Ensure the producer has <code>acks=all</code> and <code>enable.idempotence=true</code> to guarantee delivery once the batch is sent.</li> </ul>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#7-how-lingerms-fits-into-the-full-producer-send-pipeline","title":"7. How <code>linger.ms</code> fits into the full producer send pipeline","text":"<p>A simplified flow:</p> <ol> <li>The producer application calls <code>producer.send(record)</code>.</li> <li>The record is placed in a partition-specific buffer.</li> <li> <p>The producer I/O thread monitors these buffers:</p> </li> <li> <p>If the batch is full (<code>batch.size</code>), send it immediately.</p> </li> <li>If the batch is not full, wait up to <code>linger.ms</code> milliseconds.</li> <li>When either limit is reached, the batch is sent as one request to the broker.</li> <li>The producer waits for acknowledgment based on <code>acks</code>.</li> </ol> <p>So, <code>linger.ms</code> effectively throttles the send frequency of batches without affecting the acknowledgment semantics.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#8-typical-production-configurations","title":"8. Typical production configurations","text":"Setting Typical Value Purpose <code>acks=all</code> Strong durability Require all ISR replicas to acknowledge <code>enable.idempotence=true</code> Exactly-once delivery Avoid duplicates when retrying <code>batch.size=32KB \u2013 128KB</code> Efficient batching Increase throughput <code>linger.ms=5 \u2013 10</code> Balanced latency and throughput Allow small message accumulation <code>retries=Integer.MAX_VALUE</code> Retry on transient errors Ensure delivery during leader election <p>Together, these settings give high throughput, strong durability, and low probability of message loss.</p>"},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#9-summary","title":"9. Summary","text":"Concept Description Purpose Controls how long the producer waits before sending a batch of messages, even if it isn\u2019t full. Default <code>linger.ms=0</code> \u2014 send immediately. Effect Higher values improve throughput and compression; lower values reduce latency. Interaction Works with <code>batch.size</code>; batch is sent when either limit is hit. Best Practice Set between 5\u201310 ms for typical workloads; higher only for bulk ETL or non-real-time ingestion."},{"location":"streaming/kafka/55-Kafka_Using_Producers_Reliable_System_Scenarios/#in-summary","title":"In summary","text":"<p><code>linger.ms</code> allows the producer to trade a few milliseconds of delay for significantly improved throughput and network efficiency. It doesn\u2019t affect message durability \u2014 that\u2019s handled by <code>acks</code>, replication, and idempotence \u2014 but it does help Kafka scale efficiently by reducing request overhead.</p>"}]}