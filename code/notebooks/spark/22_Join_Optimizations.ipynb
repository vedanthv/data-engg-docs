{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b0f5c8-bc83-4016-9ac2-34184e7ca567",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/numBXo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0171b09-42fb-47f5-8309-2001b583d964",
   "metadata": {},
   "source": [
    "Spark tries to bring city_id's that are same together in one executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2bcec-d8ad-4eeb-bcbc-f1c829e25a06",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/pRV8kw.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c78af9-6439-4cfe-81a0-3727b660b17f",
   "metadata": {},
   "source": [
    "## Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0ef11-9a23-4dde-8d1f-6b514545cd3f",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/B0o9mQ.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1840915-bcf9-49c6-9365-b73f40f7934b",
   "metadata": {},
   "source": [
    "In above example smaller dataset city is hashed and joined with bigger table sales. There is no sorting, its reliable strategy if we have one small table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6e675-2123-4e20-a746-e10576c7adbe",
   "metadata": {},
   "source": [
    "## Sort Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135398b-3a79-4063-a20e-d309166bb933",
   "metadata": {},
   "source": [
    "In this case there is sorting that happens first like in the first image, then there is a merge and join.\n",
    "\n",
    "Its useful when we have two big tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d92e6-4922-4e5d-8a0d-da0d3589daeb",
   "metadata": {},
   "source": [
    "## Broadcast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174bc03-ba5e-416f-9530-949e31994597",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/aJbK95.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b961cb-59b7-4dd5-85bd-4d3cf7f53e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/22 02:04:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-20-231.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Join Optimizations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71a04eb73da0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Join Optimizations\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d61a984-ee88-4b54-80c8-1b0002165f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77960486-3539-40c5-815d-c56521187e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a67fd6a-f92b-4c92-9727-fa68f527ff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7053c715-b00d-4d4c-8148-5378149c8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"data/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2eac723-6132-4057-9e36-95bbdd3824fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda3cb10-6b04-489d-ac41-2d3c96f9143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Without broadcast\n",
    "df_joined = emp.join(dept,on=emp.department_id==dept.department_id,how = \"left_outer\")\n",
    "\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec5a5e5-7f7a-453f-94a7-9c0ee5fccfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [department_id#189], [department_id#211], LeftOuter\n",
      ":- *(1) Sort [department_id#189 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(department_id#189, 200), ENSURE_REQUIREMENTS, [plan_id=205]\n",
      ":     +- FileScan csv [first_name#182,last_name#183,job_title#184,dob#185,email#186,phone#187,salary#188,department_id#189] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/SparkLearning/data/employee_records.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- *(3) Sort [department_id#211 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(department_id#211, 200), ENSURE_REQUIREMENTS, [plan_id=217]\n",
      "      +- *(2) Filter isnotnull(department_id#211)\n",
      "         +- FileScan csv [department_id#211,department_name#212,description#213,city#214,state#215,country#216] Batched: false, DataFilters: [isnotnull(department_id#211)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/SparkLearning/data/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ced6f-d53c-4538-89ae-402cdd3c2e45",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/QxaDpX.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf0cc4-c823-4c51-bcad-dbe3bff66d01",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/D91exq.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0278b-ac07-417f-ac09-2ee5cd59c0ad",
   "metadata": {},
   "source": [
    "### BroadCast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f701f5f-52c0-473c-a50a-f14a37f0211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_joined = emp.join(broadcast(dept),on=emp.department_id==dept.department_id,how = \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64ced28d-79df-4505-b53e-37943e7d0711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae61579-7fb5-4d70-b9ef-b353917eb343",
   "metadata": {},
   "source": [
    "Two jobs here, one for reading the csv and other for joining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324fc61-9fc9-4636-9b81-baf40c90c8ef",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/ilNejQ.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c832dc-8ff8-4cbf-be17-1605122ad349",
   "metadata": {},
   "source": [
    "### Big Table vs Big Table Join Sort Merge No Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f17042d9-6b1d-4fcd-9d47-9044ee4b9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c373a8-df99-4a0e-9db5-325b58739790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"data/new_sales_10M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbb2635-a324-4227-9062-3c6f0bd752c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "7202569"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23030350-1ca8-4000-a16a-d08f00db8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"data/cities_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc9e01a-36fa-4c96-b459-f27ef6afe5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349391"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8093e5d5-970e-4c94-b8e1-8d8002ad7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Data\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e455f40-cae0-44fc-8af5-0f106c17331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744a5834-5795-4e0b-b759-3827f6f25b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [city_id#94], [city_id#112], LeftOuter\n",
      ":- *(1) Sort [city_id#94 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(city_id#94, 200), ENSURE_REQUIREMENTS, [plan_id=264]\n",
      ":     +- FileScan csv [transacted_at#89,trx_id#90,retailer_id#91,description#92,amount#93,city_id#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/SparkLearning/data/new_sales_10M.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n",
      "+- *(3) Sort [city_id#112 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(city_id#112, 200), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- *(2) Filter isnotnull(city_id#112)\n",
      "         +- FileScan csv [city_id#112,city#113,state#114,state_abv#115,country#116] Batched: false, DataFilters: [isnotnull(city_id#112)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/SparkLearning/data/cities_large.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4f347-3418-432d-acaa-97c6541b30c7",
   "metadata": {},
   "source": [
    "Let's try broadcasting the huge cities dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abbf2e21-4086-4329-8d73-cdc4a2de1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Data\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_sales_joined_broadcast = sales.join(broadcast(city), on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d305d8ee-e215-4094-9431-6fab79a8f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 02:13:02 WARN MemoryStore: Not enough space to cache broadcast_24 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 02:13:02 WARN BlockManager: Persisting block broadcast_24 to disk instead.\n",
      "25/02/22 02:13:06 WARN MemoryStore: Not enough space to cache broadcast_24 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 02:13:08 WARN MemoryStore: Not enough space to cache broadcast_24 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 02:13:19 WARN MemoryStore: Not enough space to cache broadcast_24 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 02:13:32 WARN MemoryStore: Not enough space to cache broadcast_24 in memory! (computed 432.0 MiB so far)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sales_joined_broadcast.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbae5b-b60a-45c4-ab44-bf2d27eb4de0",
   "metadata": {},
   "source": [
    "There's not enough storage in the executors so jobs start failing or spill to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829c07c-6fba-4537-a3c4-b52404d6de70",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/XpZuFU.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0e6e1dd-ef24-483e-a3d1-ed2c64313279",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
