{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e1862d-40fd-4464-bcf1-2c564953f0fc",
   "metadata": {},
   "source": [
    "Note : This demo is not very clear since Im running it on local cluster, its not able to read the entire data, best to use databricks cluster for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4301baec-fe0a-4aa4-b81e-05321c1a743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/17 02:05:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-20-231.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Shuffles</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c486c667500>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Shuffles\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31932a85-77f4-4e73-ab7a-473c883b64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc746e8-f12c-457e-8acb-58db20d58ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coaleascePartitions.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212808c2-32cc-4f67-80ce-875fb0890492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10M records\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cec50a0-eb60-45b7-8d6f-7a4766637ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find out avg salary as per dept\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf05699-1dc7-4aca-9c03-6dec25f72554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write data for performance benchmarking\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70dd87dd-2c1e-4506-bcad-d01d70ddf033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_avg.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a4d3b-2f7a-4886-b8eb-f24541eae1f1",
   "metadata": {},
   "source": [
    "If you carefully observe, when we read the data, 200 tasks are formed because the default shuffle partitions is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7079d21-9256-4732-8b98-63166cc997c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ab166-ed51-484f-b903-6d8e97d43776",
   "metadata": {},
   "source": [
    "Some of the tasks do not even read in any shuffled data, so its just a waste of compute running it. SO let's decrease the number of shuffle partitions to avoid this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d69d632-f4da-49e3-9604-38d498aca1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bad303-7414-4e1f-8cdc-604ab61f447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|partition_id|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
      "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|           0|\n",
      "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|           0|\n",
      "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|           0|\n",
      "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|           0|\n",
      "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|           0|\n",
      "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|           0|\n",
      "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|           0|\n",
      "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|           0|\n",
      "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|           0|\n",
      "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|           0|\n",
      "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|           0|\n",
      "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|           0|\n",
      "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|           0|\n",
      "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|           0|\n",
      "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|           0|\n",
      "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|           0|\n",
      "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|           0|\n",
      "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|           0|\n",
      "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|           0|\n",
      "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|           0|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id = 0\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6032e32d-b251-4698-abe6-6ffb6aa3ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the partitioned data\n",
    "\n",
    "emp_part = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/employee_records.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596d2249-3952-45d8-bbb1-b5fcd6ee49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg = emp_part.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee96e4f-d5c8-44dd-8373-1b35879d3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04de2d-14da-45e8-a7a9-5f5f93515756",
   "metadata": {},
   "source": [
    "Observe that in spark UI we can see, there are lesser number of tasks and most of them read some shuffle data rather than being idle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa76b60-78a3-49ac-9999-0448de42efa6",
   "metadata": {},
   "source": [
    "Also now since we did group by on partitioned data, there is no mix and match of dept records in the executors so the shuffling is reduced when we do aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "645356f2-a48f-44d8-bbe0-44228c87e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa715037-1ff4-415c-bacf-cd1555161671",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_parti = emp_avg.withColumn(\"partition_id\",spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd70f61f-3c05-4d45-b7ab-39be6c374a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------+\n",
      "|department_id|           avg_sal|partition_id|\n",
      "+-------------+------------------+------------+\n",
      "|           10| 502682.2575766687|           2|\n",
      "|            5| 504167.9429997006|           2|\n",
      "|            1|504876.96401242825|           3|\n",
      "|            3| 504697.6808514883|           3|\n",
      "|            2| 503563.2174529479|           6|\n",
      "|            6|504428.12590014644|           9|\n",
      "|            9| 504945.3055672206|           9|\n",
      "|            7|504514.38453985273|          11|\n",
      "|            4| 505419.4963977089|          14|\n",
      "|            8| 505299.1226286386|          15|\n",
      "+-------------+------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "emp_parti.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fceb9a6-581e-4ed2-adeb-01fa2c88f895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
