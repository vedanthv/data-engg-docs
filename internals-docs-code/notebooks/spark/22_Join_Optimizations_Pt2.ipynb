{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3749d451-0066-453c-b1e2-b89bf6e01f9b",
   "metadata": {},
   "source": [
    "### Data Spill to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d4aba0d-447e-4ea7-b665-e44502b85284",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29825ce6-06a0-4189-91f4-1bc33047e8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-20-231.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Join Optimization Pt2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e231cfb1d30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Join Optimization Pt2\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n",
    ").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cabf357e-9e2a-4948-8546-75e887b36f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.shuffle.spill\", \"false\")  # Disable spilling during shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19185a19-fa99-4dc6-ae1b-f1517eac7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d5404a4-3dea-48ac-a9bc-6a3b9b18f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"data/new_sales_10M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "659e3918-37b3-40a4-b0f3-be7c1011e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"data/cities_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2a529a-e7ba-4d46-90bb-96e56546fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Data\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_sales_joined_broadcast = sales.join(broadcast(city), on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11076154-25d5-42ef-92c2-2b9f27ee6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 04:25:09 WARN MemoryStore: Not enough space to cache broadcast_2 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 04:25:09 WARN BlockManager: Persisting block broadcast_2 to disk instead.\n",
      "25/02/22 04:25:13 WARN MemoryStore: Not enough space to cache broadcast_2 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 04:25:28 WARN MemoryStore: Not enough space to cache broadcast_2 in memory! (computed 432.0 MiB so far)\n",
      "25/02/22 04:25:42 WARN MemoryStore: Not enough space to cache broadcast_2 in memory! (computed 432.0 MiB so far)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sales_joined_broadcast.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e075e8b-e04d-469a-ad14-ac3550f14443",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/5NbZWs.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f5d66-7616-49e4-9a62-56049f72c579",
   "metadata": {},
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072073f5-914e-4959-bc42-b3ccb9c19c0f",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/kRdA2u.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36181737-ea1f-4964-bfcc-77162f335f86",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/F6uSVI.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04d4bc-b997-4988-bc73-c87bef1d8adf",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/jfyVQC.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378aa7e5-797b-4d4e-bedd-5af24c5b2e42",
   "metadata": {},
   "source": [
    "Bucket i from sales and Bucket i from city will be read form same executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460bda2-3c2c-49c1-a070-1f33807088cb",
   "metadata": {},
   "source": [
    "### Partitioning vs Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c67c1-a509-46c2-a550-e62247d6b184",
   "metadata": {},
   "source": [
    "**Partitioning in Spark**\n",
    "\n",
    "Partitioning is an integral concept in Spark that controls how the data is physically distributed across various nodes in the cluster during data processing. Spark, by default, performs data partitioning, which can also be manually optimized based on the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1f463-4ba2-4963-933b-e1fb141d67de",
   "metadata": {},
   "source": [
    "**How does Partitioning work?**\n",
    "\n",
    "In Spark, an RDD (Resilient Distributed Dataset), DataFrame, or Dataset is divided into a number of partitions, each of which can be computed on different nodes in the cluster. Data in each partition is processed in parallel, ensuring efficient use of cluster resources and enhancing the speed of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701221f7-f825-42c6-bc21-ba598ec93ee3",
   "metadata": {},
   "source": [
    "**Custom Partitioning**\n",
    "\n",
    "While Spark’s default partitioning works well in most cases, there are instances where custom partitioning might be necessary. For example, if your data is skewed, with some keys having significantly more values than others, the default partitioning may result in some partitions being much larger than others. This could lead to unequal distribution of work across the nodes. In such scenarios, one can use a custom partitioner to ensure a more even distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3338444-fc66-4ccd-b44d-9d80bccf2286",
   "metadata": {},
   "source": [
    "**Bucketing in Spark**\n",
    "\n",
    "Bucketing is a technique in Spark that is used to distribute data across multiple buckets or files based on the hash of a column value. This method is particularly useful when working with large datasets and performing operations like joins, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519670e-f9bf-4bf9-9277-14a39f301dae",
   "metadata": {},
   "source": [
    "**How does Bucketing work?**\n",
    "\n",
    "Bucketing works by specifying a column and a number of buckets during the creation of the DataFrame. Spark then applies a hash function to the specified column and divides the data into buckets corresponding to the hash values. The number of buckets remains fixed, so the distribution of data doesn’t change with the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4fd8d-85dd-440d-97c3-05a867338086",
   "metadata": {},
   "source": [
    "**Why is Bucketing important?**\n",
    "\n",
    "Performance Improvement: Bucketing can significantly improve the performance of Spark jobs that involve shuffle operations like groupBy, join, orderBy, etc., by limiting the number of outputs and reducing the data shuffle across the network.\n",
    "\n",
    "Avoid Data Skew: Bucketing can help avoid data skew in certain operations, leading to more efficient utilization of resources.\n",
    "\n",
    "Reduce Data Redundancy: When performing operations on a subset of data, bucketing allows Spark to avoid full data scan, reducing IO operations and improving query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982657b-a356-4e6a-9f79-68b89194287c",
   "metadata": {},
   "source": [
    "It’s essential to note that bucketing has its overhead, namely the computational cost of computing the hash of the bucket column and the increased storage used by many small files. Therefore, bucketing is most beneficial when the computational or IO savings outweigh these costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d9e55-9f25-486f-b03b-ca6eb5b8a33c",
   "metadata": {},
   "source": [
    "**Practical Examples**\n",
    "\n",
    "Partitioning in Spark\n",
    "\n",
    "Consider you have a large DataFrame df and you frequently run operations on it filtered by the country column. By default, Spark might distribute the data across the partitions arbitrarily, which means each operation involves scanning all partitions. But if you partition the DataFrame by country, Spark can optimize these operations by only accessing the relevant partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d53c76-fd69-4ff8-8bc6-5a194a2288ab",
   "metadata": {},
   "source": [
    "```\n",
    "df.write.partitionBy(\"country\").parquet(\"/path/to/data.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319a469-ca97-4ca8-ac03-1eae49753d86",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.parquet(\"/path/to/data.parquet\")\n",
    "df.filter(df.country == 'USA').show()\n",
    "```\n",
    "\n",
    "Spark skips all the partitions not related to USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a2241-a23d-49ed-a564-175901d646e3",
   "metadata": {},
   "source": [
    "**Bucketing in Spark**\n",
    "\n",
    "Suppose you have a DataFrame salesData with columns transactionId, customerId, itemId, and amount. If you frequently run queries that involve operations like JOIN or GROUP BY on customerId, you can use bucketing to speed up these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d4186-28cc-4a56-bd28-32e57f28c07e",
   "metadata": {},
   "source": [
    "```\n",
    "numBuckets = 100\n",
    "salesData.write.bucketBy(numBuckets, \"customerId\").sortBy(\"transactionId\").saveAsTable(\"salesData_bucketed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0498c-8456-47b4-b75e-50d29153dcdc",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/nLKgh3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda04921-dd74-4515-a4fa-0ca60f823d06",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04edfad1-e5b9-4696-903f-3248c37c620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"data/new_sales_10M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "766bbb70-4cf5-4cab-9e65-5cd2e16ff5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"data/cities_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1923c8cb-7bf4-40c5-88b1-65ac6aef404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write Sales data in Buckets\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"SparkLearning/data/sales_bucket\").saveAsTable(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56d9d5-f133-441e-8906-32101b7a06a6",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/hPN2OK.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1118db-f200-4221-9dca-4b23d73d9316",
   "metadata": {},
   "source": [
    "Data divided into 6 partitions, 4 files in each partition pertaining to no of buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d5d1-db7c-4412-906f-f5410072f94c",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/RAPerQ.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ed041a-ae76-416b-b53c-21d300348d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show tables in default''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e606af2-5534-4f01-b1ab-35897f7f6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"SparkLearning/data/city_bucket\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce38be-928c-408a-9565-f87073089a88",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/SfBEZz.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8511d5ac-ad48-4460-bf4d-e83670bff8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show tables in default''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3adf4f-474f-4a18-ae6f-e8ed11dd5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "# Read Sales table\n",
    "\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")\n",
    "\n",
    "# Read City table\n",
    "\n",
    "city_bucket = spark.read.table(\"city_bucket\")\n",
    "\n",
    "df_joined_bucketed = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c45ee056-8a31-40a3-bf43-c56115bc919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write dataset\n",
    "\n",
    "df_joined_bucketed.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95925797-6cf4-4186-849e-81c127063049",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/NSYGWo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee6fc1-0d00-49f7-90d0-ad2815009d7f",
   "metadata": {},
   "source": [
    "Observe there is no particular diff in times, both are approx 40s but here there is no shuffle involved + next join on these two tables would be faster as there is bucketing in place.\n",
    "\n",
    "Bucketing is a one time operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29561f1-9552-43ee-9906-7b8befe9484e",
   "metadata": {},
   "source": [
    "### Tasks Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29919af6-b4da-4387-9c64-862f5a8231b0",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/EUklwA.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63f2a7-2133-4fe6-8322-1fc52333d28c",
   "metadata": {},
   "source": [
    "Here there are 4 tasks because of 4 buckets.\n",
    "\n",
    "The input records for each task is the sum of (city bucket i) + (sales bucket i) - header rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b643ecb-392c-4250-8ec6-f471cec512cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to prevent truncation\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.max_colwidth', None) # Show full content of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd50155-26de-418d-95a6-4863566bd41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>_SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>31M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>29M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>28M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>31M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>29M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>28M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>31M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>29M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>28M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>31M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>29M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>28M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>31M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>29M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>28M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>19M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>27M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>26M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>25M</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1       2       3    4    5   6      7  \\\n",
       "0   -rw-r--r--  1  ubuntu  ubuntu    0  Feb  22  04:27   \n",
       "1   -rw-r--r--  1  ubuntu  ubuntu  22M  Feb  22  04:27   \n",
       "2   -rw-r--r--  1  ubuntu  ubuntu  31M  Feb  22  04:27   \n",
       "3   -rw-r--r--  1  ubuntu  ubuntu  29M  Feb  22  04:27   \n",
       "4   -rw-r--r--  1  ubuntu  ubuntu  28M  Feb  22  04:27   \n",
       "5   -rw-r--r--  1  ubuntu  ubuntu  22M  Feb  22  04:27   \n",
       "6   -rw-r--r--  1  ubuntu  ubuntu  31M  Feb  22  04:27   \n",
       "7   -rw-r--r--  1  ubuntu  ubuntu  29M  Feb  22  04:27   \n",
       "8   -rw-r--r--  1  ubuntu  ubuntu  28M  Feb  22  04:27   \n",
       "9   -rw-r--r--  1  ubuntu  ubuntu  22M  Feb  22  04:27   \n",
       "10  -rw-r--r--  1  ubuntu  ubuntu  31M  Feb  22  04:27   \n",
       "11  -rw-r--r--  1  ubuntu  ubuntu  29M  Feb  22  04:27   \n",
       "12  -rw-r--r--  1  ubuntu  ubuntu  28M  Feb  22  04:27   \n",
       "13  -rw-r--r--  1  ubuntu  ubuntu  22M  Feb  22  04:27   \n",
       "14  -rw-r--r--  1  ubuntu  ubuntu  31M  Feb  22  04:27   \n",
       "15  -rw-r--r--  1  ubuntu  ubuntu  29M  Feb  22  04:27   \n",
       "16  -rw-r--r--  1  ubuntu  ubuntu  28M  Feb  22  04:27   \n",
       "17  -rw-r--r--  1  ubuntu  ubuntu  22M  Feb  22  04:27   \n",
       "18  -rw-r--r--  1  ubuntu  ubuntu  31M  Feb  22  04:27   \n",
       "19  -rw-r--r--  1  ubuntu  ubuntu  29M  Feb  22  04:27   \n",
       "20  -rw-r--r--  1  ubuntu  ubuntu  28M  Feb  22  04:27   \n",
       "21  -rw-r--r--  1  ubuntu  ubuntu  19M  Feb  22  04:27   \n",
       "22  -rw-r--r--  1  ubuntu  ubuntu  27M  Feb  22  04:27   \n",
       "23  -rw-r--r--  1  ubuntu  ubuntu  26M  Feb  22  04:27   \n",
       "24  -rw-r--r--  1  ubuntu  ubuntu  25M  Feb  22  04:27   \n",
       "\n",
       "                                                                 8  \n",
       "0                                                         _SUCCESS  \n",
       "1   part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "2   part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "3   part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "4   part-00000-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  \n",
       "5   part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "6   part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "7   part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "8   part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  \n",
       "9   part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "10  part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "11  part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "12  part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  \n",
       "13  part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "14  part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "15  part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "16  part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  \n",
       "17  part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "18  part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "19  part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "20  part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  \n",
       "21  part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "22  part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00001.c000.csv  \n",
       "23  part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00002.c000.csv  \n",
       "24  part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00003.c000.csv  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "ls_output = subprocess.check_output(\"ls -lh /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket\", shell=True).decode(\"utf-8\")\n",
    "\n",
    "ls_lines = ls_output.splitlines()\n",
    "\n",
    "ls_data = [line.split() for line in ls_lines[1:]]  # Skip the first line (total count)\n",
    "\n",
    "df = pd.DataFrame(ls_data)\n",
    "\n",
    "# Show the table\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d24875-3996-4451-bee4-cdae13939918",
   "metadata": {},
   "source": [
    "Check 0th bucket data count for all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018f4cf6-a7c6-4338-bfd4-259ec34c8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22938797</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22878206</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22884271</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>22902138</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>ubuntu</td>\n",
       "      <td>19899932</td>\n",
       "      <td>Feb</td>\n",
       "      <td>22</td>\n",
       "      <td>04:27</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  1       2       3         4    5   6      7  \\\n",
       "0  -rw-r--r--  1  ubuntu  ubuntu  22938797  Feb  22  04:27   \n",
       "1  -rw-r--r--  1  ubuntu  ubuntu  22878206  Feb  22  04:27   \n",
       "2  -rw-r--r--  1  ubuntu  ubuntu  22884271  Feb  22  04:27   \n",
       "3  -rw-r--r--  1  ubuntu  ubuntu  22902138  Feb  22  04:27   \n",
       "4  -rw-r--r--  1  ubuntu  ubuntu  19899932  Feb  22  04:27   \n",
       "\n",
       "                                                                                                                                           8  \n",
       "0  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "1  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "2  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "3  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "4  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_output = subprocess.check_output(\"ls -l /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-*_00000.c000.csv\", shell=True).decode(\"utf-8\")\n",
    "\n",
    "ls_lines = ls_output.splitlines()\n",
    "\n",
    "ls_data = [line.split() for line in ls_lines[1:]]  # Skip the first line (total count)\n",
    "\n",
    "df = pd.DataFrame(ls_data)\n",
    "\n",
    "# Show the table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cf243-7fc4-41f2-933c-d45495d0012d",
   "metadata": {},
   "source": [
    "Count the total no of records for al 4 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "872082f7-5ae5-49f9-8242-8aef1d37ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245699</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245036</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245055</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245300</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213147</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1439458</td>\n",
       "      <td>total</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  \\\n",
       "0   245699   \n",
       "1   245036   \n",
       "2   245055   \n",
       "3   245300   \n",
       "4   213147   \n",
       "5  1439458   \n",
       "\n",
       "                                                                                                                                           1  \n",
       "0  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00001-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "1  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00002-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "2  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00003-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "3  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00004-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "4  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-00005-0c6abbd5-e8ef-4da3-94a0-fce1f9cad25e_00000.c000.csv  \n",
       "5                                                                                                                                      total  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_output = subprocess.check_output(\"wc -l /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/sales_bucket/part-*_00000.c000.csv\", shell=True).decode(\"utf-8\")\n",
    "\n",
    "ls_lines = ls_output.splitlines()\n",
    "\n",
    "ls_data = [line.split() for line in ls_lines[1:]]  # Skip the first line (total count)\n",
    "\n",
    "df = pd.DataFrame(ls_data)\n",
    "\n",
    "# Show the table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546cbbe-b270-4513-b723-070c737556f6",
   "metadata": {},
   "source": [
    "Check total number of records for city bucket 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63678c0a-667a-4a40-8e02-81e0f8a4b2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277240</td>\n",
       "      <td>/home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/city_bucket/part-00001-84513e10-2ffe-4fca-8940-b3b78828a59b_00000.c000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>586724</td>\n",
       "      <td>total</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  \\\n",
       "0  277240   \n",
       "1  586724   \n",
       "\n",
       "                                                                                                                                          1  \n",
       "0  /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/city_bucket/part-00001-84513e10-2ffe-4fca-8940-b3b78828a59b_00000.c000.csv  \n",
       "1                                                                                                                                     total  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_output = subprocess.check_output(\"wc -l /home/ubuntu/SparkLearning/spark-warehouse/SparkLearning/data/city_bucket/part-*_00000.c000.csv\", shell=True).decode(\"utf-8\")\n",
    "\n",
    "ls_lines = ls_output.splitlines()\n",
    "\n",
    "ls_data = [line.split() for line in ls_lines[1:]]  # Skip the first line (total count)\n",
    "\n",
    "df = pd.DataFrame(ls_data)\n",
    "\n",
    "# Show the table\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d29a7-c785-4bcc-8f7e-d2b55a4e334d",
   "metadata": {},
   "source": [
    "Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deaee0a2-64cc-4c9b-9c54-3df3292651e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026182\n"
     ]
    }
   ],
   "source": [
    "print(1439458+586724)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11514d-d87b-4b3d-b463-e75de66bcc38",
   "metadata": {},
   "source": [
    "Now subtract the header rows (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0a6b60-51ee-4185-8167-119699cd0472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026176\n"
     ]
    }
   ],
   "source": [
    "print(1439458+586724-6) # equals the number of records in the first task 432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783d4c2-df09-4566-9984-2c8b21f8e68e",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/EUklwA.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd78ba-165e-41de-8ffc-41ac15835856",
   "metadata": {},
   "source": [
    "### Points to Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc8e02-4262-4f05-9df5-225a9d5c0306",
   "metadata": {},
   "source": [
    "1. Joining columns diff from bucket column - Shuffle on both sides\n",
    "\n",
    "2. Joining column same, one column in bucket - Shuffle on non bucket table\n",
    "\n",
    "3. Joining column same, diff bucket size - Shuffle on table with smaller bucket size\n",
    "\n",
    "4. Joining column same, same bucket size - no shuffle, fast joins\n",
    "\n",
    "5. Too many buckets with not enough data leads to small file issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
