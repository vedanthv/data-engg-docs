{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248fc69c-b51c-4583-9032-c2789e99a58c",
   "metadata": {},
   "source": [
    "### Dynamic vs Static Resource Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82250c6-8f0e-4cd8-833d-688e8441d683",
   "metadata": {},
   "source": [
    "Two applications asking for resources at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb7e8-73b2-4377-86d6-52c679b368d4",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/ualWDt.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbd198-a559-4fe1-93c3-859b77e79994",
   "metadata": {},
   "source": [
    "App 2 will not have enough resources to run in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666dab49-0bb0-4e20-9a1f-d1922a644e6d",
   "metadata": {},
   "source": [
    "To avoid this, we use dynamic allocation where the executor resources are scaled up or down depending on its usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21928bd7-1c90-4a40-b36d-d651bb0ca9d6",
   "metadata": {},
   "source": [
    "### Spark Dynamic Allocation Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc438abc-d508-49b1-bae6-629b7e0ad715",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.enabled```**\n",
    "\n",
    "Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c43bf3-5c89-4b14-9988-35be8b875e97",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.executorIdleTimeout```**\n",
    "\n",
    "If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed.\n",
    "\n",
    "Default is 60s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c87692-b201-4298-9eb1-31d75521d04f",
   "metadata": {},
   "source": [
    "Note that, under most circumstances, this condition is mutually exclusive with the request condition, in that an executor should not be idle if there are still pending tasks to be scheduled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5fbca-7da1-4536-bb59-73ab2740445b",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.cachedExecutorIdleTimeout```**\n",
    "\n",
    "If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. \n",
    "\n",
    "Default is infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bfdfc-48d8-4c57-822f-0acc36fd1fd4",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.initialExecutors```**\n",
    "\n",
    "Initial number of executors to run if dynamic allocation is enabled.\n",
    "\n",
    "If `--num-executors` (or `spark.executor.instances`) is set and larger than this value, it will be used as the initial number of executors.\n",
    "\n",
    "Default is **spark.dynamicAllocation.minExecutors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfc1ab-c74c-4d45-a8c5-27bb7cc4c5e5",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.maxExecutors```**\n",
    "\n",
    "Upper bound for the number of executors if dynamic allocation is enabled.\t\n",
    "\n",
    "Default is infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5f63c-219c-4c77-bd80-1fbe5586bb8e",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.executorAllocationRatio```**\n",
    "\n",
    "By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. \n",
    "\n",
    "This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0225e-98d0-4138-a237-3c23ded52657",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.schedulerBacklogTimeout```**\n",
    "\n",
    "If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1a8c6-52be-47c1-98d4-bd878152d0b2",
   "metadata": {},
   "source": [
    "Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae441c-9e9f-4183-95b8-f29ae8d6865e",
   "metadata": {},
   "source": [
    "Spark requests executors in rounds. The actual request is triggered when there have been pending tasks for spark.dynamicAllocation.schedulerBacklogTimeout seconds, and then triggered again every spark.dynamicAllocation.sustainedSchedulerBacklogTimeout seconds thereafter if the queue of pending tasks persists. Additionally, the number of executors requested in each round increases exponentially from the previous round. For instance, an application will add 1 executor in the first round, and then 2, 4, 8 and so on executors in the subsequent rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848f45a-494f-4f75-ad48-392908d8c6f9",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.shuffleTracking.enabled```**\n",
    "\n",
    "Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60fdad-b083-49af-a6ec-9bc5a7cd9cfb",
   "metadata": {},
   "source": [
    "**```spark.dynamicAllocation.shuffleTracking.timeout```**\n",
    "\n",
    "When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82352a2b-f8cd-443f-b7eb-7a058396ab37",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/kyF8Df.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f10023-8bf4-4fe7-bc9f-3d7ccad621b8",
   "metadata": {},
   "source": [
    "After 60 seconods all the executors are killed\n",
    "\n",
    "Sometiems the executors take time to be killed due to garbage collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e689938-6d9f-4a72-939b-3412de860da1",
   "metadata": {},
   "source": [
    "<img src = \"https://snipboard.io/vUyl1F.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba07068-92a6-4090-b8dc-30ab89e5bdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
