# How does Kafka Know Which Messages are Processed, not just read?

Excellent â€” this question gets to the **core of how Kafka tracks message progress** and why the line

> â€œKafka doesnâ€™t know what youâ€™ve processed â€” only what youâ€™ve read and committedâ€
> is one of the most important truths for any Kafka developer to understand.

Letâ€™s unpack this carefully.

---

## ğŸ”¹ 1. Kafka **does not** inherently know whether a consumer â€œprocessedâ€ a message

Kafka itself is **completely unaware** of what happens to a message **after** it is fetched by a consumer.

Hereâ€™s what Kafka does track:

* Which consumer groups exist (`group.id`)
* Which partitions each consumer owns
* The **last committed offset** per consumer group and partition

Kafka does **not** track:

* Whether the consumer application finished processing a record
* Whether it wrote results to a database, API, or data warehouse
* Whether processing succeeded or failed

From Kafkaâ€™s perspective, **â€œprocessingâ€** simply means **â€œoffset committed.â€**

---

## ğŸ”¹ 2. What Kafka actually sees during consumption

Letâ€™s visualize what happens in a consumer loop:

```java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

for (ConsumerRecord<String, String> record : records) {
    process(record);  // your business logic
}

consumer.commitSync();  // tell Kafka â€œIâ€™ve processed up to this offsetâ€
```

Internally, Kafka only sees two key actions:

1. The **poll()** call â€” consumer fetches messages from a partition (e.g., offsets 100â€“110)
2. The **commitSync()** call â€” consumer tells Kafka â€œIâ€™m done up to offset Xâ€

That commit updates the **offset checkpoint** for the consumer group in Kafkaâ€™s internal topic:

```
__consumer_offsets
```

So the broker stores, for example:

```
(group.id = order_service, topic = orders, partition = 0) â†’ offset = 110
```

When another consumer in that group starts, it resumes from **offset 110** (next message is 111).

Kafka never verifies if messages 100â€“110 were really processed â€” it trusts the consumer to commit correctly.

---

## ğŸ”¹ 3. Why this distinction matters: â€œreadâ€ vs. â€œprocessedâ€

* **Read messages:** messages returned by `poll()` â€” fetched from Kafka, but not necessarily processed.
* **Processed messages:** messages that your application has completed handling â€” e.g., written to a DB, sent to an API, enriched, etc.

If you **commit too early** â€” i.e., before your app has finished processing â€” then on a crash, Kafka assumes you already handled those messages.

Result: **data loss** (messages are skipped and never reprocessed).

If you **commit too late** â€” i.e., after a large batch â€” then a crash may cause Kafka to replay already-processed messages.

Result: **duplicates** (messages reprocessed).

---

## ğŸ”¹ 4. Example: committing offsets incorrectly inside the poll loop

Imagine this common mistake:

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    consumer.commitSync(); // âŒ wrong place!
    for (ConsumerRecord<String, String> record : records) {
        process(record);
    }
}
```

What happens:

1. Consumer polls messages 100â€“110.
2. Immediately commits offset 110 (before processing).
3. Then processes only records 100â€“105.
4. Crashes.

Now, Kafka thinks this consumer has processed up to offset 110.
When it restarts, it resumes at offset 111 â€” skipping messages 106â€“110 entirely.

â†’ **Five messages lost forever.**

Kafka didnâ€™t â€œknowâ€ they werenâ€™t processed â€” it only saw the offset commit.

---

## ğŸ”¹ 5. Correct approach: commit *after* processing

Instead, the offset commit should happen **after successful processing**:

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        process(record);
        consumer.commitSync(Collections.singletonMap(
            new TopicPartition(record.topic(), record.partition()),
            new OffsetAndMetadata(record.offset() + 1)
        ));
    }
}
```

Now, youâ€™re committing **offset + 1** only after youâ€™ve successfully processed that record.

This ensures:

* If you crash mid-processing â†’ unprocessed messages will be retried (at-least-once).
* No message is skipped.

---

## ğŸ”¹ 6. How Kafka stores and uses committed offsets

Offsets are stored in Kafkaâ€™s **internal topic**:

```
__consumer_offsets
```

Each commit writes a key-value pair:

| Key                                | Value                 |
| ---------------------------------- | --------------------- |
| (`group.id`, `topic`, `partition`) | `offset` + `metadata` |

This topic is compacted (old offset commits are deleted, only the latest remains).
When a consumer group restarts or rebalances, it reads these committed offsets and resumes consumption from there.

So the **only signal** Kafka uses to decide whatâ€™s been processed is:

> the latest committed offset.

---

## ğŸ”¹ 7. How to safely ensure correct processing + committing

### Option 1: **Manual commits (recommended)**

Commit offsets manually *after* processing to achieve **at-least-once** semantics:

```java
enable.auto.commit=false
```

Then commit using `commitSync()` or `commitAsync()` after successful processing.

---

### Option 2: **Transactional (exactly-once) processing**

For critical systems where duplicates are not acceptable (e.g., financial trades), use **Kafka transactions**.

In this mode, producers and consumers coordinate atomic writes:

* The producer writes output messages **and commits offsets** as one transaction.
* If the transaction succeeds â†’ offsets are committed.
* If it fails â†’ offsets arenâ€™t committed (messages are retried).

This is configured using:

```properties
enable.idempotence=true
transactional.id=unique-producer-id
isolation.level=read_committed
```

Kafka then guarantees:

> Messages are processed **exactly once**, even across retries or restarts.

---

## ğŸ”¹ 8. Analogy â€” â€œKafka is a delivery log, not a ledger of completionâ€

Think of Kafka as a **courier service**:

* It logs every package delivered to your door (messages fetched).
* But it doesnâ€™t know whether you opened the package or used whatâ€™s inside (message processed).

You, the consumer, must tell Kafka:

> â€œIâ€™ve successfully unpacked everything up to here.â€ (commit offset)

If you confirm before opening â€” you risk losing packages.
If you delay too long â€” you may re-open old packages.
Kafkaâ€™s job ends once the package is delivered â€” itâ€™s your responsibility to acknowledge **only after successful handling**.

---

## ğŸ”¹ 9. Summary

| Concept            | Kafka Knows? | Tracked How?                                   |
| ------------------ | ------------ | ---------------------------------------------- |
| Message fetched    | âœ… Yes        | Through the `poll()` request                   |
| Message processed  | âŒ No         | Only your application knows                    |
| Message committed  | âœ… Yes        | Through offset commits to `__consumer_offsets` |
| Offset position    | âœ… Yes        | Latest committed offset per group-partition    |
| Processing success | âŒ No         | Must be implied by commit timing               |

---

### ğŸ”¸ Final takeaway:

Kafka only knows what you **committed**, not what you **processed**.
Itâ€™s up to your application logic to **commit offsets only after successful processing** â€” this is the foundation of reliable Kafka consumption.
