{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3620ea75-6baf-4f7b-947a-ec303cbd4527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=2837794124011939#joblist/pipelines/create?initialSource=%2FUsers%2Fvedanthvbaliga%40gmail.com%2FNotebooks%2FDelta+Live+Tables&redirectNotebookId=2565325723207242\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4525527394208293>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# DLT works with 3 types of datasets\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Streaming Tables (Permanent / Temporary) - Used as append data sources, Incremental data\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Materialized Views - Used for transformations, aggregation or computation\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Views - Used for intermediate transformations not stored in target schemas.\u001B[39;00m\n",
       "\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/autoreload_discoverability_hook.py:96\u001B[0m, in \u001B[0;36mAutoreloadDiscoverabilityHook._patched_import\u001B[0;34m(self, name, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;129;01mand\u001B[39;00m (\n",
       "\u001B[1;32m     91\u001B[0m     (module \u001B[38;5;241m:=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mget(absolute_name)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m     92\u001B[0m     (fname \u001B[38;5;241m:=\u001B[39m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m     93\u001B[0m     (mtime \u001B[38;5;241m:=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_mtime_by_modname\u001B[38;5;241m.\u001B[39mget(\n",
       "\u001B[1;32m     94\u001B[0m         absolute_name, \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint):\n",
       "\u001B[1;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m---> 96\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_builtins_import(name, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (fname \u001B[38;5;241m:=\u001B[39m fname \u001B[38;5;129;01mor\u001B[39;00m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     98\u001B[0m     mtime \u001B[38;5;241m=\u001B[39m mtime \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:239\u001B[0m, in \u001B[0;36mmake_patched_exec_module.<locals>.patched_exec_module\u001B[0;34m(module)\u001B[0m\n",
       "\u001B[1;32m    236\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(exec_module)\n",
       "\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpatched_exec_module\u001B[39m(module: ModuleType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 239\u001B[0m         exec_module(module)\n",
       "\u001B[1;32m    240\u001B[0m         notify_module_loaded(module)\n",
       "\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:24\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n",
       "\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n",
       "\u001B[1;32m     25\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     26\u001B[0m     )\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01musage_warnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecated_function\n",
       "\n",
       "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "DLTImportException",
        "evalue": "Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>DLTImportException</span>: Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)",
        "File \u001B[0;32m<command-4525527394208293>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# DLT works with 3 types of datasets\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Streaming Tables (Permanent / Temporary) - Used as append data sources, Incremental data\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Materialized Views - Used for transformations, aggregation or computation\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Views - Used for intermediate transformations not stored in target schemas.\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/autoreload_discoverability_hook.py:96\u001B[0m, in \u001B[0;36mAutoreloadDiscoverabilityHook._patched_import\u001B[0;34m(self, name, *args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m     91\u001B[0m     (module \u001B[38;5;241m:=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mget(absolute_name)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m     92\u001B[0m     (fname \u001B[38;5;241m:=\u001B[39m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m     93\u001B[0m     (mtime \u001B[38;5;241m:=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_mtime_by_modname\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m     94\u001B[0m         absolute_name, \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 96\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_builtins_import(name, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (fname \u001B[38;5;241m:=\u001B[39m fname \u001B[38;5;129;01mor\u001B[39;00m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     98\u001B[0m     mtime \u001B[38;5;241m=\u001B[39m mtime \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:239\u001B[0m, in \u001B[0;36mmake_patched_exec_module.<locals>.patched_exec_module\u001B[0;34m(module)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(exec_module)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpatched_exec_module\u001B[39m(module: ModuleType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 239\u001B[0m         exec_module(module)\n\u001B[1;32m    240\u001B[0m         notify_module_loaded(module)\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     26\u001B[0m     )\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01musage_warnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecated_function\n",
        "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DLT works with 3 types of datasets\n",
    "# Streaming Tables (Permanent / Temporary) - Used as append data sources, Incremental data\n",
    "# Materialized Views - Used for transformations, aggregation or computation\n",
    "# Views - Used for intermediate transformations not stored in target schemas.\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dcb8b6c-79de-4178-ad54-7d6d13e55011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_order_status = spark.conf.get(\"custom.orderStatus\",\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8fb0bc-2a96-4ca0-94c0-f84d6b1ca79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rules for data quality (drop,fail,warn)\n",
    "\n",
    "__order_rules = {\n",
    "    \"Valid Order Status\" : \"o_orderstatus in ('O','F','P')\",\n",
    "    \"Valid Order Price\" : \"o_totalprice > 0\"\n",
    "}\n",
    "\n",
    "__customer_rules = {\n",
    "    \"Valid Market Segment\" : \"c_mktsegment is not null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c1f0c4-3ca9-4c62-8798-21b896a47208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Streaming Table\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\":\"bronze\"},\n",
    "  comment = 'order bronze table'\n",
    ")\n",
    "def orders_bronze():\n",
    "    df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffccde30-20fa-479f-9ce8-f974d931c217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Orders Autoloader table\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\":\"bronze\",\"pipeline.reset.allowed\":\"false\"}, # pipeline reset false makes sure this table is not refreshed during full refresh\n",
    "  comment = 'order bronze table',\n",
    "  name = 'orders_autoloader_bronze'\n",
    ")\n",
    "def func():\n",
    "    df = spark.readStream.format('cloudFiles').option('cloudFiles.schemaHints','o_orderkey long,o_custkey long,o_orderstatus string,o_totalprice decimal(18,2),o_orderdate date,o_orderpriority string,o_clerk string,o_shippriority integer,o_comment string').option('cloudFiles.schemaLocation','/Volumes/dev/etl/landing/autoloader/schemas/1').option('cloudFiles.format','CSV').option('pathGlobFilter','*.csv').option('cloudFiles.schemaEvolutionMode','none').load('/Volumes/dev/etl/landing/files')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e241fb2-28c1-44e7-9b66-290871d88826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union in two or more streaming tables\n",
    "\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "# Append Flow table 1\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def orders_delta_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "\n",
    "# Append Flow table 2\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def orders_autoloader_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd667ef3-5a88-4127-8dc4-b09dbe9ecff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Create materialized view for customer\n",
    "# @dlt.table(\n",
    "#   comment = 'customer bronze table'\n",
    "# )\n",
    "# def customer_bronze():\n",
    "#     df = spark.read.table(\"dev.bronze.customers_raw\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3c10a7-af49-41e8-8e76-049cdcb786ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create materialized view for customer\n",
    "@dlt.view( \n",
    "  comment = 'customer bronze view'\n",
    ")\n",
    "def customer_bronze_vw():\n",
    "    df = spark.readStream.table(\"dev.bronze.customers_raw\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81f14eaa-556b-46cd-ac62-49ed4900121c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "dlt.create_streaming_table(\"customer_scd1_bronze\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd1_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = ['c_custkey'],\n",
    "    apply_as_deletes = expr(\"_src_action = 'D'\"),\n",
    "    apply_as_truncates=expr(\"_src_action = 'T'\"),\n",
    "    except_column_list=['_src_action','_src_insert_dt'],\n",
    "    sequence_by = \"_src_insert_dt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f86a83-5e11-4959-9782-1f3440d6bc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "dlt.create_streaming_table(\"customer_scd2_bronze\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd2_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = ['c_custkey'],\n",
    "    sequence_by = \"_src_insert_dt\",\n",
    "    stored_as_scd_type=2,\n",
    "    except_column_list=['_src_action','_src_insert_dt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a6f4839-32c9-462c-b540-ec9b1488604b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view to join orders with customers\n",
    "@dlt.view(\n",
    "    comment = 'Joined View'\n",
    ")\n",
    "@dlt.expect_all(__order_rules)\n",
    "@dlt.expect_all(__customer_rules)\n",
    "\n",
    "def joined_vw():\n",
    "    df_c = spark.read.table(\"LIVE.customer_scd2_bronze\").where(\"__END_AT is null\")\n",
    "    df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n",
    "    return df_o.join(df_c,how = 'left_outer',on = df_c.c_custkey == df_o.o_custkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ec5373-19d2-4629-a6b9-b5d8994123c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp,count,sum\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\"quality\":\"silver\"},\n",
    "    comment = \"Joined_Table\",\n",
    "    name = \"joined_silver\"\n",
    ")\n",
    "def joined_silver():\n",
    "    df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"_insert_date\", current_timestamp())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3f0008-8649-4bb3-acc0-3d49b11fba95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "        table_properties = {\"quality\":\"gold\"},\n",
    "        comment = \"order aggregated table\",\n",
    "        name = f\"orders_agg_gold\"\n",
    "    )\n",
    "def orders_aggregated_gold():\n",
    "    df = spark.read.table(\"LIVE.joined_silver\")\n",
    "    df_final = df.groupBy(\"c_mktsegment\").agg(count('o_orderkey').alias(\"count_of_orders\"),sum(\"o_totalprice\").alias('sum_totalprice')).withColumn(\"_insert_date\", current_timestamp())\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1152db25-bb9f-4d45-9ba7-5c61f76f969f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for _status in _order_status.split(\",\"):\n",
    "    # create gold table\n",
    "    @dlt.table(\n",
    "        table_properties = {\"quality\":\"gold\"},\n",
    "        comment = \"order aggregated table\",\n",
    "        name = f\"orders_agg_{_status}_gold\"\n",
    "    )\n",
    "    def orders_aggregated_gold():\n",
    "        df = spark.read.table(\"LIVE.joined_silver\")\n",
    "        df_final = df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count('o_orderkey').alias(\"count_of_orders\"),sum(\"o_totalprice\").alias('sum_totalprice')).withColumn(\"_insert_date\", current_timestamp())\n",
    "\n",
    "        return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5535801836315154,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta Live Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}